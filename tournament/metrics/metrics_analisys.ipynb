{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a64c7dc",
   "metadata": {},
   "source": [
    "# ðŸ¤– AnÃ¡lisis de MÃ©tricas de Entrenamiento Q-Learning\n",
    "\n",
    "Este notebook analiza las mÃ©tricas generadas durante el entrenamiento del agente Q-Learning para Connect 4.\n",
    "\n",
    "## ðŸ“Š Contenido del AnÃ¡lisis:\n",
    "1. **Progreso de Entrenamiento**: EvoluciÃ³n de victorias, derrotas y empates\n",
    "2. **Tasa de Victoria**: Porcentaje de partidas ganadas a lo largo del tiempo\n",
    "3. **Recompensas**: EvoluciÃ³n de las recompensas obtenidas\n",
    "4. **DistribuciÃ³n de DuraciÃ³n**: AnÃ¡lisis de la longitud de las partidas\n",
    "5. **EstadÃ­sticas Finales**: Resumen completo del rendimiento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar los datos de mÃ©tricas\n",
    "try:\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Datos cargados exitosamente: {len(data)} puntos de mÃ©tricas\")\n",
    "    \n",
    "    # Extraer datos\n",
    "    episodes = [d[\"episode\"] for d in data]\n",
    "    rewards = [d[\"reward\"] for d in data]\n",
    "    steps = [d[\"steps\"] for d in data] \n",
    "    wins = [d[\"wins\"] for d in data]\n",
    "    losses = [d[\"losses\"] for d in data]\n",
    "    draws = [d[\"draws\"] for d in data]\n",
    "    \n",
    "    # Calcular mÃ©tricas adicionales\n",
    "    total_games = np.array(wins) + np.array(losses) + np.array(draws)\n",
    "    win_rates = np.array(wins) / total_games * 100\n",
    "    \n",
    "    # Verificar la consistencia de los datos\n",
    "    print(f\"ðŸ” VerificaciÃ³n de datos:\")\n",
    "    print(f\"   - Rango de episodios: {min(episodes)} - {max(episodes)}\")\n",
    "    print(f\"   - Total de puntos de datos: {len(episodes)}\")\n",
    "    print(f\"   - Victorias finales: {wins[-1]}\")\n",
    "    print(f\"   - Tasa de victoria final: {win_rates[-1]:.2f}%\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de haber ejecutado el entrenamiento primero\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"âŒ Error al leer el archivo JSON: {e}\")\n",
    "    print(\"ðŸ’¡ El archivo de mÃ©tricas puede estar corrupto\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error inesperado: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las visualizaciones principales\n",
    "print(\"ðŸ“Š Generando visualizaciones...\")\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Progreso de victorias/derrotas/empates\n",
    "axes[0, 0].plot(episodes, wins, label=\"Victorias\", color='green', linewidth=2, marker='o', markersize=2)\n",
    "axes[0, 0].plot(episodes, losses, label=\"Derrotas\", color='red', linewidth=2, marker='s', markersize=2)\n",
    "axes[0, 0].plot(episodes, draws, label=\"Empates\", color='orange', linewidth=2, marker='^', markersize=2)\n",
    "axes[0, 0].set_xlabel(\"Episodio\")\n",
    "axes[0, 0].set_ylabel(\"Partidas Acumuladas\")\n",
    "axes[0, 0].set_title(\"ðŸ“ˆ Progreso de Entrenamiento\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Tasa de victoria\n",
    "axes[0, 1].plot(episodes, win_rates, label=\"Tasa de Victoria (%)\", color='blue', linewidth=2)\n",
    "axes[0, 1].set_xlabel(\"Episodio\")\n",
    "axes[0, 1].set_ylabel(\"Porcentaje de Victorias\")\n",
    "axes[0, 1].set_title(\"ðŸŽ¯ Tasa de Victoria\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 100])\n",
    "# AÃ±adir lÃ­nea horizontal en 50% para referencia\n",
    "axes[0, 1].axhline(y=50, color='red', linestyle='--', alpha=0.5, label='Aleatorio (50%)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Recompensas promedio con suavizado\n",
    "window = min(20, len(rewards)//10)\n",
    "if window > 1:\n",
    "    rewards_smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    episodes_smooth = episodes[window-1:]\n",
    "    axes[1, 0].plot(episodes_smooth, rewards_smooth, label=f\"Recompensa (suavizada, ventana={window})\", color='purple', linewidth=2)\n",
    "    axes[1, 0].plot(episodes, rewards, alpha=0.3, color='gray', linewidth=0.5, label='Recompensa original')\n",
    "else:\n",
    "    axes[1, 0].plot(episodes, rewards, label=\"Recompensa\", color='purple', linewidth=2)\n",
    "\n",
    "axes[1, 0].set_xlabel(\"Episodio\")\n",
    "axes[1, 0].set_ylabel(\"Recompensa\")\n",
    "axes[1, 0].set_title(\"ðŸ’° EvoluciÃ³n de Recompensas\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "# LÃ­nea horizontal en 0 para referencia\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# 4. DistribuciÃ³n de duraciÃ³n de partidas\n",
    "axes[1, 1].hist(steps, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].axvline(np.mean(steps), color='red', linestyle='--', linewidth=2, label=f'Media: {np.mean(steps):.1f}')\n",
    "axes[1, 1].axvline(np.median(steps), color='green', linestyle='--', linewidth=2, label=f'Mediana: {np.median(steps):.1f}')\n",
    "axes[1, 1].set_xlabel(\"Pasos por Partida\")\n",
    "axes[1, 1].set_ylabel(\"Frecuencia\")\n",
    "axes[1, 1].set_title(\"â±ï¸ DistribuciÃ³n de DuraciÃ³n de Partidas\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"ðŸ¤– AnÃ¡lisis Completo de Entrenamiento Q-Learning - Connect 4\", fontsize=18, y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstadÃ­sticas detalladas y anÃ¡lisis avanzado\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š ANÃLISIS ESTADÃSTICO COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# EstadÃ­sticas bÃ¡sicas\n",
    "print(\"ðŸŽ¯ RENDIMIENTO FINAL:\")\n",
    "print(f\"   ðŸ† Victorias: {wins[-1]:,} ({win_rates[-1]:.2f}%)\")\n",
    "print(f\"   ðŸ’€ Derrotas: {losses[-1]:,} ({losses[-1]/total_games[-1]*100:.2f}%)\")\n",
    "print(f\"   ðŸ¤ Empates: {draws[-1]:,} ({draws[-1]/total_games[-1]*100:.2f}%)\")\n",
    "print(f\"   ðŸŽ² Total de partidas: {total_games[-1]:,}\")\n",
    "\n",
    "print(\"\\nâ±ï¸ DURACIÃ“N DE PARTIDAS:\")\n",
    "print(f\"   ðŸ“Š Promedio: {np.mean(steps):.1f} pasos\")\n",
    "print(f\"   ðŸ“ˆ Mediana: {np.median(steps):.1f} pasos\")\n",
    "print(f\"   ðŸ“‰ MÃ­nima: {min(steps)} pasos\")\n",
    "print(f\"   ðŸ“ˆ MÃ¡xima: {max(steps)} pasos\")\n",
    "print(f\"   ðŸ“ DesviaciÃ³n estÃ¡ndar: {np.std(steps):.1f} pasos\")\n",
    "\n",
    "# AnÃ¡lisis de convergencia\n",
    "print(\"\\nðŸ”„ ANÃLISIS DE CONVERGENCIA:\")\n",
    "# Calcular la mejora en la tasa de victoria\n",
    "if len(win_rates) >= 10:\n",
    "    initial_wr = np.mean(win_rates[:10])  # Primeros 10 puntos\n",
    "    final_wr = np.mean(win_rates[-10:])   # Ãšltimos 10 puntos\n",
    "    improvement = final_wr - initial_wr\n",
    "    print(f\"   ðŸ“ˆ Tasa de victoria inicial: {initial_wr:.2f}%\")\n",
    "    print(f\"   ðŸŽ¯ Tasa de victoria final: {final_wr:.2f}%\")\n",
    "    print(f\"   ðŸš€ Mejora total: {improvement:+.2f}%\")\n",
    "\n",
    "# Encontrar el punto de convergencia (donde la mejora se estabiliza)\n",
    "if len(win_rates) >= 20:\n",
    "    # Calcular la varianza mÃ³vil para encontrar estabilizaciÃ³n\n",
    "    window_size = max(10, len(win_rates) // 20)\n",
    "    variances = []\n",
    "    for i in range(window_size, len(win_rates)):\n",
    "        window_data = win_rates[i-window_size:i]\n",
    "        variances.append(np.var(window_data))\n",
    "    \n",
    "    # Encontrar donde la varianza es mÃ­nima (mÃ¡s estable)\n",
    "    if variances:\n",
    "        min_var_idx = np.argmin(variances) + window_size\n",
    "        convergence_episode = episodes[min_var_idx]\n",
    "        print(f\"   ðŸŽ¯ Convergencia aproximada en episodio: {convergence_episode}\")\n",
    "\n",
    "print(\"\\nðŸ’° ANÃLISIS DE RECOMPENSAS:\")\n",
    "print(f\"   ðŸ“Š Recompensa promedio: {np.mean(rewards):.3f}\")\n",
    "print(f\"   ðŸ“ˆ Recompensa mÃ¡xima: {max(rewards):.3f}\")\n",
    "print(f\"   ðŸ“‰ Recompensa mÃ­nima: {min(rewards):.3f}\")\n",
    "\n",
    "# Porcentaje de partidas con recompensa positiva, negativa y neutra\n",
    "positive_rewards = sum(1 for r in rewards if r > 0)\n",
    "negative_rewards = sum(1 for r in rewards if r < 0)\n",
    "neutral_rewards = sum(1 for r in rewards if r == 0)\n",
    "\n",
    "print(f\"   âœ… Recompensas positivas: {positive_rewards} ({positive_rewards/len(rewards)*100:.1f}%)\")\n",
    "print(f\"   âŒ Recompensas negativas: {negative_rewards} ({negative_rewards/len(rewards)*100:.1f}%)\")\n",
    "print(f\"   âšª Recompensas neutras: {neutral_rewards} ({neutral_rewards/len(rewards)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d288a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrÃ¡ficos adicionales y anÃ¡lisis de tendencias\n",
    "print(\"ðŸ“ˆ Generando anÃ¡lisis adicionales...\")\n",
    "\n",
    "# Crear mÃ¡s visualizaciones especÃ­ficas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. EvoluciÃ³n de la exploraciÃ³n (aproximada por la varianza de recompensas)\n",
    "if len(episodes) > 10:\n",
    "    window = max(5, len(episodes) // 50)\n",
    "    exploration_proxy = []\n",
    "    for i in range(window, len(rewards)):\n",
    "        window_rewards = rewards[i-window:i]\n",
    "        exploration_proxy.append(np.std(window_rewards))\n",
    "    \n",
    "    axes[0].plot(episodes[window:], exploration_proxy, color='orange', linewidth=2)\n",
    "    axes[0].set_xlabel(\"Episodio\")\n",
    "    axes[0].set_ylabel(\"Variabilidad de Recompensas\")\n",
    "    axes[0].set_title(\"ðŸ” EvoluciÃ³n de la ExploraciÃ³n\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. DistribuciÃ³n de recompensas por bins\n",
    "axes[1].hist(rewards, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1].axvline(np.mean(rewards), color='blue', linestyle='--', linewidth=2, label=f'Media: {np.mean(rewards):.3f}')\n",
    "axes[1].set_xlabel(\"Recompensa\")\n",
    "axes[1].set_ylabel(\"Frecuencia\")\n",
    "axes[1].set_title(\"ðŸŽ DistribuciÃ³n de Recompensas\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. CorrelaciÃ³n entre duraciÃ³n de partida y resultado\n",
    "win_steps = [steps[i] for i in range(len(steps)) if rewards[i] > 0]\n",
    "loss_steps = [steps[i] for i in range(len(steps)) if rewards[i] < 0]\n",
    "draw_steps = [steps[i] for i in range(len(steps)) if rewards[i] == 0]\n",
    "\n",
    "if win_steps:\n",
    "    axes[2].hist(win_steps, bins=20, alpha=0.6, color='green', label=f'Victorias (n={len(win_steps)})', density=True)\n",
    "if loss_steps:\n",
    "    axes[2].hist(loss_steps, bins=20, alpha=0.6, color='red', label=f'Derrotas (n={len(loss_steps)})', density=True)\n",
    "if draw_steps:\n",
    "    axes[2].hist(draw_steps, bins=20, alpha=0.6, color='orange', label=f'Empates (n={len(draw_steps)})', density=True)\n",
    "\n",
    "axes[2].set_xlabel(\"DuraciÃ³n (pasos)\")\n",
    "axes[2].set_ylabel(\"Densidad\")\n",
    "axes[2].set_title(\"âš–ï¸ DuraciÃ³n vs Resultado\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar un resumen de las mÃ©tricas principales\n",
    "summary = {\n",
    "    \"episodios_totales\": episodes[-1],\n",
    "    \"tasa_victoria_final\": win_rates[-1],\n",
    "    \"victorias_totales\": wins[-1],\n",
    "    \"derrotas_totales\": losses[-1],\n",
    "    \"empates_totales\": draws[-1],\n",
    "    \"duracion_promedio\": np.mean(steps),\n",
    "    \"recompensa_promedio\": np.mean(rewards),\n",
    "    \"estados_q_table\": \"InformaciÃ³n disponible en q_table.pkl\"\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ’¾ Resumen guardado en memoria:\")\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis completado exitosamente!\")\n",
    "print(\"ðŸ’¡ Tips para mejorar el rendimiento:\")\n",
    "print(\"   â€¢ Entrenar mÃ¡s episodios si la tasa de victoria no se ha estabilizado\")\n",
    "print(\"   â€¢ Ajustar parÃ¡metros de aprendizaje (alpha, gamma, epsilon)\")\n",
    "print(\"   â€¢ Entrenar contra oponentes mÃ¡s fuertes (como MCTS)\")\n",
    "print(\"   â€¢ Considerar tÃ©cnicas de Deep Q-Learning para espacios de estado grandes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iaenv)",
   "language": "python",
   "name": "iaenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
