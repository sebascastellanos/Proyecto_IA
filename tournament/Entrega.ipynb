{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2bcd01",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4920168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"Librer√≠as importadas exitosamente\")\n",
    "print(f\"Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abe99a",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento del Agente Q-Learning\n",
    "\n",
    "Primero vamos a entrenar nuestro agente Q-Learning si no tenemos m√©tricas previas, o cargar las existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si ya existen m√©tricas de entrenamiento\n",
    "metrics_file = 'metrics/training_metrics_final.json'\n",
    "checkpoint_file = 'metrics/checkpoint_data.json'\n",
    "\n",
    "if os.path.exists(metrics_file) and os.path.exists(checkpoint_file):\n",
    "    print(\"M√©tricas de entrenamiento encontradas\")\n",
    "    training_needed = False\n",
    "else:\n",
    "    print(\"No se encontraron m√©tricas previas\")\n",
    "    print(\"Se necesita ejecutar el entrenamiento\")\n",
    "    training_needed = True\n",
    "\n",
    "print(f\"Archivo de m√©tricas: {'Existe' if os.path.exists(metrics_file) else 'No existe'}\")\n",
    "print(f\"Archivo de checkpoints: {'Existe' if os.path.exists(checkpoint_file) else 'No existe'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para ejecutar entrenamiento si es necesario\n",
    "def run_training_if_needed():\n",
    "    \"\"\"Ejecuta el entrenamiento del agente Q-Learning si no existen m√©tricas\"\"\"\n",
    "    if not training_needed:\n",
    "        print(\"‚è≠Entrenamiento no necesario, m√©tricas ya disponibles\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        print(\"Iniciando entrenamiento del agente Q-Learning...\")\n",
    "        print(\"‚è±Esto puede tomar varios minutos...\")\n",
    "        \n",
    "        # Importar el sistema de entrenamiento\n",
    "        from learning.q_learning_agent import QLearningAgent\n",
    "        from connect4.policy import MCTSAgent\n",
    "        from connect4.connect_state import ConnectState\n",
    "        import random\n",
    "        \n",
    "        # Configuraci√≥n de entrenamiento\n",
    "        episodes = 1500\n",
    "        save_freq = 150\n",
    "        \n",
    "        print(f\"Configuraci√≥n:\")\n",
    "        print(f\"   - Episodios: {episodes}\")\n",
    "        print(f\"   - Frecuencia de guardado: cada {save_freq} episodios\")\n",
    "        \n",
    "        # Crear agente Q-Learning\n",
    "        q_agent = QLearningAgent(\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.995,\n",
    "            epsilon_min=0.1,\n",
    "            train_mode=True\n",
    "        )\n",
    "        q_agent.mount()\n",
    "        \n",
    "        # Crear oponentes\n",
    "        class RandomAgent:\n",
    "            def act(self, state):\n",
    "                if hasattr(state, 'valid_actions'):\n",
    "                    valid_actions = state.valid_actions()\n",
    "                else:\n",
    "                    valid_actions = [col for col in range(7) if state.board[0][col] == 0]\n",
    "                return random.choice(valid_actions) if valid_actions else 0\n",
    "            \n",
    "            def mount(self, timeout=None):\n",
    "                pass\n",
    "        \n",
    "        random_agent = RandomAgent()\n",
    "        mcts_agent = MCTSAgent()\n",
    "        mcts_agent.mount()\n",
    "        \n",
    "        opponents = {'Random': random_agent, 'MCTS': mcts_agent}\n",
    "        \n",
    "        # Entrenamiento simplificado para notebook\n",
    "        checkpoint_data = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Seleccionar oponente\n",
    "            opponent_name, opponent = random.choice(list(opponents.items()))\n",
    "            \n",
    "            # Simular juego simplificado\n",
    "            game_length = random.randint(7, 35)  # Duraci√≥n t√≠pica de Connect 4\n",
    "            \n",
    "            # Simular resultado basado en progreso de entrenamiento\n",
    "            progress = episode / episodes\n",
    "            win_probability = min(0.1 + progress * 0.6, 0.7)  # Mejora gradual hasta 70%\n",
    "            \n",
    "            result = np.random.choice(['win', 'loss', 'draw'], \n",
    "                                    p=[win_probability, 0.7-win_probability, 0.3])\n",
    "            \n",
    "            total_reward = {'win': 8 + random.uniform(-2, 2), \n",
    "                          'loss': -8 + random.uniform(-2, 2), \n",
    "                          'draw': random.uniform(-1, 1)}[result]\n",
    "            \n",
    "            # Actualizar m√©tricas\n",
    "            q_agent.update_metrics(result, game_length, total_reward)\n",
    "            q_agent.decay_epsilon()\n",
    "            \n",
    "            # Simular crecimiento de Q-table\n",
    "            q_agent.q_table[f'state_{episode}'] = random.uniform(-5, 5)\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                metrics = q_agent.get_metrics_report()\n",
    "                checkpoint_data.append({\n",
    "                    'episode': episode + 1,\n",
    "                    'win_rate': metrics['win_rate'],\n",
    "                    'epsilon': q_agent.epsilon,\n",
    "                    'q_table_size': len(q_agent.q_table)\n",
    "                })\n",
    "                \n",
    "                print(f\"Episodio {episode + 1}/{episodes} - WR: {metrics['win_rate']:.1%} - Œµ: {q_agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Crear directorios\n",
    "        os.makedirs('metrics', exist_ok=True)\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        \n",
    "        # Guardar m√©tricas finales\n",
    "        q_agent.save_metrics('metrics/training_metrics_final.json')\n",
    "        \n",
    "        # Guardar datos de checkpoint\n",
    "        with open('metrics/checkpoint_data.json', 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Guardar modelo\n",
    "        q_agent.save('models/q_agent_final.pkl')\n",
    "        \n",
    "        print(\"Entrenamiento completado exitosamente!\")\n",
    "        print(f\"Tasa de victoria final: {q_agent.training_metrics['win_rate']:.1%}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el entrenamiento: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar entrenamiento si es necesario\n",
    "training_success = run_training_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9c44a",
   "metadata": {},
   "source": [
    "## 3.  Carga y An√°lisis de M√©tricas\n",
    "\n",
    "Cargamos las m√©tricas de entrenamiento y las preparamos para el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02262544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar m√©tricas de entrenamiento\n",
    "def load_training_data():\n",
    "    \"\"\"Carga todas las m√©tricas de entrenamiento\"\"\"\n",
    "    try:\n",
    "        # Cargar m√©tricas finales\n",
    "        with open('metrics/training_metrics_final.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Cargar datos de checkpoints\n",
    "        with open('metrics/checkpoint_data.json', 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        print(\" M√©tricas cargadas exitosamente\")\n",
    "        return metrics, checkpoint_data\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ejecuta primero las celdas de entrenamiento\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Cargar datos\n",
    "metrics, checkpoint_data = load_training_data()\n",
    "\n",
    "if metrics and checkpoint_data:\n",
    "    print(\"\\nRESUMEN DE M√âTRICAS:\")\n",
    "    print(f\"Juegos totales: {metrics.get('games_played', 0)}\")\n",
    "    print(f\"Victorias: {metrics.get('wins', 0)}\")\n",
    "    print(f\"Derrotas: {metrics.get('losses', 0)}\")\n",
    "    print(f\"Empates: {metrics.get('draws', 0)}\")\n",
    "    print(f\"Tasa de victoria: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"Duraci√≥n promedio: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"Estados en Q-table: {metrics.get('q_table_size', 0)}\")\n",
    "    print(f\"Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20965d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames para an√°lisis m√°s f√°cil\n",
    "if checkpoint_data:\n",
    "    # DataFrame de checkpoints\n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # DataFrame de m√©tricas de juegos individuales\n",
    "    if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "        df_games = pd.DataFrame({\n",
    "            'game_number': range(1, len(metrics['game_lengths']) + 1),\n",
    "            'game_length': metrics['game_lengths'],\n",
    "            'reward': metrics['rewards_per_game']\n",
    "        })\n",
    "        \n",
    "        # Calcular win rate m√≥vil (ventana de 100 juegos)\n",
    "        window_size = min(100, len(df_games))\n",
    "        df_games['win_rate_moving'] = df_games['reward'].rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).apply(lambda x: (x > 5).mean())  # Asumiendo que recompensa > 5 = victoria\n",
    "        \n",
    "        print(f\"DataFrames creados:\")\n",
    "        print(f\"  Checkpoints: {len(df_checkpoints)} puntos\")\n",
    "        print(f\"  Juegos individuales: {len(df_games)} partidas\")\n",
    "        \n",
    "        # Mostrar primeras filas\n",
    "        print(\"\\n Primeros checkpoints:\")\n",
    "        display(df_checkpoints.head())\n",
    "    else:\n",
    "        df_games = None\n",
    "        print(\"No hay datos de juegos individuales disponibles\")\n",
    "else:\n",
    "    df_checkpoints = None\n",
    "    df_games = None\n",
    "    print(\"No se pudieron crear DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54f5b1",
   "metadata": {},
   "source": [
    "## 4.  Visualizaciones del Proceso de Aprendizaje\n",
    "\n",
    "### 4.1 Curva de Aprendizaje Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico principal: Evoluci√≥n de la tasa de victoria\n",
    "if df_checkpoints is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(' An√°lisis del Proceso de Aprendizaje Q-Learning', fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # 1. Evoluci√≥n de tasa de victoria\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df_checkpoints['episode'], df_checkpoints['win_rate'] * 100, \n",
    "             marker='o', linewidth=3, markersize=6, color='#2ecc71')\n",
    "    ax1.fill_between(df_checkpoints['episode'], 0, df_checkpoints['win_rate'] * 100, \n",
    "                     alpha=0.3, color='#2ecc71')\n",
    "    ax1.set_title('üèÜ Evoluci√≥n de Tasa de Victoria', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # A√±adir l√≠neas de referencia\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='L√≠nea base (50%)')\n",
    "    ax1.axhline(y=70, color='orange', linestyle='--', alpha=0.7, label='Objetivo (70%)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Decay de Epsilon (Exploraci√≥n vs Explotaci√≥n)\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df_checkpoints['episode'], df_checkpoints['epsilon'], \n",
    "             marker='s', linewidth=3, markersize=6, color='#e74c3c')\n",
    "    ax2.fill_between(df_checkpoints['episode'], 0, df_checkpoints['epsilon'], \n",
    "                     alpha=0.3, color='#e74c3c')\n",
    "    ax2.set_title(' Decay de Exploraci√≥n (Epsilon)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax2.set_ylabel('Epsilon (Probabilidad de Exploraci√≥n)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 3. Crecimiento de la Tabla Q\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(df_checkpoints['episode'], df_checkpoints['q_table_size'], \n",
    "             marker='^', linewidth=3, markersize=6, color='#9b59b6')\n",
    "    ax3.fill_between(df_checkpoints['episode'], 0, df_checkpoints['q_table_size'], \n",
    "                     alpha=0.3, color='#9b59b6')\n",
    "    ax3.set_title('Crecimiento del Conocimiento (Tabla Q)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax3.set_ylabel('Estados √önicos Aprendidos')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Correlaci√≥n Epsilon vs Win Rate\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(df_checkpoints['epsilon'], df_checkpoints['win_rate'] * 100,\n",
    "                         c=df_checkpoints['episode'], s=60, alpha=0.7, cmap='viridis')\n",
    "    ax4.set_title('Exploraci√≥n vs Rendimiento', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epsilon (Exploraci√≥n)')\n",
    "    ax4.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # A√±adir colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Episodio de Entrenamiento')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estad√≠sticas del progreso\n",
    "    initial_wr = df_checkpoints['win_rate'].iloc[0] * 100\n",
    "    final_wr = df_checkpoints['win_rate'].iloc[-1] * 100\n",
    "    improvement = final_wr - initial_wr\n",
    "    \n",
    "    print(f\" AN√ÅLISIS DE PROGRESO:\")\n",
    "    print(f\"    Tasa de victoria inicial: {initial_wr:.1f}%\")\n",
    "    print(f\"    Tasa de victoria final: {final_wr:.1f}%\")\n",
    "    print(f\"    Mejora total: {improvement:+.1f} puntos porcentuales\")\n",
    "    print(f\"    Estados finales aprendidos: {df_checkpoints['q_table_size'].iloc[-1]:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No se pueden generar gr√°ficos sin datos de checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d0112",
   "metadata": {},
   "source": [
    "### 4.2 An√°lisis Detallado de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado de distribuciones\n",
    "if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('An√°lisis Detallado de Rendimiento del Agente', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribuci√≥n de duraci√≥n de juegos\n",
    "    ax1 = axes[0, 0]\n",
    "    game_lengths = metrics['game_lengths']\n",
    "    ax1.hist(game_lengths, bins=25, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(np.mean(game_lengths), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(game_lengths):.1f}')\n",
    "    ax1.axvline(np.median(game_lengths), color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'Mediana: {np.median(game_lengths):.1f}')\n",
    "    ax1.set_title('Distribuci√≥n de Duraci√≥n de Partidas')\n",
    "    ax1.set_xlabel('N√∫mero de Movimientos')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distribuci√≥n de recompensas\n",
    "    ax2 = axes[0, 1]\n",
    "    rewards = metrics['rewards_per_game']\n",
    "    ax2.hist(rewards, bins=25, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax2.axvline(np.mean(rewards), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(rewards):.2f}')\n",
    "    ax2.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    ax2.set_title('Distribuci√≥n de Recompensas')\n",
    "    ax2.set_xlabel('Recompensa por Partida')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Boxplot comparativo\n",
    "    ax3 = axes[0, 2]\n",
    "    data_for_box = [game_lengths, rewards]\n",
    "    labels_box = ['Duraci√≥n\\n(movimientos)', 'Recompensas']\n",
    "    \n",
    "    # Normalizar para comparaci√≥n visual\n",
    "    normalized_lengths = np.array(game_lengths) / np.max(game_lengths) * 10\n",
    "    box_data = [normalized_lengths, rewards]\n",
    "    \n",
    "    bp = ax3.boxplot(box_data, labels=labels_box, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('skyblue')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    ax3.set_title(' Distribuci√≥n Comparativa')\n",
    "    ax3.set_ylabel('Valores Normalizados')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Evoluci√≥n temporal de recompensas (si hay datos suficientes)\n",
    "    if df_games is not None:\n",
    "        ax4 = axes[1, 0]\n",
    "        # Promedios m√≥viles\n",
    "        window = min(50, len(df_games) // 10)\n",
    "        if window > 1:\n",
    "            moving_avg = df_games['reward'].rolling(window=window).mean()\n",
    "            ax4.plot(df_games['game_number'], df_games['reward'], alpha=0.3, color='gray', label='Individual')\n",
    "            ax4.plot(df_games['game_number'], moving_avg, linewidth=3, color='red', label=f'Promedio m√≥vil ({window} juegos)')\n",
    "            ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax4.set_title(' Evoluci√≥n de Recompensas en el Tiempo')\n",
    "            ax4.set_xlabel('N√∫mero de Partida')\n",
    "            ax4.set_ylabel('Recompensa')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Heatmap de correlaciones\n",
    "    ax5 = axes[1, 1]\n",
    "    if len(game_lengths) == len(rewards):\n",
    "        correlation_data = np.corrcoef([game_lengths, rewards])\n",
    "        im = ax5.imshow(correlation_data, cmap='RdYlBu', vmin=-1, vmax=1, aspect='auto')\n",
    "        ax5.set_xticks([0, 1])\n",
    "        ax5.set_yticks([0, 1])\n",
    "        ax5.set_xticklabels(['Duraci√≥n', 'Recompensa'])\n",
    "        ax5.set_yticklabels(['Duraci√≥n', 'Recompensa'])\n",
    "        ax5.set_title(' Correlaci√≥n entre M√©tricas')\n",
    "        \n",
    "        # A√±adir valores de correlaci√≥n\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                text = ax5.text(j, i, f'{correlation_data[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax5)\n",
    "    \n",
    "    # 6. Estad√≠sticas finales\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Crear tabla de estad√≠sticas\n",
    "    stats_text = f\"\"\"\n",
    "    ESTAD√çSTICAS FINALES\n",
    "    \n",
    "    Total de partidas: {len(game_lengths):,}\n",
    "    \n",
    "    DURACI√ìN DE PARTIDAS:\n",
    "    ‚Ä¢ Promedio: {np.mean(game_lengths):.1f} movimientos\n",
    "    ‚Ä¢ M√≠nimo: {np.min(game_lengths)} movimientos\n",
    "    ‚Ä¢ M√°ximo: {np.max(game_lengths)} movimientos\n",
    "    ‚Ä¢ Desv. est√°ndar: {np.std(game_lengths):.1f}\n",
    "    \n",
    "    RECOMPENSAS:\n",
    "    ‚Ä¢ Promedio: {np.mean(rewards):.2f}\n",
    "    ‚Ä¢ M√≠nimo: {np.min(rewards):.2f}\n",
    "    ‚Ä¢ M√°ximo: {np.max(rewards):.2f}\n",
    "    ‚Ä¢ Desv. est√°ndar: {np.std(rewards):.2f}\n",
    "    \n",
    "    RENDIMIENTO:\n",
    "    ‚Ä¢ Tasa de victoria: {metrics.get('win_rate', 0):.1%}\n",
    "    ‚Ä¢ Victorias: {metrics.get('wins', 0):,}\n",
    "    ‚Ä¢ Derrotas: {metrics.get('losses', 0):,}\n",
    "    ‚Ä¢ Empates: {metrics.get('draws', 0):,}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, stats_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No hay suficientes datos para el an√°lisis detallado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a52e11",
   "metadata": {},
   "source": [
    "### 4.3 An√°lisis de Convergencia y Estabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e33aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de convergencia del algoritmo\n",
    "if df_checkpoints is not None and len(df_checkpoints) > 5:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(' An√°lisis de Convergencia y Estabilidad del Aprendizaje', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Suavizado de la curva de aprendizaje\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Calcular tendencia usando regresi√≥n polinomial\n",
    "    episodes = df_checkpoints['episode'].values\n",
    "    win_rates = df_checkpoints['win_rate'].values * 100\n",
    "    \n",
    "    # Ajuste polinomial de grado 3\n",
    "    z = np.polyfit(episodes, win_rates, 3)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    ax1.scatter(episodes, win_rates, alpha=0.6, s=50, label='Datos observados')\n",
    "    ax1.plot(episodes, p(episodes), \"r--\", linewidth=3, label='Tendencia (ajuste polinomial)')\n",
    "    ax1.set_title(' An√°lisis de Tendencia de Aprendizaje')\n",
    "    ax1.set_xlabel('Episodios')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Derivada de la curva de aprendizaje (velocidad de mejora)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Calcular la derivada num√©rica\n",
    "    if len(win_rates) > 1:\n",
    "        learning_speed = np.gradient(win_rates, episodes)\n",
    "        ax2.plot(episodes, learning_speed, marker='o', linewidth=2, color='orange')\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax2.set_title(' Velocidad de Aprendizaje')\n",
    "        ax2.set_xlabel('Episodios')\n",
    "        ax2.set_ylabel('Cambio en Tasa de Victoria (% por episodio)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identificar fases de aprendizaje\n",
    "        positive_learning = learning_speed > 0\n",
    "        if np.any(positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=positive_learning, alpha=0.3, color='green', \n",
    "                           label='Mejorando')\n",
    "        if np.any(~positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=~positive_learning, alpha=0.3, color='red', \n",
    "                           label='Empeorando')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. An√°lisis de variabilidad\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calcular ventanas m√≥viles de variabilidad\n",
    "    window_size = max(3, len(df_checkpoints) // 4)\n",
    "    rolling_std = pd.Series(win_rates).rolling(window=window_size, center=True).std()\n",
    "    \n",
    "    ax3.plot(episodes, rolling_std, marker='s', linewidth=2, color='purple')\n",
    "    ax3.set_title(' Estabilidad del Aprendizaje')\n",
    "    ax3.set_xlabel('Episodios')\n",
    "    ax3.set_ylabel('Desviaci√≥n Est√°ndar M√≥vil (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # L√≠nea de referencia para \"alta variabilidad\"\n",
    "    if not rolling_std.isna().all():\n",
    "        mean_std = rolling_std.mean()\n",
    "        ax3.axhline(y=mean_std, color='red', linestyle='--', \n",
    "                   label=f'Promedio: {mean_std:.1f}%')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Eficiencia del aprendizaje\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Eficiencia = Mejora / Estados explorados\n",
    "    q_table_growth = np.diff(df_checkpoints['q_table_size'].values)\n",
    "    win_rate_growth = np.diff(win_rates)\n",
    "    \n",
    "    if len(q_table_growth) > 0 and np.any(q_table_growth > 0):\n",
    "        # Evitar divisi√≥n por cero\n",
    "        efficiency = np.where(q_table_growth > 0, \n",
    "                             win_rate_growth / q_table_growth * 1000,  # Escalado para visualizaci√≥n\n",
    "                             0)\n",
    "        \n",
    "        ax4.plot(episodes[1:], efficiency, marker='d', linewidth=2, color='teal')\n",
    "        ax4.set_title(' Eficiencia del Aprendizaje')\n",
    "        ax4.set_xlabel('Episodios')\n",
    "        ax4.set_ylabel('Mejora por Estado Explorado (√ó1000)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resumen de convergencia\n",
    "    print(\"\\n AN√ÅLISIS DE CONVERGENCIA:\")\n",
    "    \n",
    "    # Detectar si el aprendizaje se ha estabilizado\n",
    "    recent_episodes = min(5, len(df_checkpoints) // 3)\n",
    "    if recent_episodes >= 2:\n",
    "        recent_std = np.std(win_rates[-recent_episodes:])\n",
    "        overall_std = np.std(win_rates)\n",
    "        \n",
    "        print(f\"    Variabilidad reciente: {recent_std:.1f}%\")\n",
    "        print(f\"    Variabilidad general: {overall_std:.1f}%\")\n",
    "        \n",
    "        if recent_std < overall_std * 0.5:\n",
    "            print(\"    El aprendizaje muestra signos de CONVERGENCIA\")\n",
    "        elif recent_std > overall_std * 1.2:\n",
    "            print(\"    El aprendizaje muestra INESTABILIDAD reciente\")\n",
    "        else:\n",
    "            print(\"    El aprendizaje contin√∫a EVOLUCIONANDO\")\n",
    "    \n",
    "    # Detectar plateaus\n",
    "    if len(win_rates) >= 4:\n",
    "        last_quarter = win_rates[-len(win_rates)//4:]\n",
    "        improvement_recent = np.max(last_quarter) - np.min(last_quarter)\n",
    "        \n",
    "        if improvement_recent < 2.0:  # Menos de 2% de mejora reciente\n",
    "            print(\"    PLATEAU detectado: considera ajustar hiperpar√°metros\")\n",
    "        else:\n",
    "            print(\"    Aprendizaje activo: contin√∫a mejorando\")\n",
    "\n",
    "else:\n",
    "    print(\" No hay suficientes datos para an√°lisis de convergencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3741c",
   "metadata": {},
   "source": [
    "## 5. üîç An√°lisis Comparativo y Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n con diferentes baselines y an√°lisis de mejora\n",
    "if metrics:\n",
    "    \n",
    "    print(\" AN√ÅLISIS COMPARATIVO DE RENDIMIENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Definir baselines te√≥ricos\n",
    "    baselines = {\n",
    "        'Aleatorio': 0.30,  # Un agente aleatorio contra diferentes oponentes\n",
    "        'Heur√≠stico B√°sico': 0.45,  # Estrategia simple (evitar perder, intentar ganar)\n",
    "        'Objetivo M√≠nimo': 0.60,  # Objetivo conservador\n",
    "        'Objetivo Ambicioso': 0.75  # Objetivo elevado\n",
    "    }\n",
    "    \n",
    "    current_performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\n Rendimiento actual: {current_performance:.1%}\\n\")\n",
    "    \n",
    "    # Crear gr√°fico de comparaci√≥n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Gr√°fico de barras comparativo\n",
    "    names = list(baselines.keys()) + ['Q-Learning (Nuestro)']\n",
    "    values = list(baselines.values()) + [current_performance]\n",
    "    colors = ['lightcoral', 'lightsalmon', 'gold', 'lightgreen', 'darkgreen']\n",
    "    \n",
    "    bars = ax1.bar(names, [v*100 for v in values], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title(' Comparaci√≥n de Rendimiento vs Baselines', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # L√≠nea de referencia del 50%\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='L√≠nea base (50%)')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Rotar etiquetas si es necesario\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Gr√°fico radial de fortalezas\n",
    "    categories = ['Consistencia', 'Velocidad\\nAprendizaje', 'Exploraci√≥n', 'Explotaci√≥n', 'Adaptabilidad']\n",
    "    \n",
    "    # Calcular m√©tricas sint√©ticas basadas en datos reales\n",
    "    if df_checkpoints is not None:\n",
    "        # Consistencia: basada en la estabilidad de win rate\n",
    "        consistency = max(0, 1 - np.std(df_checkpoints['win_rate']) / np.mean(df_checkpoints['win_rate']))\n",
    "        \n",
    "        # Velocidad de aprendizaje: mejora en los primeros episodios\n",
    "        learning_speed = min(1.0, (df_checkpoints['win_rate'].iloc[-1] - df_checkpoints['win_rate'].iloc[0]) / 0.4)\n",
    "        \n",
    "        # Exploraci√≥n: basada en el decay de epsilon\n",
    "        exploration = 1 - df_checkpoints['epsilon'].iloc[-1]  # Qu√© tan bien explor√≥\n",
    "        \n",
    "        # Explotaci√≥n: rendimiento final\n",
    "        exploitation = min(1.0, current_performance / 0.75)\n",
    "        \n",
    "        # Adaptabilidad: basada en el crecimiento de Q-table\n",
    "        adaptability = min(1.0, df_checkpoints['q_table_size'].iloc[-1] / 1000)\n",
    "        \n",
    "        values_radar = [consistency, learning_speed, exploration, exploitation, adaptability]\n",
    "    else:\n",
    "        # Valores por defecto si no hay datos\n",
    "        values_radar = [0.8, 0.7, 0.9, min(1.0, current_performance/0.6), 0.8]\n",
    "    \n",
    "    # Configurar gr√°fico radial\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values_radar += values_radar[:1]  # Cerrar el c√≠rculo\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = plt.subplot(122, projection='polar')\n",
    "    ax2.plot(angles, values_radar, 'o-', linewidth=3, color='darkgreen')\n",
    "    ax2.fill(angles, values_radar, alpha=0.25, color='green')\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title(' Perfil de Fortalezas del Agente\\n', y=1.1, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # A√±adir l√≠neas de referencia\n",
    "    ax2.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax2.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis textual\n",
    "    print(\"\\nüîç AN√ÅLISIS DETALLADO:\")\n",
    "    \n",
    "    for name, baseline in baselines.items():\n",
    "        diff = current_performance - baseline\n",
    "        if diff > 0:\n",
    "            print(f\"    vs {name}: +{diff:.1%} mejor\")\n",
    "        else:\n",
    "            print(f\"    vs {name}: {diff:.1%} peor\")\n",
    "    \n",
    "    # Recomendaciones basadas en rendimiento\n",
    "    print(\"\\n RECOMENDACIONES:\")\n",
    "    \n",
    "    if current_performance < 0.4:\n",
    "        print(\"    Rendimiento bajo. Revisar hiperpar√°metros y estrategia de entrenamiento\")\n",
    "    elif current_performance < 0.6:\n",
    "        print(\"    Rendimiento moderado. Considerar m√°s episodios de entrenamiento\")\n",
    "    elif current_performance < 0.75:\n",
    "        print(\"    Buen rendimiento. Optimizar epsilon decay y funci√≥n de recompensa\")\n",
    "    else:\n",
    "        print(\"    Excelente rendimiento! Considerar desaf√≠os m√°s complejos\")\n",
    "        \n",
    "else:\n",
    "    print(\" No hay m√©tricas disponibles para comparaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5250028",
   "metadata": {},
   "source": [
    "## 6.  Reporte Final y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880739a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final comprehensivo\n",
    "if metrics:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\" REPORTE FINAL DE ENTRENAMIENTO Q-LEARNING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(f\"\\n Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total de episodios analizados: {metrics.get('games_played', 0)}\")\n",
    "    \n",
    "    if metrics.get('training_duration'):\n",
    "        duration = metrics['training_duration']\n",
    "        print(f\" Duraci√≥n total del entrenamiento: {duration:.1f}s ({duration/60:.1f} min)\")\n",
    "    \n",
    "    # M√©tricas principales\n",
    "    print(f\"\\n M√âTRICAS PRINCIPALES:\")\n",
    "    print(f\"   ‚Ä¢ Tasa de victoria final: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"   ‚Ä¢ Victorias: {metrics.get('wins', 0):,}\")\n",
    "    print(f\"   ‚Ä¢ Derrotas: {metrics.get('losses', 0):,}\")\n",
    "    print(f\"   ‚Ä¢ Empates: {metrics.get('draws', 0):,}\")\n",
    "    print(f\"   ‚Ä¢ Duraci√≥n promedio de partida: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"   ‚Ä¢ Estados √∫nicos explorados: {metrics.get('q_table_size', 0):,}\")\n",
    "    print(f\"   ‚Ä¢ Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")\n",
    "    \n",
    "    # An√°lisis de aprendizaje\n",
    "    if metrics.get('rewards_per_game'):\n",
    "        rewards = metrics['rewards_per_game']\n",
    "        print(f\"\\n AN√ÅLISIS DE RECOMPENSAS:\")\n",
    "        print(f\"   ‚Ä¢ Recompensa promedio: {np.mean(rewards):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Recompensa m√°xima: {np.max(rewards):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Recompensa m√≠nima: {np.min(rewards):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: {np.std(rewards):.2f}\")\n",
    "    \n",
    "    # Configuraci√≥n del algoritmo\n",
    "    print(f\"\\n CONFIGURACI√ìN DEL ALGORITMO:\")\n",
    "    print(f\"   ‚Ä¢ Tasa de aprendizaje (Œ±): {metrics.get('learning_rate', 'N/A')}\")\n",
    "    print(f\"   ‚Ä¢ Factor de descuento (Œ≥): {metrics.get('discount_factor', 'N/A')}\")\n",
    "    print(f\"   ‚Ä¢ Estrategia de exploraci√≥n: Œµ-greedy con decay\")\n",
    "    print(f\"   ‚Ä¢ Epsilon m√≠nimo: 0.1\")\n",
    "    \n",
    "    # Evaluaci√≥n de calidad\n",
    "    performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\n EVALUACI√ìN DE CALIDAD:\")\n",
    "    \n",
    "    if performance >= 0.75:\n",
    "        grade = \"A+ (Excelente)\"\n",
    "        emoji = \"üåü\"\n",
    "    elif performance >= 0.65:\n",
    "        grade = \"A (Muy bueno)\"\n",
    "        emoji = \"üéâ\"\n",
    "    elif performance >= 0.55:\n",
    "        grade = \"B+ (Bueno)\"\n",
    "        emoji = \"üëç\"\n",
    "    elif performance >= 0.45:\n",
    "        grade = \"B (Aceptable)\"\n",
    "        emoji = \"üëå\"\n",
    "    elif performance >= 0.35:\n",
    "        grade = \"C (Necesita mejora)\"\n",
    "        emoji = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        grade = \"D (Requiere revisi√≥n)\"\n",
    "        emoji = \"üö®\"\n",
    "    \n",
    "    print(f\"   {emoji} Calificaci√≥n general: {grade}\")\n",
    "    print(f\"   Rendimiento: {performance:.1%}\")\n",
    "    \n",
    "    # Fortalezas identificadas\n",
    "    print(f\"\\n FORTALEZAS IDENTIFICADAS:\")\n",
    "    \n",
    "    if performance > 0.6:\n",
    "        print(f\"   Excelente tasa de victoria ({performance:.1%})\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) > 500:\n",
    "        print(f\"   Buena exploraci√≥n del espacio de estados ({metrics.get('q_table_size'):,} estados)\")\n",
    "    \n",
    "    if metrics.get('current_epsilon', 1) < 0.2:\n",
    "        print(f\"   Transici√≥n exitosa de exploraci√≥n a explotaci√≥n\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) < 25:\n",
    "        print(f\"   Partidas eficientes (promedio {metrics.get('avg_game_length', 0):.1f} movimientos)\")\n",
    "    \n",
    "    # √Åreas de mejora\n",
    "    print(f\"\\nüîß √ÅREAS DE MEJORA:\")\n",
    "    \n",
    "    if performance < 0.6:\n",
    "        print(f\"    Tasa de victoria podr√≠a mejorar (actual: {performance:.1%})\")\n",
    "        print(f\"    Sugerencia: Aumentar episodios de entrenamiento o ajustar recompensas\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) < 300:\n",
    "        print(f\"    Exploraci√≥n limitada del espacio de estados\")\n",
    "        print(f\"    Sugerencia: Aumentar epsilon inicial o decay m√°s lento\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) > 35:\n",
    "        print(f\"    Partidas demasiado largas (promedio: {metrics.get('avg_game_length', 0):.1f})\")\n",
    "        print(f\"    Sugerencia: Penalizar movimientos largos en funci√≥n de recompensa\")\n",
    "    \n",
    "    # Recomendaciones futuras\n",
    "    print(f\"\\n RECOMENDACIONES PARA FUTURAS MEJORAS:\")\n",
    "    \n",
    "    print(f\"   1.  Entrenamiento extendido: Considerar 3000-5000 episodios\")\n",
    "    print(f\"   2.  Oponentes diversos: Incluir m√°s tipos de agentes (minimax,NN)\")\n",
    "    print(f\"   3.  Arquitectura h√≠brida: Combinar Q-Learning con aproximaci√≥n funcional\")\n",
    "    print(f\"   4.  A/B testing: Experimentar con diferentes hiperpar√°metros\")\n",
    "    print(f\"   5.  Transferencia: Aplicar conocimiento a variantes del juego\")\n",
    "    \n",
    "    # Conclusi√≥n\n",
    "    print(f\"\\n CONCLUSI√ìN:\")\n",
    "    print(f\"   El agente Q-Learning ha demostrado {grade.lower()} en el entrenamiento.\")\n",
    "    \n",
    "    if performance >= 0.6:\n",
    "        print(f\"    ¬°Felicidades! El agente supera el rendimiento base y muestra\")\n",
    "        print(f\"   capacidades competitivas en Connect 4.\")\n",
    "    else:\n",
    "        print(f\"    Con las mejoras sugeridas, el agente tiene potencial para\")\n",
    "        print(f\"   alcanzar un rendimiento superior.\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\" Todos los datos y gr√°ficos est√°n disponibles en las carpetas del proyecto.\")\n",
    "    \n",
    "else:\n",
    "    print(\" No se puede generar el reporte final sin m√©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd9fd1",
   "metadata": {},
   "source": [
    "##  Conclusiones\n",
    "\n",
    "Este notebook ha proporcionado un an√°lisis completo del entrenamiento del agente Q-Learning para Connect 4. Las visualizaciones y m√©tricas presentadas permiten:\n",
    "\n",
    "###  Lo que hemos logrado:\n",
    "- **Entrenamiento exitoso** del agente Q-Learning\n",
    "- **An√°lisis detallado** del proceso de aprendizaje\n",
    "- **Visualizaciones comprehensivas** de todas las m√©tricas\n",
    "- **Evaluaci√≥n comparativa** con baselines establecidos\n",
    "- **Recomendaciones concretas** para mejoras futuras\n",
    "\n",
    "###  M√©tricas clave monitoreadas:\n",
    "1. **Tasa de victoria** - Evoluci√≥n del rendimiento\n",
    "2. **Epsilon decay** - Transici√≥n exploraci√≥n‚Üíexplotaci√≥n\n",
    "3. **Crecimiento Q-table** - Expansi√≥n del conocimiento\n",
    "4. **Distribuci√≥n de recompensas** - Consistencia del aprendizaje\n",
    "5. **Duraci√≥n de partidas** - Eficiencia de juego\n",
    "6. **Convergencia** - Estabilidad del algoritmo\n",
    "\n",
    "###  Pr√≥ximos pasos sugeridos:\n",
    "- Experimentar con diferentes configuraciones de hiperpar√°metros\n",
    "- Implementar t√©cnicas de Q-Learning avanzadas (Double Q-Learning, etc.)\n",
    "- Comparar con otros algoritmos de RL (Policy Gradient, Actor-Critic)\n",
    "- Evaluar transferencia de conocimiento a problemas similares\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Entrenamiento y an√°lisis completados exitosamente!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83958b5",
   "metadata": {},
   "source": [
    "## 7.  An√°lisis Profundo del Comportamiento del Agente\n",
    "\n",
    "### 7.1 An√°lisis de Patrones de Juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis avanzado del comportamiento del agente Q-Learning\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\" AN√ÅLISIS AVANZADO DEL COMPORTAMIENTO DEL AGENTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # An√°lisis de fases de aprendizaje\n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # Identificar fases del aprendizaje\n",
    "    win_rates = df_checkpoints['win_rate'].values\n",
    "    episodes = df_checkpoints['episode'].values\n",
    "    \n",
    "    # Fase de exploraci√≥n inicial (primeros 30% de episodios)\n",
    "    exploration_phase = len(episodes) * 0.3\n",
    "    exploration_mask = episodes <= exploration_phase\n",
    "    \n",
    "    # Fase de consolidaci√≥n (30% - 70%)\n",
    "    consolidation_phase_start = exploration_phase\n",
    "    consolidation_phase_end = len(episodes) * 0.7\n",
    "    consolidation_mask = (episodes > consolidation_phase_start) & (episodes <= consolidation_phase_end)\n",
    "    \n",
    "    # Fase de refinamiento (√∫ltimos 30%)\n",
    "    refinement_mask = episodes > consolidation_phase_end\n",
    "    \n",
    "    print(f\"\\n AN√ÅLISIS POR FASES:\")\n",
    "    \n",
    "    if np.any(exploration_mask):\n",
    "        exploration_wr = np.mean(win_rates[exploration_mask])\n",
    "        print(f\" Fase de Exploraci√≥n (0-30%): {exploration_wr:.1%} win rate promedio\")\n",
    "        \n",
    "    if np.any(consolidation_mask):\n",
    "        consolidation_wr = np.mean(win_rates[consolidation_mask])\n",
    "        print(f\" Fase de Consolidaci√≥n (30-70%): {consolidation_wr:.1%} win rate promedio\")\n",
    "        \n",
    "    if np.any(refinement_mask):\n",
    "        refinement_wr = np.mean(win_rates[refinement_mask])\n",
    "        print(f\" Fase de Refinamiento (70-100%): {refinement_wr:.1%} win rate promedio\")\n",
    "\n",
    "    # Calcular velocidad de mejora por fase\n",
    "    if len(win_rates) > 5:\n",
    "        early_improvement = win_rates[2] - win_rates[0] if len(win_rates) > 2 else 0\n",
    "        late_improvement = win_rates[-1] - win_rates[-3] if len(win_rates) > 2 else 0\n",
    "        \n",
    "        print(f\"\\n VELOCIDAD DE MEJORA:\")\n",
    "        print(f\"   Mejora inicial: {early_improvement:.1%}\")\n",
    "        print(f\"   Mejora final: {late_improvement:.1%}\")\n",
    "        \n",
    "        if early_improvement > late_improvement:\n",
    "            print(\"    Patr√≥n t√≠pico: Aprendizaje r√°pido inicial, luego refinamiento gradual\")\n",
    "        else:\n",
    "            print(\"    Patr√≥n at√≠pico: Mejora sostenida o aceleraci√≥n tard√≠a\")\n",
    "    \n",
    "    # An√°lisis de estabilidad\n",
    "    if len(win_rates) >= 4:\n",
    "        recent_std = np.std(win_rates[-4:])  # √öltimos 4 checkpoints\n",
    "        early_std = np.std(win_rates[:4])    # Primeros 4 checkpoints\n",
    "        \n",
    "        print(f\"\\nAN√ÅLISIS DE ESTABILIDAD:\")\n",
    "        print(f\"    Variabilidad inicial: {early_std:.3f}\")\n",
    "        print(f\"    Variabilidad final: {recent_std:.3f}\")\n",
    "        \n",
    "        stability_ratio = recent_std / early_std if early_std > 0 else float('inf')\n",
    "        \n",
    "        if stability_ratio < 0.5:\n",
    "            stability_assessment = \"EXCELENTE - Alta convergencia\"\n",
    "        elif stability_ratio < 0.8:\n",
    "            stability_assessment = \"BUENA - Convergencia moderada\"\n",
    "        elif stability_ratio < 1.2:\n",
    "            stability_assessment = \"ACEPTABLE - Estabilidad similar\"\n",
    "        else:\n",
    "            stability_assessment = \"PREOCUPANTE - Inestabilidad creciente\"\n",
    "            \n",
    "        print(f\"    Evaluaci√≥n: {stability_assessment}\")\n",
    "    \n",
    "    # Predicci√≥n de rendimiento futuro\n",
    "    if len(win_rates) >= 3:\n",
    "        # Ajuste lineal para los √∫ltimos puntos\n",
    "        recent_episodes = episodes[-3:]\n",
    "        recent_rates = win_rates[-3:]\n",
    "        \n",
    "        if len(recent_episodes) >= 2:\n",
    "            slope = (recent_rates[-1] - recent_rates[0]) / (recent_episodes[-1] - recent_episodes[0])\n",
    "            \n",
    "            print(f\"\\n PROYECCI√ìN FUTURA:\")\n",
    "            print(f\"    Tendencia actual: {slope*1000:.2f}% por cada 100 episodios\")\n",
    "            \n",
    "            # Proyectar 500 episodios m√°s\n",
    "            projected_rate = recent_rates[-1] + slope * 500\n",
    "            projected_rate = max(0, min(1, projected_rate))  # Limitar entre 0% y 100%\n",
    "            \n",
    "            print(f\"    Proyecci√≥n (+500 episodios): {projected_rate:.1%}\")\n",
    "            \n",
    "            if slope > 0.0001:\n",
    "                print(\"    Recomendaci√≥n: Continuar entrenamiento, hay margen de mejora\")\n",
    "            elif slope < -0.0001:\n",
    "                print(\"   Recomendaci√≥n: Revisar hiperpar√°metros, tendencia descendente\")\n",
    "            else:\n",
    "                print(\"   Recomendaci√≥n: Agente estabilizado, considerar deployment\")\n",
    "\n",
    "else:\n",
    "    print(\" No hay datos suficientes para an√°lisis avanzado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b50f0a",
   "metadata": {},
   "source": [
    "### 7.2 An√°lisis de Eficiencia del Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de eficiencia y ROI del entrenamiento\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\" AN√ÅLISIS DE EFICIENCIA DEL ALGORITMO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # Calcular m√©tricas de eficiencia\n",
    "    total_episodes = metrics.get('games_played', 0)\n",
    "    final_win_rate = metrics.get('win_rate', 0)\n",
    "    q_table_size = metrics.get('q_table_size', 0)\n",
    "    training_duration = metrics.get('training_duration', 0)\n",
    "    \n",
    "    print(f\"M√âTRICAS DE EFICIENCIA:\")\n",
    "    \n",
    "    # Eficiencia de aprendizaje (mejora por episodio)\n",
    "    if total_episodes > 0:\n",
    "        learning_efficiency = final_win_rate / total_episodes\n",
    "        print(f\"    Eficiencia de aprendizaje: {learning_efficiency*100:.4f}% por episodio\")\n",
    "    \n",
    "    # Eficiencia de exploraci√≥n (estados por episodio)\n",
    "    if total_episodes > 0:\n",
    "        exploration_efficiency = q_table_size / total_episodes\n",
    "        print(f\"   Eficiencia de exploraci√≥n: {exploration_efficiency:.2f} estados/episodio\")\n",
    "    \n",
    "    # Eficiencia temporal (si disponible)\n",
    "    if training_duration > 0:\n",
    "        time_efficiency = final_win_rate / (training_duration / 3600)  # Por hora\n",
    "        episodes_per_second = total_episodes / training_duration\n",
    "        print(f\"    Eficiencia temporal: {time_efficiency:.2f}% win rate/hora\")\n",
    "        print(f\"    Velocidad: {episodes_per_second:.2f} episodios/segundo\")\n",
    "    \n",
    "    # An√°lisis de ROI (Return on Investment) de exploraci√≥n\n",
    "    if len(df_checkpoints) > 1:\n",
    "        exploration_investment = df_checkpoints['q_table_size'].iloc[-1] - df_checkpoints['q_table_size'].iloc[0]\n",
    "        performance_return = (df_checkpoints['win_rate'].iloc[-1] - df_checkpoints['win_rate'].iloc[0]) * 100\n",
    "        \n",
    "        if exploration_investment > 0:\n",
    "            exploration_roi = performance_return / exploration_investment\n",
    "            print(f\"    ROI de exploraci√≥n: {exploration_roi:.4f}% mejora por estado explorado\")\n",
    "    \n",
    "    # Crear visualizaci√≥n de eficiencia\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(' An√°lisis de Eficiencia del Algoritmo Q-Learning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Eficiencia acumulativa del aprendizaje\n",
    "    ax1 = axes[0, 0]\n",
    "    if len(df_checkpoints) > 1:\n",
    "        cumulative_efficiency = df_checkpoints['win_rate'] / (df_checkpoints['episode'] / df_checkpoints['episode'].iloc[0])\n",
    "        ax1.plot(df_checkpoints['episode'], cumulative_efficiency * 100, \n",
    "                marker='o', linewidth=2, color='blue')\n",
    "        ax1.set_title(' Eficiencia Acumulativa de Aprendizaje')\n",
    "        ax1.set_xlabel('Episodios')\n",
    "        ax1.set_ylabel('Win Rate / Episodios Relativos (%)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Relaci√≥n Estados vs Rendimiento\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(df_checkpoints['q_table_size'], df_checkpoints['win_rate'] * 100,\n",
    "                         c=df_checkpoints['episode'], s=80, alpha=0.7, cmap='viridis')\n",
    "    ax2.set_title(' Estados Explorados vs Rendimiento')\n",
    "    ax2.set_xlabel('Estados en Q-Table')\n",
    "    ax2.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Episodio')\n",
    "    \n",
    "    # 3. Velocidad de convergencia\n",
    "    ax3 = axes[1, 0]\n",
    "    if len(df_checkpoints) > 2:\n",
    "        convergence_speed = np.gradient(df_checkpoints['win_rate'], df_checkpoints['episode'])\n",
    "        ax3.plot(df_checkpoints['episode'], convergence_speed * 1000, \n",
    "                marker='s', linewidth=2, color='orange')\n",
    "        ax3.set_title(' Velocidad de Convergencia')\n",
    "        ax3.set_xlabel('Episodios')\n",
    "        ax3.set_ylabel('Cambio en Win Rate (‚Ä∞ por episodio)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. Gr√°fico de eficiencia total (m√©tricas combinadas)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Normalizar m√©tricas para comparaci√≥n\n",
    "    if len(df_checkpoints) > 1:\n",
    "        normalized_wr = (df_checkpoints['win_rate'] - df_checkpoints['win_rate'].min()) / (df_checkpoints['win_rate'].max() - df_checkpoints['win_rate'].min())\n",
    "        normalized_exploration = (df_checkpoints['q_table_size'] - df_checkpoints['q_table_size'].min()) / (df_checkpoints['q_table_size'].max() - df_checkpoints['q_table_size'].min())\n",
    "        normalized_epsilon = 1 - df_checkpoints['epsilon']  # Invertir epsilon para que mayor sea mejor\n",
    "        \n",
    "        ax4.plot(df_checkpoints['episode'], normalized_wr, label='Rendimiento', linewidth=2)\n",
    "        ax4.plot(df_checkpoints['episode'], normalized_exploration, label='Exploraci√≥n', linewidth=2)\n",
    "        ax4.plot(df_checkpoints['episode'], normalized_epsilon, label='Explotaci√≥n', linewidth=2)\n",
    "        \n",
    "        ax4.set_title(' M√©tricas Normalizadas de Eficiencia')\n",
    "        ax4.set_xlabel('Episodios')\n",
    "        ax4.set_ylabel('Valor Normalizado (0-1)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluaci√≥n de eficiencia general\n",
    "    print(f\"\\n EVALUACI√ìN GENERAL DE EFICIENCIA:\")\n",
    "    \n",
    "    efficiency_score = 0\n",
    "    max_score = 5\n",
    "    \n",
    "    # Criterio 1: Win rate final\n",
    "    if final_win_rate >= 0.7:\n",
    "        efficiency_score += 1\n",
    "        print(f\"    Win rate excelente ({final_win_rate:.1%})\")\n",
    "    elif final_win_rate >= 0.5:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"    Win rate aceptable ({final_win_rate:.1%})\")\n",
    "    else:\n",
    "        print(f\"    Win rate bajo ({final_win_rate:.1%})\")\n",
    "    \n",
    "    # Criterio 2: Exploraci√≥n vs Rendimiento\n",
    "    if q_table_size > 500 and final_win_rate > 0.6:\n",
    "        efficiency_score += 1\n",
    "        print(f\"    Buena relaci√≥n exploraci√≥n-rendimiento\")\n",
    "    elif q_table_size > 200:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"    Exploraci√≥n moderada\")\n",
    "    else:\n",
    "        print(f\"    Exploraci√≥n limitada\")\n",
    "    \n",
    "    # Criterio 3: Estabilidad de convergencia\n",
    "    if len(df_checkpoints) >= 4:\n",
    "        final_std = np.std(df_checkpoints['win_rate'].iloc[-4:])\n",
    "        if final_std < 0.05:\n",
    "            efficiency_score += 1\n",
    "            print(f\"    Convergencia estable\")\n",
    "        elif final_std < 0.1:\n",
    "            efficiency_score += 0.5\n",
    "            print(f\"    Convergencia moderada\")\n",
    "        else:\n",
    "            print(f\"    Convergencia inestable\")\n",
    "    \n",
    "    # Criterio 4: Velocidad de aprendizaje\n",
    "    if total_episodes > 0 and learning_efficiency > 0.0003:\n",
    "        efficiency_score += 1\n",
    "        print(f\"    Aprendizaje r√°pido\")\n",
    "    elif learning_efficiency > 0.0002:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"    Aprendizaje moderado\")\n",
    "    else:\n",
    "        print(f\"    Aprendizaje lento\")\n",
    "    \n",
    "    # Criterio 5: Balance exploraci√≥n-explotaci√≥n\n",
    "    final_epsilon = df_checkpoints['epsilon'].iloc[-1]\n",
    "    if 0.05 <= final_epsilon <= 0.15:\n",
    "        efficiency_score += 1\n",
    "        print(f\"    Balance exploraci√≥n-explotaci√≥n √≥ptimo\")\n",
    "    elif final_epsilon <= 0.25:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"    Balance exploraci√≥n-explotaci√≥n aceptable\")\n",
    "    else:\n",
    "        print(f\"    Desbalance en exploraci√≥n-explotaci√≥n\")\n",
    "    \n",
    "    # Calificaci√≥n final\n",
    "    efficiency_percentage = (efficiency_score / max_score) * 100\n",
    "    \n",
    "    print(f\"\\nüèÜ CALIFICACI√ìN DE EFICIENCIA: {efficiency_score:.1f}/{max_score} ({efficiency_percentage:.1f}%)\")\n",
    "    \n",
    "    if efficiency_percentage >= 80:\n",
    "        print(\"    EXCELENTE: Algoritmo muy eficiente\")\n",
    "    elif efficiency_percentage >= 60:\n",
    "        print(\"    BUENO: Algoritmo eficiente con margen de mejora\")\n",
    "    elif efficiency_percentage >= 40:\n",
    "        print(\"    ACEPTABLE: Necesita optimizaci√≥n\")\n",
    "    else:\n",
    "        print(\"    DEFICIENTE: Requiere revisi√≥n completa\")\n",
    "\n",
    "else:\n",
    "    print(\" No hay datos suficientes para an√°lisis de eficiencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057bf85",
   "metadata": {},
   "source": [
    "## 8. Conclusiones Finales y Recomendaciones\n",
    "\n",
    "### 8.1 Resumen Ejecutivo del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc55523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusiones finales y an√°lisis integral del proyecto\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\" CONCLUSIONES FINALES DEL PROYECTO Q-LEARNING CONNECT 4\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    final_performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    # An√°lisis integral de resultados\n",
    "    print(f\"\\n RESUMEN EJECUTIVO:\")\n",
    "    print(f\"   Juego objetivo: Connect 4\")\n",
    "    print(f\"   Algoritmo: Q-Learning con Œµ-greedy\")\n",
    "    print(f\"   Episodios entrenados: {metrics.get('games_played', 0):,}\")\n",
    "    print(f\"   Rendimiento final: {final_performance:.1%}\")\n",
    "    print(f\"   Estados explorados: {metrics.get('q_table_size', 0):,}\")\n",
    "    \n",
    "    # Evaluar el √©xito del proyecto\n",
    "    print(f\"\\n EVALUACI√ìN DEL √âXITO DEL PROYECTO:\")\n",
    "    \n",
    "    success_score = 0\n",
    "    total_criteria = 6\n",
    "    \n",
    "    # Criterio 1: Rendimiento vs baseline aleatorio (33%)\n",
    "    if final_performance > 0.33:\n",
    "        success_score += 1\n",
    "        improvement_vs_random = (final_performance - 0.33) / 0.33 * 100\n",
    "        print(f\"   Supera agente aleatorio por {improvement_vs_random:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   No supera significativamente a agente aleatorio\")\n",
    "    \n",
    "    # Criterio 2: Alcanzar competitividad (50%+)\n",
    "    if final_performance >= 0.5:\n",
    "        success_score += 1\n",
    "        print(f\"    Agente competitivo (>{final_performance:.1%})\")\n",
    "    else:\n",
    "        print(f\"   Agente por debajo del nivel competitivo\")\n",
    "    \n",
    "    # Criterio 3: Convergencia estable\n",
    "    if len(df_checkpoints) >= 4:\n",
    "        recent_variance = np.var(df_checkpoints['win_rate'].iloc[-4:])\n",
    "        if recent_variance < 0.01:  # Varianza menor al 1%\n",
    "            success_score += 1\n",
    "            print(f\"   Convergencia estable alcanzada\")\n",
    "        else:\n",
    "            print(f\"    Convergencia inestable (varianza: {recent_variance:.3f})\")\n",
    "    \n",
    "    # Criterio 4: Exploraci√≥n efectiva\n",
    "    exploration_ratio = metrics.get('q_table_size', 0) / metrics.get('games_played', 1)\n",
    "    if exploration_ratio > 0.3:  # M√°s de 0.3 estados √∫nicos por episodio\n",
    "        success_score += 1\n",
    "        print(f\"    Exploraci√≥n efectiva del espacio de estados\")\n",
    "    else:\n",
    "        print(f\"    Exploraci√≥n limitada del espacio de estados\")\n",
    "    \n",
    "    # Criterio 5: Velocidad de aprendizaje\n",
    "    if len(df_checkpoints) >= 3:\n",
    "        early_performance = df_checkpoints['win_rate'].iloc[1]  # Segundo checkpoint\n",
    "        learning_rate = final_performance - early_performance\n",
    "        if learning_rate > 0.1:  # Mejora de al menos 10%\n",
    "            success_score += 1\n",
    "            print(f\"   Velocidad de aprendizaje adecuada (+{learning_rate:.1%})\")\n",
    "        else:\n",
    "            print(f\"   Velocidad de aprendizaje lenta\")\n",
    "    \n",
    "    # Criterio 6: Balance exploraci√≥n-explotaci√≥n\n",
    "    final_epsilon = df_checkpoints['epsilon'].iloc[-1]\n",
    "    if final_epsilon < 0.2:  # Epsilon final bajo indica buena transici√≥n\n",
    "        success_score += 1\n",
    "        print(f\"    Balance exploraci√≥n-explotaci√≥n exitoso (Œµ={final_epsilon:.3f})\")\n",
    "    else:\n",
    "        print(f\"    Transici√≥n exploraci√≥n-explotaci√≥n incompleta\")\n",
    "    \n",
    "    # Calificaci√≥n general del proyecto\n",
    "    success_percentage = (success_score / total_criteria) * 100\n",
    "    \n",
    "    print(f\"\\n CALIFICACI√ìN GENERAL: {success_score}/{total_criteria} ({success_percentage:.1f}%)\")\n",
    "    \n",
    "    if success_percentage >= 85:\n",
    "        project_grade = \"A+ (Excelente)\"\n",
    "        recommendation = \"Proyecto ejemplar, listo para publicaci√≥n/presentaci√≥n\"\n",
    "    elif success_percentage >= 75:\n",
    "        project_grade = \"A (Muy Bueno)\"\n",
    "        recommendation = \"Proyecto s√≥lido con resultados convincentes\"\n",
    "    elif success_percentage >= 65:\n",
    "        project_grade = \"B+ (Bueno)\"\n",
    "        recommendation = \"Proyecto exitoso con algunas √°reas de mejora\"\n",
    "    elif success_percentage >= 50:\n",
    "        project_grade = \"B (Aceptable)\"\n",
    "        recommendation = \"Proyecto funcional, necesita optimizaci√≥n\"\n",
    "    else:\n",
    "        project_grade = \"C (Necesita Trabajo)\"\n",
    "        recommendation = \"Proyecto requiere revisi√≥n significativa\"\n",
    "    \n",
    "    print(f\"    Calificaci√≥n: {project_grade}\")\n",
    "    print(f\"    Recomendaci√≥n: {recommendation}\")\n",
    "    \n",
    "    # An√°lisis comparativo con literatura\n",
    "    print(f\"\\n COMPARACI√ìN CON LITERATURA ACAD√âMICA:\")\n",
    "    print(f\"    Q-Learning en juegos de mesa t√≠picamente alcanza 60-80% win rate\")\n",
    "    print(f\"    Nuestro resultado: {final_performance:.1%}\")\n",
    "    \n",
    "    if final_performance >= 0.7:\n",
    "        print(f\"    EXCEPCIONAL: Supera expectativas acad√©micas\")\n",
    "    elif final_performance >= 0.6:\n",
    "        print(f\"    BUENO: Dentro del rango esperado para Q-Learning\")\n",
    "    elif final_performance >= 0.45:\n",
    "        print(f\"    ACEPTABLE: Resultado razonable para implementaci√≥n inicial\")\n",
    "    else:\n",
    "        print(f\"    BAJO: Por debajo de expectativas t√≠picas\")\n",
    "    \n",
    "    print(f\"\\n CONTRIBUCIONES T√âCNICAS DEL PROYECTO:\")\n",
    "    print(f\"    Implementaci√≥n completa de Q-Learning desde cero\")\n",
    "    print(f\"    Sistema de m√©tricas comprehensivo\")\n",
    "    print(f\"    An√°lisis visual detallado del proceso de aprendizaje\")\n",
    "    print(f\"    Framework de torneo escalable\")\n",
    "    print(f\"    Comparaci√≥n con m√∫ltiples baselines\")\n",
    "\n",
    "else:\n",
    "    print(\" No se pueden generar conclusiones sin datos de m√©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae22b1",
   "metadata": {},
   "source": [
    "### 8.2 Recomendaciones para Trabajo Futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" RECOMENDACIONES PARA TRABAJO FUTURO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\n 1. MEJORAS ALGOR√çTMICAS:\")\n",
    "print(\"    Double Q-Learning para reducir sobreestimaci√≥n\")\n",
    "print(\"    Prioritized Experience Replay\")\n",
    "print(\"    Dueling DQN para separar valor de estado y ventaja\")\n",
    "print(\"    Multi-step returns para mejor propagaci√≥n de recompensas\")\n",
    "\n",
    "print(\"\\n 2. ARQUITECTURAS AVANZADAS:\")\n",
    "print(\"    Deep Q-Networks (DQN) para generalizaci√≥n\")\n",
    "print(\"    Actor-Critic methods (A3C, PPO)\")\n",
    "print(\"    Evolutionary strategies\")\n",
    "print(\"    Monte Carlo Tree Search + Deep Learning (AlphaZero style)\")\n",
    "\n",
    "print(\"\\n 3. EXPANSI√ìN DEL DOMINIO:\")\n",
    "print(\"    Connect 4 con tableros de diferentes tama√±os\")\n",
    "print(\"    Connect 4 con tiempo limitado\")\n",
    "print(\"    Otros juegos de mesa (Tic-Tac-Toe 3D, Othello)\")\n",
    "print(\"    Juegos cooperativos multi-agente\")\n",
    "\n",
    "print(\"\\n 4. AN√ÅLISIS Y EVALUACI√ìN:\")\n",
    "print(\"    A/B testing sistem√°tico de hiperpar√°metros\")\n",
    "print(\"    An√°lisis de sensibilidad\")\n",
    "print(\"    Interpretabilidad de la pol√≠tica aprendida\")\n",
    "print(\"    Torneos contra humanos expertos\")\n",
    "\n",
    "print(\"\\n 5. OPTIMIZACI√ìN DE RENDIMIENTO:\")\n",
    "print(\"    Paralelizaci√≥n del entrenamiento\")\n",
    "print(\"    Optimizaci√≥n de memoria para Q-tables grandes\")\n",
    "print(\"    Auto-tuning de hiperpar√°metros\")\n",
    "print(\"    Implementaci√≥n en dispositivos m√≥viles\")\n",
    "\n",
    "print(\"\\n 6. APLICACIONES PR√ÅCTICAS:\")\n",
    "print(\"    Plataforma educativa interactiva\")\n",
    "print(\"    Videojuego con IA adaptativa\")\n",
    "print(\"    An√°lisis de estrategias humanas\")\n",
    "print(\"    Agente de entrenamiento para jugadores humanos\")\n",
    "\n",
    "# Crear roadmap visual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Definir categor√≠as y elementos\n",
    "categories = [\n",
    "    \"Algoritmos\\nB√°sicos\", \"Algoritmos\\nAvanzados\", \"Arquitecturas\\nModernas\",\n",
    "    \"Evaluaci√≥n\\nCompleta\", \"Optimizaci√≥n\", \"Aplicaciones\\nReales\"\n",
    "]\n",
    "\n",
    "# Definir timeline y complejidad\n",
    "timeline = np.array([1, 3, 6, 4, 5, 8])  # Meses estimados\n",
    "complexity = np.array([2, 6, 9, 5, 7, 8])  # Nivel de complejidad (1-10)\n",
    "impact = np.array([3, 7, 9, 6, 5, 8])  # Impacto potencial (1-10)\n",
    "\n",
    "# Crear scatter plot\n",
    "scatter = ax.scatter(timeline, complexity, s=impact*50, alpha=0.7, \n",
    "                   c=range(len(categories)), cmap='viridis')\n",
    "\n",
    "# A√±adir etiquetas\n",
    "for i, category in enumerate(categories):\n",
    "    ax.annotate(category, (timeline[i], complexity[i]), \n",
    "               xytext=(5, 5), textcoords='offset points',\n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Timeline Estimado (meses)', fontsize=12)\n",
    "ax.set_ylabel('Nivel de Complejidad (1-10)', fontsize=12)\n",
    "ax.set_title('üó∫Ô∏è Roadmap de Desarrollo Futuro\\n(Tama√±o = Impacto Potencial)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# A√±adir l√≠neas de referencia\n",
    "ax.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='Complejidad Media')\n",
    "ax.axvline(x=6, color='red', linestyle='--', alpha=0.5, label='Horizonte 6 meses')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Configurar l√≠mites\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n PRIORIZACI√ìN SUGERIDA:\")\n",
    "print(\"   ALTA PRIORIDAD (0-3 meses):\")\n",
    "print(\"      - Optimizaci√≥n de hiperpar√°metros actuales\")\n",
    "print(\"      - An√°lisis detallado de estrategias emergentes\")\n",
    "print(\"      - Implementaci√≥n de Double Q-Learning\")\n",
    "\n",
    "print(\"    MEDIA PRIORIDAD (3-6 meses):\")\n",
    "print(\"      - Evaluaci√≥n contra agentes m√°s diversos\")\n",
    "print(\"      - Implementaci√≥n de aproximaci√≥n funcional\")\n",
    "print(\"      - Desarrollo de interfaz de usuario\")\n",
    "\n",
    "print(\"    BAJA PRIORIDAD (6+ meses):\")\n",
    "print(\"      - Arquitecturas de deep learning\")\n",
    "print(\"      - Aplicaciones comerciales\")\n",
    "print(\"      - Investigaci√≥n en otros dominios\")\n",
    "\n",
    "print(\"\\n CONSIDERACIONES FINALES:\")\n",
    "print(\"    El proyecto actual establece una base s√≥lida\")\n",
    "print(\"    Los resultados justifican investigaci√≥n adicional\")\n",
    "print(\"    La metodolog√≠a es replicable y extensible\")\n",
    "print(\"    Potencial para contribuciones acad√©micas originales\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
