# ğŸ¤– Connect 4 AI Tournament - Proyecto de Inteligencia Artificial

## ğŸ“– IntroducciÃ³n

Este proyecto implementa un **sistema completo de Inteligencia Artificial** para el juego **Connect 4** (Conecta 4), utilizando **algoritmos de aprendizaje por refuerzo** y **sistemas de torneo automatizados**. El objetivo principal es desarrollar, entrenar y evaluar agentes inteligentes capaces de competir en partidas de Connect 4 con un alto nivel de rendimiento.

### ğŸ¯ Objetivos del Proyecto

1. **ğŸ§  Implementar Q-Learning desde cero** - Algoritmo fundamental de aprendizaje por refuerzo
2. **ğŸ† Desarrollar sistema de torneos** - Framework para competencias automatizadas entre agentes
3. **ğŸ“Š AnÃ¡lisis profundo de mÃ©tricas** - EvaluaciÃ³n detallada del proceso de aprendizaje
4. **ğŸ® Crear agentes competitivos** - IA capaz de superar estrategias bÃ¡sicas y aleatorias
5. **ğŸ”¬ InvestigaciÃ³n aplicada** - Explorar tÃ©cnicas modernas de IA en juegos de mesa

### ğŸŒŸ CaracterÃ­sticas Principales

- âœ… **Q-Learning Completo**: ImplementaciÃ³n robusta con Îµ-greedy exploration
- âœ… **Sistema de Torneo Automatizado**: Bracket eliminatorio con mÃºltiples agentes
- âœ… **MÃ©tricas Avanzadas**: Seguimiento detallado del progreso de entrenamiento
- âœ… **Visualizaciones Interactivas**: GrÃ¡ficos y anÃ¡lisis en Jupyter Notebooks
- âœ… **Arquitectura Modular**: CÃ³digo reutilizable y extensible
- âœ… **Compatibilidad Gradescope**: Preserva archivos originales para evaluaciÃ³n

### ğŸ”¬ Fundamentos TeÃ³ricos

El proyecto se basa en conceptos fundamentales de **Machine Learning** e **Inteligencia Artificial**:

- **Q-Learning**: Algoritmo de diferencia temporal para aprender polÃ­ticas Ã³ptimas
- **ExploraciÃ³n vs ExplotaciÃ³n**: Balance crÃ­tico en aprendizaje por refuerzo
- **Monte Carlo Tree Search (MCTS)**: Algoritmo de bÃºsqueda para juegos
- **EvaluaciÃ³n de PolÃ­ticas**: MÃ©tricas para medir rendimiento de agentes IA
- **Game Theory**: AnÃ¡lisis estratÃ©gico de interacciones competitivas

## ğŸš€ GuÃ­a de InstalaciÃ³n y EjecuciÃ³n

### ğŸ“‹ Prerrequisitos

1. **Python 3.10+** instalado en el sistema
2. **Conda** o **pip** para gestiÃ³n de paquetes
3. **Jupyter Notebook** para anÃ¡lisis interactivo
4. **Git** para control de versiones (opcional)

### ğŸ› ï¸ InstalaciÃ³n

#### OpciÃ³n 1: Usando Conda (Recomendado)

```bash
# 1. Clonar o descargar el proyecto
git clone <repository-url>
cd tournament/

# 2. Crear entorno conda
conda create -n iaenv python=3.10
conda activate iaenv

# 3. Instalar dependencias
conda install numpy matplotlib seaborn pandas jupyter
pip install pickle-mixin
```

#### OpciÃ³n 2: Usando pip

```bash
# 1. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows

# 2. Instalar dependencias
pip install numpy matplotlib seaborn pandas jupyter
```

### ğŸ® GuÃ­a de EjecuciÃ³n

#### 1. **ğŸ† Ejecutar Torneo Entre Agentes**

```bash
# Activar entorno
conda activate iaenv

# Cambiar al directorio del proyecto
cd tournament/

# Ejecutar torneo
python main.py
```

**Â¿QuÃ© hace?**
- Detecta automÃ¡ticamente todos los agentes en la carpeta `groups/`
- Ejecuta un torneo bracket eliminatorio
- Muestra resultados en tiempo real
- Guarda resultados en `versus/`

**Salida esperada:**
```
ğŸ† Iniciando torneo entre agentes...
ğŸ” Agentes detectados: ['Group A', 'Group B', 'Group C']
ğŸ“‹ Participantes del torneo: ['Group A', 'Group B', 'Group C']
ğŸ¯ Total de participantes: 4
...
ğŸ† Â¡CampeÃ³n del torneo: MCTS-Champion!
```

#### 2. **ğŸ§  Entrenar Agente Q-Learning**

```bash
# Ejecutar entrenamiento
python train_agent.py
```

**Â¿QuÃ© hace?**
- Entrena un agente Q-Learning desde cero
- Juega contra agentes MCTS y aleatorios
- Guarda checkpoints cada 150 episodios
- Genera mÃ©tricas detalladas de progreso

**ConfiguraciÃ³n por defecto:**
- ğŸ® **1500 episodios** de entrenamiento
- ğŸ² **Îµ-greedy** con decay (1.0 â†’ 0.1)
- ğŸ“ˆ **Î± = 0.1** (learning rate)
- ğŸ’° **Î³ = 0.95** (discount factor)

**Salida esperada:**
```
ğŸš€ Iniciando entrenamiento Q-Learning por 1500 episodios...
ğŸ¯ Oponentes: ['Random', 'MCTS']
ğŸ“Š Episodio 150/1500 - WR: 45.3% - Îµ: 0.862
ğŸ“Š Episodio 300/1500 - WR: 58.1% - Îµ: 0.743
...
âœ… Entrenamiento completado!
```

#### 3. **ğŸ“Š AnÃ¡lisis Completo de MÃ©tricas**

```bash
# Iniciar Jupyter Notebook
jupyter notebook

# Abrir en el navegador: Q_Learning_Analysis.ipynb
# Ejecutar todas las celdas: Cell â†’ Run All
```

**Â¿QuÃ© incluye el notebook?**
- ğŸ“ˆ **Curvas de aprendizaje** detalladas
- ğŸ² **AnÃ¡lisis de exploraciÃ³n vs explotaciÃ³n**
- ğŸ“Š **EstadÃ­sticas de rendimiento**
- ğŸ” **AnÃ¡lisis de convergencia**
- ğŸ¯ **Comparaciones con baselines**
- ğŸ“‹ **Conclusiones y recomendaciones**

### ğŸ“ Estructura del Proyecto

```
tournament/
â”œâ”€â”€ ğŸ“„ main.py                     # Script principal del torneo
â”œâ”€â”€ ğŸ“„ tournament.py              # LÃ³gica del sistema de torneo
â”œâ”€â”€ ğŸ“„ train_agent.py             # Entrenamiento del agente Q-Learning
â”œâ”€â”€ ğŸ“Š Q_Learning_Analysis.ipynb  # Notebook completo de anÃ¡lisis
â”‚
â”œâ”€â”€ ğŸ¤– connect4/                  # Motor del juego Connect 4
â”‚   â”œâ”€â”€ ğŸ“„ base_policy.py         # Clase base para polÃ­ticas/agentes
â”‚   â”œâ”€â”€ ğŸ“„ policy.py              # Agente MCTS (ORIGINAL - no modificar)
â”‚   â”œâ”€â”€ ğŸ“„ connect_state.py       # Estado del juego y reglas
â”‚   â”œâ”€â”€ ğŸ“„ environment_state.py   # Clase abstracta de estado
â”‚   â”œâ”€â”€ ğŸ“„ dtos.py                # Tipos de datos del torneo
â”‚   â”œâ”€â”€ ğŸ“„ utils.py               # Utilidades para importar agentes
â”‚   â””â”€â”€ ğŸ“„ __init__.py            # Paquete Python
â”‚
â”œâ”€â”€ ğŸ¯ groups/                    # Agentes participantes del torneo
â”‚   â”œâ”€â”€ ğŸ“ Group A/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ policy.py          # Agente del Grupo A
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ Group B/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ policy.py          # Agente del Grupo B
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ Group C/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ policy.py          # Agente del Grupo C
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”œâ”€â”€ ğŸ§  learning/                  # Sistema de aprendizaje Q-Learning
â”‚   â””â”€â”€ ğŸ“„ q_learning_agent.py   # Agente Q-Learning entrenado
â”‚
â”œâ”€â”€ ğŸ“Š metrics/                   # Datos de entrenamiento y mÃ©tricas
â”‚   â”œâ”€â”€ ğŸ“„ metrics_logger.py     # Logger de mÃ©tricas (legacy)
â”‚   â”œâ”€â”€ ğŸ“„ training_metrics.json # MÃ©tricas del entrenamiento
â”‚   â””â”€â”€ ğŸ“„ q_table*.pkl          # Tablas Q guardadas
â”‚
â””â”€â”€ ğŸ¥‡ versus/                    # Resultados de partidas entre agentes
    â”œâ”€â”€ ğŸ“„ match_Group*_vs_Group*.json  # Resultados de torneos
    â”œâ”€â”€ ğŸ“„ match_MCTS-Champion_vs_*.json
    â””â”€â”€ ğŸ“„ match_Q-Learning-AI_vs_*.json
```

## ğŸš€ CÃ³mo usar el proyecto

### 1. **Ejecutar Torneo**
```bash
python main.py
```

### 2. **Entrenar Agente Q-Learning**
```bash
python train_agent.py
```

### 3. **AnÃ¡lizar Resultados**
- Abrir `Q_Learning_Analysis.ipynb` en Jupyter
- Ejecutar todas las celdas para ver grÃ¡ficos y anÃ¡lisis

## ğŸ¯ Archivos Principales

| Archivo | PropÃ³sito | Estado |
|---------|-----------|--------|
| `main.py` | ğŸ† Ejecuta torneos entre agentes | âœ… Activo |
| `policy.py` | ğŸ¤– Agente MCTS original | âœ… **NO MODIFICAR** (Gradescope) |
| `train_agent.py` | ğŸ“ˆ Entrena agente Q-Learning | âœ… Activo |
| `Q_Learning_Analysis.ipynb` | ğŸ“Š AnÃ¡lisis completo | âœ… Activo |

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ğŸ”§ Personalizar Entrenamiento

Edita `train_agent.py` para modificar parÃ¡metros:

```python
# ConfiguraciÃ³n de entrenamiento
episodes = 2000           # NÃºmero de episodios
save_freq = 200          # Frecuencia de checkpoints
opponents = ['random', 'mcts']  # Tipos de oponentes

# ParÃ¡metros del agente Q-Learning
q_agent = QLearningAgent(
    alpha=0.1,           # Tasa de aprendizaje
    gamma=0.95,          # Factor de descuento
    epsilon=1.0,         # ExploraciÃ³n inicial
    epsilon_decay=0.995, # Velocidad de decay
    epsilon_min=0.1      # ExploraciÃ³n mÃ­nima
)
```

### ğŸ¯ Agregar Nuevos Agentes

1. Crear carpeta en `groups/`: `groups/Mi_Agente/`
2. Crear archivo `policy.py`:

```python
import numpy as np
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from connect4.base_policy import Policy

class MiAgente(Policy):
    def mount(self, timeout=None):
        pass
    
    def act(self, state):
        # Tu lÃ³gica aquÃ­
        valid_actions = state.valid_actions()
        return valid_actions[0]  # Ejemplo simple
```

3. El agente aparecerÃ¡ automÃ¡ticamente en torneos

### ğŸ› SoluciÃ³n de Problemas Comunes

#### Error: "No se encontraron agentes"
```bash
# Verificar estructura de carpetas
ls groups/*/policy.py

# Verificar imports en policy.py
python -c "from groups.Group_A.policy import *"
```

#### Error: "ModuleNotFoundError"
```bash
# Verificar entorno Python
conda list numpy matplotlib
pip list | grep pandas
```

#### Jupyter Notebook no inicia
```bash
# Instalar/actualizar Jupyter
conda install jupyter
# o
pip install jupyter notebook

# Verificar puerto
jupyter notebook --port=8888
```

## ğŸ† Funcionalidades Detalladas

### ğŸ¤– Agentes Implementados

| Agente | Tipo | DescripciÃ³n | Rendimiento |
|--------|------|-------------|-------------|
| **MCTS-Champion** | ğŸŒ³ Tree Search | Monte Carlo Tree Search con UCT | ~70-80% |
| **Q-Learning-AI** | ğŸ§  Reinforcement Learning | Q-Learning con Îµ-greedy | ~60-75% |
| **Group A, B, C** | ğŸ¯ Diversos | Agentes estudiantiles variados | Variable |

### ğŸ“Š Sistema de MÃ©tricas

El proyecto genera mÃ©tricas comprehensivas:

#### ğŸ® MÃ©tricas de Juego
- **Win Rate**: Porcentaje de victorias
- **Game Length**: DuraciÃ³n promedio de partidas
- **Draw Rate**: Frecuencia de empates

#### ğŸ§  MÃ©tricas de Aprendizaje
- **Q-Table Size**: Estados Ãºnicos explorados
- **Epsilon Decay**: EvoluciÃ³n exploraciÃ³nâ†’explotaciÃ³n
- **Convergence**: Estabilidad del rendimiento

#### âš¡ MÃ©tricas de Eficiencia
- **Training Speed**: Episodios por segundo
- **Learning Efficiency**: Mejora por episodio
- **ROI**: Retorno de inversiÃ³n en exploraciÃ³n

### ğŸ¨ Visualizaciones Disponibles

1. **ğŸ“ˆ Curvas de Aprendizaje**
   - EvoluciÃ³n de win rate
   - Decay de epsilon
   - Crecimiento Q-table

2. **ğŸ“Š AnÃ¡lisis EstadÃ­stico**
   - Distribuciones de duraciÃ³n
   - Histogramas de recompensas
   - Correlaciones entre mÃ©tricas

3. **ğŸ¯ AnÃ¡lisis de Convergencia**
   - Estabilidad temporal
   - Velocidad de aprendizaje
   - Eficiencia algorÃ­tmica

4. **ğŸ† Comparaciones**
   - Vs agentes baseline
   - GrÃ¡ficos radiales de fortalezas
   - Benchmarking acadÃ©mico

## ğŸ”¬ MetodologÃ­a CientÃ­fica

### ğŸ“š Fundamentos TeÃ³ricos

El proyecto implementa conceptos de vanguardia en IA:

#### Q-Learning Algorithm
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
```
- **s, a**: Estado y acciÃ³n actuales
- **r**: Recompensa inmediata  
- **Î³**: Factor de descuento
- **Î±**: Tasa de aprendizaje

#### Îµ-Greedy Exploration
```
Ï€(a|s) = {
  random action     if rand() < Îµ
  argmax Q(s,a)     otherwise
}
```

#### Decay Schedule
```
Îµ(t) = max(Îµ_min, Îµ_init Ã— decay^t)
```

### ğŸ§ª Protocolo Experimental

1. **ğŸ¯ HipÃ³tesis**: Q-Learning puede aprender estrategias competitivas en Connect 4
2. **ğŸ”¬ Variables**:
   - Independiente: ParÃ¡metros del algoritmo (Î±, Î³, Îµ)
   - Dependiente: Win rate, convergencia, eficiencia
3. **ğŸ“Š MÃ©tricas**: Multiple evaluaciones estadÃ­sticas
4. **ğŸ® Controles**: Baselines aleatorio y MCTS
5. **ğŸ“ˆ AnÃ¡lisis**: VisualizaciÃ³n y pruebas de significancia

## âš™ï¸ ConfiguraciÃ³n

- **Python**: 3.10+
- **Dependencias**: numpy, matplotlib, seaborn, pandas
- **Entorno**: Conda environment "iaenv"

## ğŸ† Funcionalidades

âœ… **Sistema de Torneo Completo**
- DetecciÃ³n automÃ¡tica de agentes
- Bracket eliminatorio
- Guardado de resultados

âœ… **Entrenamiento Q-Learning**
- Entrenamiento contra MCTS y agentes aleatorios
- MÃ©tricas detalladas de progreso
- Guardado automÃ¡tico de checkpoints

âœ… **AnÃ¡lisis Visual**
- Curvas de aprendizaje
- AnÃ¡lisis de convergencia
- Comparaciones de rendimiento
- Recomendaciones automÃ¡ticas

## ğŸ“ Notas Importantes

âš ï¸ **IMPORTANTE**: El archivo `connect4/policy.py` es el original y **NO debe modificarse** ya que es evaluado por Gradescope.

âœ¨ **Limpieza realizada**:
- âŒ Eliminados archivos duplicados
- âŒ Eliminadas carpetas vacÃ­as
- âŒ Eliminados archivos de cache (`__pycache__`)
- âŒ Eliminados archivos de debug no utilizados

## ğŸ“ Resultados y Logros Esperados

### ğŸ“Š MÃ©tricas de Ã‰xito

Al completar el proyecto, deberÃ­as observar:

- âœ… **Win Rate > 60%**: Agente Q-Learning supera baseline aleatorio
- âœ… **Convergencia Estable**: Varianza < 5% en fases finales  
- âœ… **ExploraciÃ³n Efectiva**: > 500 estados Ãºnicos explorados
- âœ… **Eficiencia Temporal**: < 2000 episodios para convergencia
- âœ… **GeneralizaciÃ³n**: Rendimiento consistente vs mÃºltiples oponentes

### ğŸ† Benchmark AcadÃ©mico

| MÃ©trica | Esperado | Excelente | Observado |
|---------|----------|-----------|-----------|
| Win Rate Final | 60% | 75% | **Tu resultado** |
| Convergencia | 1500 eps | 1000 eps | **Tu resultado** |
| Estados Explorados | 500 | 1000+ | **Tu resultado** |
| Estabilidad (Ïƒ) | < 0.05 | < 0.03 | **Tu resultado** |

### ğŸ”¬ Contribuciones CientÃ­ficas

Este proyecto demuestra:
1. **Viabilidad de Q-Learning** en espacios de estado moderados
2. **Importancia del balance** exploraciÃ³n-explotaciÃ³n
3. **Efectividad de mÃ©tricas** para evaluaciÃ³n de RL
4. **MetodologÃ­a replicable** para investigaciÃ³n en juegos

## ğŸ’¡ Casos de Uso y Aplicaciones

### ğŸ® Entretenimiento
- Oponente IA en videojuegos
- Tutor personalizado para aprender Connect 4
- AnÃ¡lisis de estrategias humanas

### ğŸ“ EducaciÃ³n
- DemostraciÃ³n de conceptos de IA
- Laboratorio para cursos de Machine Learning
- Proyecto base para investigaciÃ³n estudiantil

### ğŸ”¬ InvestigaciÃ³n
- Baseline para algoritmos mÃ¡s avanzados
- Estudio de convergencia en RL
- AnÃ¡lisis de transferencia de conocimiento

### ğŸ¢ Comercial
- Motor de IA para aplicaciones mÃ³viles
- Sistema de recomendaciÃ³n de movimientos
- Herramienta de anÃ¡lisis competitivo

## ğŸ“š Referencias y Recursos

### ğŸ“– Literatura Fundamental
- **Sutton & Barto** (2018): "Reinforcement Learning: An Introduction"
- **Russell & Norvig** (2021): "Artificial Intelligence: A Modern Approach"
- **Silver et al.** (2016): "Mastering the game of Go with deep neural networks"

### ğŸŒ Recursos Online
- [OpenAI Spinning Up](https://spinningup.openai.com/): RL educational resource
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/): RL algorithms library
- [Gymnasium](https://gymnasium.farama.org/): RL environments

### ğŸ”— Repositorios Relacionados
- [Connect4-AI](https://github.com/topics/connect4-ai): Proyectos similares
- [RL-Games](https://github.com/topics/reinforcement-learning-games): Juegos con RL
- [Q-Learning](https://github.com/topics/q-learning): Implementaciones variadas

## ğŸ¤ Contribuciones y ColaboraciÃ³n

### ğŸ› ï¸ CÃ³mo Contribuir

1. **Fork del proyecto**
2. **Crear branch**: `git checkout -b feature/nueva-funcionalidad`
3. **Implementar mejoras**
4. **Testing completo**
5. **Pull request con descripciÃ³n detallada**

### ğŸ¯ Ãreas de Mejora Priorizadas

1. **ğŸ§  Algoritmos Avanzados**
   - Double Q-Learning
   - Prioritized Experience Replay
   - Deep Q-Networks (DQN)

2. **âš¡ OptimizaciÃ³n**
   - ParalelizaciÃ³n de entrenamiento
   - OptimizaciÃ³n de memoria
   - AceleraciÃ³n por GPU

3. **ğŸ“Š AnÃ¡lisis**
   - MÃ©tricas adicionales
   - Visualizaciones 3D
   - AnÃ¡lisis de sensibilidad

4. **ğŸ® Extensiones**
   - Otros juegos de mesa
   - Interfaces grÃ¡ficas
   - Competencia online

## ğŸ“ Soporte y Contacto

### ğŸ› Reporte de Issues
- Usar GitHub Issues para bugs
- Incluir logs completos y pasos de reproducciÃ³n
- Especificar entorno (OS, Python version, etc.)

### â“ Preguntas Frecuentes

**P: Â¿Por quÃ© el entrenamiento es lento?**
R: Q-Learning explora muchos estados. Considera reducir episodios o implementar aproximaciÃ³n funcional.

**P: Â¿El agente no mejora?**
R: Revisa hiperparÃ¡metros (Î±, Î³, Îµ). Aumenta episodios o ajusta funciÃ³n de recompensa.

**P: Â¿Errores de importaciÃ³n?**
R: Verifica estructura de carpetas y rutas en sys.path. Usa paths absolutos si es necesario.

### âš™ï¸ ConfiguraciÃ³n

- **Python**: 3.10+
- **Dependencias**: numpy, matplotlib, seaborn, pandas
- **Entorno**: Conda environment "iaenv"
- **Memoria**: ~2GB RAM para entrenamiento completo
- **Tiempo**: 10-30 minutos para entrenamiento bÃ¡sico

## ğŸ† Funcionalidades

âœ… **Sistema de Torneo Completo**
- DetecciÃ³n automÃ¡tica de agentes
- Bracket eliminatorio
- Guardado de resultados

âœ… **Entrenamiento Q-Learning**
- Entrenamiento contra MCTS y agentes aleatorios
- MÃ©tricas detalladas de progreso
- Guardado automÃ¡tico de checkpoints

âœ… **AnÃ¡lisis Visual**
- Curvas de aprendizaje
- AnÃ¡lisis de convergencia
- Comparaciones de rendimiento
- Recomendaciones automÃ¡ticas

## ğŸ“ Notas Importantes

âš ï¸ **IMPORTANTE**: El archivo `connect4/policy.py` es el original y **NO debe modificarse** ya que es evaluado por Gradescope.

âœ¨ **OptimizaciÃ³n completa**:
- âŒ Eliminados archivos duplicados
- âŒ Eliminadas carpetas vacÃ­as  
- âŒ Eliminados archivos de cache (`__pycache__`)
- âŒ Eliminados archivos de debug no utilizados

---

## ğŸ¯ Resumen Ejecutivo

**Connect 4 AI Tournament** es un proyecto integral de **Inteligencia Artificial** que demuestra la aplicaciÃ³n prÃ¡ctica de **Q-Learning** en un entorno competitivo. Combina **teorÃ­a acadÃ©mica sÃ³lida** con **implementaciÃ³n prÃ¡ctica robusta**, proporcionando una plataforma completa para el **desarrollo, entrenamiento y evaluaciÃ³n** de agentes inteligentes.

### ğŸŒŸ Valor AcadÃ©mico
- **ImplementaciÃ³n desde cero** de algoritmos fundamentales
- **MetodologÃ­a cientÃ­fica rigurosa** con mÃ©tricas comprehensivas  
- **AnÃ¡lisis estadÃ­stico detallado** del proceso de aprendizaje
- **Framework extensible** para investigaciÃ³n futura

### ğŸš€ Impacto PrÃ¡ctico
- **Agente competitivo** con rendimiento superior al azar
- **Sistema escalable** para torneos automatizados
- **Herramientas de anÃ¡lisis** para evaluaciÃ³n de IA
- **Base sÃ³lida** para proyectos avanzados

**ğŸ‰ Â¡Proyecto optimizado y listo para demostrar el poder de la Inteligencia Artificial!** ğŸ‰