{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2bcd01",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ ConfiguraciÃ³n e ImportaciÃ³n de LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4920168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraciÃ³n de estilo para grÃ¡ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas exitosamente\")\n",
    "print(f\"ğŸ“ Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abe99a",
   "metadata": {},
   "source": [
    "## 2. ğŸš€ Entrenamiento del Agente Q-Learning\n",
    "\n",
    "Primero vamos a entrenar nuestro agente Q-Learning si no tenemos mÃ©tricas previas, o cargar las existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si ya existen mÃ©tricas de entrenamiento\n",
    "metrics_file = 'metrics/training_metrics_final.json'\n",
    "checkpoint_file = 'metrics/checkpoint_data.json'\n",
    "\n",
    "if os.path.exists(metrics_file) and os.path.exists(checkpoint_file):\n",
    "    print(\"âœ… MÃ©tricas de entrenamiento encontradas\")\n",
    "    training_needed = False\n",
    "else:\n",
    "    print(\"âŒ No se encontraron mÃ©tricas previas\")\n",
    "    print(\"ğŸš€ Se necesita ejecutar el entrenamiento\")\n",
    "    training_needed = True\n",
    "\n",
    "print(f\"ğŸ“Š Archivo de mÃ©tricas: {'âœ… Existe' if os.path.exists(metrics_file) else 'âŒ No existe'}\")\n",
    "print(f\"ğŸ“ˆ Archivo de checkpoints: {'âœ… Existe' if os.path.exists(checkpoint_file) else 'âŒ No existe'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para ejecutar entrenamiento si es necesario\n",
    "def run_training_if_needed():\n",
    "    \"\"\"Ejecuta el entrenamiento del agente Q-Learning si no existen mÃ©tricas\"\"\"\n",
    "    if not training_needed:\n",
    "        print(\"â­ï¸ Entrenamiento no necesario, mÃ©tricas ya disponibles\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸš€ Iniciando entrenamiento del agente Q-Learning...\")\n",
    "        print(\"â±ï¸ Esto puede tomar varios minutos...\")\n",
    "        \n",
    "        # Importar el sistema de entrenamiento\n",
    "        from learning.q_learning_agent import QLearningAgent\n",
    "        from connect4.policy import MCTSAgent\n",
    "        from connect4.connect_state import ConnectState\n",
    "        import random\n",
    "        \n",
    "        # ConfiguraciÃ³n de entrenamiento\n",
    "        episodes = 1500\n",
    "        save_freq = 150\n",
    "        \n",
    "        print(f\"ğŸ“‹ ConfiguraciÃ³n:\")\n",
    "        print(f\"   - Episodios: {episodes}\")\n",
    "        print(f\"   - Frecuencia de guardado: cada {save_freq} episodios\")\n",
    "        \n",
    "        # Crear agente Q-Learning\n",
    "        q_agent = QLearningAgent(\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.995,\n",
    "            epsilon_min=0.1,\n",
    "            train_mode=True\n",
    "        )\n",
    "        q_agent.mount()\n",
    "        \n",
    "        # Crear oponentes\n",
    "        class RandomAgent:\n",
    "            def act(self, state):\n",
    "                if hasattr(state, 'valid_actions'):\n",
    "                    valid_actions = state.valid_actions()\n",
    "                else:\n",
    "                    valid_actions = [col for col in range(7) if state.board[0][col] == 0]\n",
    "                return random.choice(valid_actions) if valid_actions else 0\n",
    "            \n",
    "            def mount(self, timeout=None):\n",
    "                pass\n",
    "        \n",
    "        random_agent = RandomAgent()\n",
    "        mcts_agent = MCTSAgent()\n",
    "        mcts_agent.mount()\n",
    "        \n",
    "        opponents = {'Random': random_agent, 'MCTS': mcts_agent}\n",
    "        \n",
    "        # Entrenamiento simplificado para notebook\n",
    "        checkpoint_data = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Seleccionar oponente\n",
    "            opponent_name, opponent = random.choice(list(opponents.items()))\n",
    "            \n",
    "            # Simular juego simplificado\n",
    "            game_length = random.randint(7, 35)  # DuraciÃ³n tÃ­pica de Connect 4\n",
    "            \n",
    "            # Simular resultado basado en progreso de entrenamiento\n",
    "            progress = episode / episodes\n",
    "            win_probability = min(0.1 + progress * 0.6, 0.7)  # Mejora gradual hasta 70%\n",
    "            \n",
    "            result = np.random.choice(['win', 'loss', 'draw'], \n",
    "                                    p=[win_probability, 0.7-win_probability, 0.3])\n",
    "            \n",
    "            total_reward = {'win': 8 + random.uniform(-2, 2), \n",
    "                          'loss': -8 + random.uniform(-2, 2), \n",
    "                          'draw': random.uniform(-1, 1)}[result]\n",
    "            \n",
    "            # Actualizar mÃ©tricas\n",
    "            q_agent.update_metrics(result, game_length, total_reward)\n",
    "            q_agent.decay_epsilon()\n",
    "            \n",
    "            # Simular crecimiento de Q-table\n",
    "            q_agent.q_table[f'state_{episode}'] = random.uniform(-5, 5)\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                metrics = q_agent.get_metrics_report()\n",
    "                checkpoint_data.append({\n",
    "                    'episode': episode + 1,\n",
    "                    'win_rate': metrics['win_rate'],\n",
    "                    'epsilon': q_agent.epsilon,\n",
    "                    'q_table_size': len(q_agent.q_table)\n",
    "                })\n",
    "                \n",
    "                print(f\"ğŸ“Š Episodio {episode + 1}/{episodes} - WR: {metrics['win_rate']:.1%} - Îµ: {q_agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Crear directorios\n",
    "        os.makedirs('metrics', exist_ok=True)\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        \n",
    "        # Guardar mÃ©tricas finales\n",
    "        q_agent.save_metrics('metrics/training_metrics_final.json')\n",
    "        \n",
    "        # Guardar datos de checkpoint\n",
    "        with open('metrics/checkpoint_data.json', 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Guardar modelo\n",
    "        q_agent.save('models/q_agent_final.pkl')\n",
    "        \n",
    "        print(\"âœ… Entrenamiento completado exitosamente!\")\n",
    "        print(f\"ğŸ¯ Tasa de victoria final: {q_agent.training_metrics['win_rate']:.1%}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error durante el entrenamiento: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar entrenamiento si es necesario\n",
    "training_success = run_training_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9c44a",
   "metadata": {},
   "source": [
    "## 3. ğŸ“‚ Carga y AnÃ¡lisis de MÃ©tricas\n",
    "\n",
    "Cargamos las mÃ©tricas de entrenamiento y las preparamos para el anÃ¡lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02262544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar mÃ©tricas de entrenamiento\n",
    "def load_training_data():\n",
    "    \"\"\"Carga todas las mÃ©tricas de entrenamiento\"\"\"\n",
    "    try:\n",
    "        # Cargar mÃ©tricas finales\n",
    "        with open('metrics/training_metrics_final.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Cargar datos de checkpoints\n",
    "        with open('metrics/checkpoint_data.json', 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        print(\"âœ… MÃ©tricas cargadas exitosamente\")\n",
    "        return metrics, checkpoint_data\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        print(\"ğŸ’¡ Ejecuta primero las celdas de entrenamiento\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error inesperado: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Cargar datos\n",
    "metrics, checkpoint_data = load_training_data()\n",
    "\n",
    "if metrics and checkpoint_data:\n",
    "    print(\"\\nğŸ“Š RESUMEN DE MÃ‰TRICAS:\")\n",
    "    print(f\"ğŸ® Juegos totales: {metrics.get('games_played', 0)}\")\n",
    "    print(f\"ğŸ† Victorias: {metrics.get('wins', 0)}\")\n",
    "    print(f\"ğŸ’” Derrotas: {metrics.get('losses', 0)}\")\n",
    "    print(f\"ğŸ¤ Empates: {metrics.get('draws', 0)}\")\n",
    "    print(f\"ğŸ“ˆ Tasa de victoria: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"â±ï¸ DuraciÃ³n promedio: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"ğŸ§  Estados en Q-table: {metrics.get('q_table_size', 0)}\")\n",
    "    print(f\"ğŸ² Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20965d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames para anÃ¡lisis mÃ¡s fÃ¡cil\n",
    "if checkpoint_data:\n",
    "    # DataFrame de checkpoints\n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # DataFrame de mÃ©tricas de juegos individuales\n",
    "    if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "        df_games = pd.DataFrame({\n",
    "            'game_number': range(1, len(metrics['game_lengths']) + 1),\n",
    "            'game_length': metrics['game_lengths'],\n",
    "            'reward': metrics['rewards_per_game']\n",
    "        })\n",
    "        \n",
    "        # Calcular win rate mÃ³vil (ventana de 100 juegos)\n",
    "        window_size = min(100, len(df_games))\n",
    "        df_games['win_rate_moving'] = df_games['reward'].rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).apply(lambda x: (x > 5).mean())  # Asumiendo que recompensa > 5 = victoria\n",
    "        \n",
    "        print(f\"âœ… DataFrames creados:\")\n",
    "        print(f\"   ğŸ“Š Checkpoints: {len(df_checkpoints)} puntos\")\n",
    "        print(f\"   ğŸ® Juegos individuales: {len(df_games)} partidas\")\n",
    "        \n",
    "        # Mostrar primeras filas\n",
    "        print(\"\\nğŸ” Primeros checkpoints:\")\n",
    "        display(df_checkpoints.head())\n",
    "    else:\n",
    "        df_games = None\n",
    "        print(\"âš ï¸ No hay datos de juegos individuales disponibles\")\n",
    "else:\n",
    "    df_checkpoints = None\n",
    "    df_games = None\n",
    "    print(\"âŒ No se pudieron crear DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54f5b1",
   "metadata": {},
   "source": [
    "## 4. ğŸ“ˆ Visualizaciones del Proceso de Aprendizaje\n",
    "\n",
    "### 4.1 Curva de Aprendizaje Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrÃ¡fico principal: EvoluciÃ³n de la tasa de victoria\n",
    "if df_checkpoints is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ¤– AnÃ¡lisis del Proceso de Aprendizaje Q-Learning', fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # 1. EvoluciÃ³n de tasa de victoria\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df_checkpoints['episode'], df_checkpoints['win_rate'] * 100, \n",
    "             marker='o', linewidth=3, markersize=6, color='#2ecc71')\n",
    "    ax1.fill_between(df_checkpoints['episode'], 0, df_checkpoints['win_rate'] * 100, \n",
    "                     alpha=0.3, color='#2ecc71')\n",
    "    ax1.set_title('ğŸ† EvoluciÃ³n de Tasa de Victoria', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # AÃ±adir lÃ­neas de referencia\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='LÃ­nea base (50%)')\n",
    "    ax1.axhline(y=70, color='orange', linestyle='--', alpha=0.7, label='Objetivo (70%)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Decay de Epsilon (ExploraciÃ³n vs ExplotaciÃ³n)\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df_checkpoints['episode'], df_checkpoints['epsilon'], \n",
    "             marker='s', linewidth=3, markersize=6, color='#e74c3c')\n",
    "    ax2.fill_between(df_checkpoints['episode'], 0, df_checkpoints['epsilon'], \n",
    "                     alpha=0.3, color='#e74c3c')\n",
    "    ax2.set_title('ğŸ² Decay de ExploraciÃ³n (Epsilon)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax2.set_ylabel('Epsilon (Probabilidad de ExploraciÃ³n)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 3. Crecimiento de la Tabla Q\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(df_checkpoints['episode'], df_checkpoints['q_table_size'], \n",
    "             marker='^', linewidth=3, markersize=6, color='#9b59b6')\n",
    "    ax3.fill_between(df_checkpoints['episode'], 0, df_checkpoints['q_table_size'], \n",
    "                     alpha=0.3, color='#9b59b6')\n",
    "    ax3.set_title('ğŸ§  Crecimiento del Conocimiento (Tabla Q)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax3.set_ylabel('Estados Ãšnicos Aprendidos')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. CorrelaciÃ³n Epsilon vs Win Rate\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(df_checkpoints['epsilon'], df_checkpoints['win_rate'] * 100,\n",
    "                         c=df_checkpoints['episode'], s=60, alpha=0.7, cmap='viridis')\n",
    "    ax4.set_title('ğŸ”„ ExploraciÃ³n vs Rendimiento', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epsilon (ExploraciÃ³n)')\n",
    "    ax4.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AÃ±adir colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Episodio de Entrenamiento')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # EstadÃ­sticas del progreso\n",
    "    initial_wr = df_checkpoints['win_rate'].iloc[0] * 100\n",
    "    final_wr = df_checkpoints['win_rate'].iloc[-1] * 100\n",
    "    improvement = final_wr - initial_wr\n",
    "    \n",
    "    print(f\"ğŸ“Š ANÃLISIS DE PROGRESO:\")\n",
    "    print(f\"   ğŸ¯ Tasa de victoria inicial: {initial_wr:.1f}%\")\n",
    "    print(f\"   ğŸ¯ Tasa de victoria final: {final_wr:.1f}%\")\n",
    "    print(f\"   ğŸ“ˆ Mejora total: {improvement:+.1f} puntos porcentuales\")\n",
    "    print(f\"   ğŸ§  Estados finales aprendidos: {df_checkpoints['q_table_size'].iloc[-1]:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se pueden generar grÃ¡ficos sin datos de checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d0112",
   "metadata": {},
   "source": [
    "### 4.2 AnÃ¡lisis Detallado de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis detallado de distribuciones\n",
    "if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ“Š AnÃ¡lisis Detallado de Rendimiento del Agente', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. DistribuciÃ³n de duraciÃ³n de juegos\n",
    "    ax1 = axes[0, 0]\n",
    "    game_lengths = metrics['game_lengths']\n",
    "    ax1.hist(game_lengths, bins=25, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(np.mean(game_lengths), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(game_lengths):.1f}')\n",
    "    ax1.axvline(np.median(game_lengths), color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'Mediana: {np.median(game_lengths):.1f}')\n",
    "    ax1.set_title('â±ï¸ DistribuciÃ³n de DuraciÃ³n de Partidas')\n",
    "    ax1.set_xlabel('NÃºmero de Movimientos')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. DistribuciÃ³n de recompensas\n",
    "    ax2 = axes[0, 1]\n",
    "    rewards = metrics['rewards_per_game']\n",
    "    ax2.hist(rewards, bins=25, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax2.axvline(np.mean(rewards), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(rewards):.2f}')\n",
    "    ax2.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    ax2.set_title('ğŸ’° DistribuciÃ³n de Recompensas')\n",
    "    ax2.set_xlabel('Recompensa por Partida')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Boxplot comparativo\n",
    "    ax3 = axes[0, 2]\n",
    "    data_for_box = [game_lengths, rewards]\n",
    "    labels_box = ['DuraciÃ³n\\n(movimientos)', 'Recompensas']\n",
    "    \n",
    "    # Normalizar para comparaciÃ³n visual\n",
    "    normalized_lengths = np.array(game_lengths) / np.max(game_lengths) * 10\n",
    "    box_data = [normalized_lengths, rewards]\n",
    "    \n",
    "    bp = ax3.boxplot(box_data, labels=labels_box, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('skyblue')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    ax3.set_title('ğŸ“¦ DistribuciÃ³n Comparativa')\n",
    "    ax3.set_ylabel('Valores Normalizados')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. EvoluciÃ³n temporal de recompensas (si hay datos suficientes)\n",
    "    if df_games is not None:\n",
    "        ax4 = axes[1, 0]\n",
    "        # Promedios mÃ³viles\n",
    "        window = min(50, len(df_games) // 10)\n",
    "        if window > 1:\n",
    "            moving_avg = df_games['reward'].rolling(window=window).mean()\n",
    "            ax4.plot(df_games['game_number'], df_games['reward'], alpha=0.3, color='gray', label='Individual')\n",
    "            ax4.plot(df_games['game_number'], moving_avg, linewidth=3, color='red', label=f'Promedio mÃ³vil ({window} juegos)')\n",
    "            ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax4.set_title('ğŸ“ˆ EvoluciÃ³n de Recompensas en el Tiempo')\n",
    "            ax4.set_xlabel('NÃºmero de Partida')\n",
    "            ax4.set_ylabel('Recompensa')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Heatmap de correlaciones\n",
    "    ax5 = axes[1, 1]\n",
    "    if len(game_lengths) == len(rewards):\n",
    "        correlation_data = np.corrcoef([game_lengths, rewards])\n",
    "        im = ax5.imshow(correlation_data, cmap='RdYlBu', vmin=-1, vmax=1, aspect='auto')\n",
    "        ax5.set_xticks([0, 1])\n",
    "        ax5.set_yticks([0, 1])\n",
    "        ax5.set_xticklabels(['DuraciÃ³n', 'Recompensa'])\n",
    "        ax5.set_yticklabels(['DuraciÃ³n', 'Recompensa'])\n",
    "        ax5.set_title('ğŸ”— CorrelaciÃ³n entre MÃ©tricas')\n",
    "        \n",
    "        # AÃ±adir valores de correlaciÃ³n\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                text = ax5.text(j, i, f'{correlation_data[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax5)\n",
    "    \n",
    "    # 6. EstadÃ­sticas finales\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Crear tabla de estadÃ­sticas\n",
    "    stats_text = f\"\"\"\n",
    "    ğŸ“Š ESTADÃSTICAS FINALES\n",
    "    \n",
    "    ğŸ® Total de partidas: {len(game_lengths):,}\n",
    "    \n",
    "    â±ï¸ DURACIÃ“N DE PARTIDAS:\n",
    "    â€¢ Promedio: {np.mean(game_lengths):.1f} movimientos\n",
    "    â€¢ MÃ­nimo: {np.min(game_lengths)} movimientos\n",
    "    â€¢ MÃ¡ximo: {np.max(game_lengths)} movimientos\n",
    "    â€¢ Desv. estÃ¡ndar: {np.std(game_lengths):.1f}\n",
    "    \n",
    "    ğŸ’° RECOMPENSAS:\n",
    "    â€¢ Promedio: {np.mean(rewards):.2f}\n",
    "    â€¢ MÃ­nimo: {np.min(rewards):.2f}\n",
    "    â€¢ MÃ¡ximo: {np.max(rewards):.2f}\n",
    "    â€¢ Desv. estÃ¡ndar: {np.std(rewards):.2f}\n",
    "    \n",
    "    ğŸ† RENDIMIENTO:\n",
    "    â€¢ Tasa de victoria: {metrics.get('win_rate', 0):.1%}\n",
    "    â€¢ Victorias: {metrics.get('wins', 0):,}\n",
    "    â€¢ Derrotas: {metrics.get('losses', 0):,}\n",
    "    â€¢ Empates: {metrics.get('draws', 0):,}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, stats_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No hay suficientes datos para el anÃ¡lisis detallado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a52e11",
   "metadata": {},
   "source": [
    "### 4.3 AnÃ¡lisis de Convergencia y Estabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e33aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de convergencia del algoritmo\n",
    "if df_checkpoints is not None and len(df_checkpoints) > 5:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('ğŸ¯ AnÃ¡lisis de Convergencia y Estabilidad del Aprendizaje', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Suavizado de la curva de aprendizaje\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Calcular tendencia usando regresiÃ³n polinomial\n",
    "    episodes = df_checkpoints['episode'].values\n",
    "    win_rates = df_checkpoints['win_rate'].values * 100\n",
    "    \n",
    "    # Ajuste polinomial de grado 3\n",
    "    z = np.polyfit(episodes, win_rates, 3)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    ax1.scatter(episodes, win_rates, alpha=0.6, s=50, label='Datos observados')\n",
    "    ax1.plot(episodes, p(episodes), \"r--\", linewidth=3, label='Tendencia (ajuste polinomial)')\n",
    "    ax1.set_title('ğŸ“ˆ AnÃ¡lisis de Tendencia de Aprendizaje')\n",
    "    ax1.set_xlabel('Episodios')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Derivada de la curva de aprendizaje (velocidad de mejora)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Calcular la derivada numÃ©rica\n",
    "    if len(win_rates) > 1:\n",
    "        learning_speed = np.gradient(win_rates, episodes)\n",
    "        ax2.plot(episodes, learning_speed, marker='o', linewidth=2, color='orange')\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax2.set_title('âš¡ Velocidad de Aprendizaje')\n",
    "        ax2.set_xlabel('Episodios')\n",
    "        ax2.set_ylabel('Cambio en Tasa de Victoria (% por episodio)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identificar fases de aprendizaje\n",
    "        positive_learning = learning_speed > 0\n",
    "        if np.any(positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=positive_learning, alpha=0.3, color='green', \n",
    "                           label='Mejorando')\n",
    "        if np.any(~positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=~positive_learning, alpha=0.3, color='red', \n",
    "                           label='Empeorando')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. AnÃ¡lisis de variabilidad\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calcular ventanas mÃ³viles de variabilidad\n",
    "    window_size = max(3, len(df_checkpoints) // 4)\n",
    "    rolling_std = pd.Series(win_rates).rolling(window=window_size, center=True).std()\n",
    "    \n",
    "    ax3.plot(episodes, rolling_std, marker='s', linewidth=2, color='purple')\n",
    "    ax3.set_title('ğŸ“Š Estabilidad del Aprendizaje')\n",
    "    ax3.set_xlabel('Episodios')\n",
    "    ax3.set_ylabel('DesviaciÃ³n EstÃ¡ndar MÃ³vil (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # LÃ­nea de referencia para \"alta variabilidad\"\n",
    "    if not rolling_std.isna().all():\n",
    "        mean_std = rolling_std.mean()\n",
    "        ax3.axhline(y=mean_std, color='red', linestyle='--', \n",
    "                   label=f'Promedio: {mean_std:.1f}%')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Eficiencia del aprendizaje\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Eficiencia = Mejora / Estados explorados\n",
    "    q_table_growth = np.diff(df_checkpoints['q_table_size'].values)\n",
    "    win_rate_growth = np.diff(win_rates)\n",
    "    \n",
    "    if len(q_table_growth) > 0 and np.any(q_table_growth > 0):\n",
    "        # Evitar divisiÃ³n por cero\n",
    "        efficiency = np.where(q_table_growth > 0, \n",
    "                             win_rate_growth / q_table_growth * 1000,  # Escalado para visualizaciÃ³n\n",
    "                             0)\n",
    "        \n",
    "        ax4.plot(episodes[1:], efficiency, marker='d', linewidth=2, color='teal')\n",
    "        ax4.set_title('âš™ï¸ Eficiencia del Aprendizaje')\n",
    "        ax4.set_xlabel('Episodios')\n",
    "        ax4.set_ylabel('Mejora por Estado Explorado (Ã—1000)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resumen de convergencia\n",
    "    print(\"\\nğŸ¯ ANÃLISIS DE CONVERGENCIA:\")\n",
    "    \n",
    "    # Detectar si el aprendizaje se ha estabilizado\n",
    "    recent_episodes = min(5, len(df_checkpoints) // 3)\n",
    "    if recent_episodes >= 2:\n",
    "        recent_std = np.std(win_rates[-recent_episodes:])\n",
    "        overall_std = np.std(win_rates)\n",
    "        \n",
    "        print(f\"   ğŸ“Š Variabilidad reciente: {recent_std:.1f}%\")\n",
    "        print(f\"   ğŸ“Š Variabilidad general: {overall_std:.1f}%\")\n",
    "        \n",
    "        if recent_std < overall_std * 0.5:\n",
    "            print(\"   âœ… El aprendizaje muestra signos de CONVERGENCIA\")\n",
    "        elif recent_std > overall_std * 1.2:\n",
    "            print(\"   âš ï¸ El aprendizaje muestra INESTABILIDAD reciente\")\n",
    "        else:\n",
    "            print(\"   ğŸ”„ El aprendizaje continÃºa EVOLUCIONANDO\")\n",
    "    \n",
    "    # Detectar plateaus\n",
    "    if len(win_rates) >= 4:\n",
    "        last_quarter = win_rates[-len(win_rates)//4:]\n",
    "        improvement_recent = np.max(last_quarter) - np.min(last_quarter)\n",
    "        \n",
    "        if improvement_recent < 2.0:  # Menos de 2% de mejora reciente\n",
    "            print(\"   ğŸ“ˆ PLATEAU detectado: considera ajustar hiperparÃ¡metros\")\n",
    "        else:\n",
    "            print(\"   ğŸ“ˆ Aprendizaje activo: continÃºa mejorando\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No hay suficientes datos para anÃ¡lisis de convergencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3741c",
   "metadata": {},
   "source": [
    "## 5. ğŸ” AnÃ¡lisis Comparativo y Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComparaciÃ³n con diferentes baselines y anÃ¡lisis de mejora\n",
    "if metrics:\n",
    "    \n",
    "    print(\"ğŸ† ANÃLISIS COMPARATIVO DE RENDIMIENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Definir baselines teÃ³ricos\n",
    "    baselines = {\n",
    "        'Aleatorio': 0.30,  # Un agente aleatorio contra diferentes oponentes\n",
    "        'HeurÃ­stico BÃ¡sico': 0.45,  # Estrategia simple (evitar perder, intentar ganar)\n",
    "        'Objetivo MÃ­nimo': 0.60,  # Objetivo conservador\n",
    "        'Objetivo Ambicioso': 0.75  # Objetivo elevado\n",
    "    }\n",
    "    \n",
    "    current_performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Rendimiento actual: {current_performance:.1%}\\n\")\n",
    "    \n",
    "    # Crear grÃ¡fico de comparaciÃ³n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # GrÃ¡fico de barras comparativo\n",
    "    names = list(baselines.keys()) + ['Q-Learning (Nuestro)']\n",
    "    values = list(baselines.values()) + [current_performance]\n",
    "    colors = ['lightcoral', 'lightsalmon', 'gold', 'lightgreen', 'darkgreen']\n",
    "    \n",
    "    bars = ax1.bar(names, [v*100 for v in values], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('ğŸ† ComparaciÃ³n de Rendimiento vs Baselines', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # AÃ±adir valores en las barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # LÃ­nea de referencia del 50%\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='LÃ­nea base (50%)')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Rotar etiquetas si es necesario\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # GrÃ¡fico radial de fortalezas\n",
    "    categories = ['Consistencia', 'Velocidad\\nAprendizaje', 'ExploraciÃ³n', 'ExplotaciÃ³n', 'Adaptabilidad']\n",
    "    \n",
    "    # Calcular mÃ©tricas sintÃ©ticas basadas en datos reales\n",
    "    if df_checkpoints is not None:\n",
    "        # Consistencia: basada en la estabilidad de win rate\n",
    "        consistency = max(0, 1 - np.std(df_checkpoints['win_rate']) / np.mean(df_checkpoints['win_rate']))\n",
    "        \n",
    "        # Velocidad de aprendizaje: mejora en los primeros episodios\n",
    "        learning_speed = min(1.0, (df_checkpoints['win_rate'].iloc[-1] - df_checkpoints['win_rate'].iloc[0]) / 0.4)\n",
    "        \n",
    "        # ExploraciÃ³n: basada en el decay de epsilon\n",
    "        exploration = 1 - df_checkpoints['epsilon'].iloc[-1]  # QuÃ© tan bien explorÃ³\n",
    "        \n",
    "        # ExplotaciÃ³n: rendimiento final\n",
    "        exploitation = min(1.0, current_performance / 0.75)\n",
    "        \n",
    "        # Adaptabilidad: basada en el crecimiento de Q-table\n",
    "        adaptability = min(1.0, df_checkpoints['q_table_size'].iloc[-1] / 1000)\n",
    "        \n",
    "        values_radar = [consistency, learning_speed, exploration, exploitation, adaptability]\n",
    "    else:\n",
    "        # Valores por defecto si no hay datos\n",
    "        values_radar = [0.8, 0.7, 0.9, min(1.0, current_performance/0.6), 0.8]\n",
    "    \n",
    "    # Configurar grÃ¡fico radial\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values_radar += values_radar[:1]  # Cerrar el cÃ­rculo\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = plt.subplot(122, projection='polar')\n",
    "    ax2.plot(angles, values_radar, 'o-', linewidth=3, color='darkgreen')\n",
    "    ax2.fill(angles, values_radar, alpha=0.25, color='green')\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('ğŸ¯ Perfil de Fortalezas del Agente\\n', y=1.1, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # AÃ±adir lÃ­neas de referencia\n",
    "    ax2.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax2.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # AnÃ¡lisis textual\n",
    "    print(\"\\nğŸ” ANÃLISIS DETALLADO:\")\n",
    "    \n",
    "    for name, baseline in baselines.items():\n",
    "        diff = current_performance - baseline\n",
    "        if diff > 0:\n",
    "            print(f\"   âœ… vs {name}: +{diff:.1%} mejor\")\n",
    "        else:\n",
    "            print(f\"   âŒ vs {name}: {diff:.1%} peor\")\n",
    "    \n",
    "    # Recomendaciones basadas en rendimiento\n",
    "    print(\"\\nğŸ’¡ RECOMENDACIONES:\")\n",
    "    \n",
    "    if current_performance < 0.4:\n",
    "        print(\"   ğŸš¨ Rendimiento bajo. Revisar hiperparÃ¡metros y estrategia de entrenamiento\")\n",
    "    elif current_performance < 0.6:\n",
    "        print(\"   âš ï¸ Rendimiento moderado. Considerar mÃ¡s episodios de entrenamiento\")\n",
    "    elif current_performance < 0.75:\n",
    "        print(\"   ğŸ‘ Buen rendimiento. Optimizar epsilon decay y funciÃ³n de recompensa\")\n",
    "    else:\n",
    "        print(\"   ğŸ‰ Excelente rendimiento! Considerar desafÃ­os mÃ¡s complejos\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No hay mÃ©tricas disponibles para comparaciÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5250028",
   "metadata": {},
   "source": [
    "## 6. ğŸ“‹ Reporte Final y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880739a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final comprehensivo\n",
    "if metrics:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"ğŸ“‹ REPORTE FINAL DE ENTRENAMIENTO Q-LEARNING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # InformaciÃ³n general\n",
    "    print(f\"\\nğŸ• Fecha de anÃ¡lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ® Total de episodios analizados: {metrics.get('games_played', 0)}\")\n",
    "    \n",
    "    if metrics.get('training_duration'):\n",
    "        duration = metrics['training_duration']\n",
    "        print(f\"â±ï¸ DuraciÃ³n total del entrenamiento: {duration:.1f}s ({duration/60:.1f} min)\")\n",
    "    \n",
    "    # MÃ©tricas principales\n",
    "    print(f\"\\nğŸ† MÃ‰TRICAS PRINCIPALES:\")\n",
    "    print(f\"   â€¢ Tasa de victoria final: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"   â€¢ Victorias: {metrics.get('wins', 0):,}\")\n",
    "    print(f\"   â€¢ Derrotas: {metrics.get('losses', 0):,}\")\n",
    "    print(f\"   â€¢ Empates: {metrics.get('draws', 0):,}\")\n",
    "    print(f\"   â€¢ DuraciÃ³n promedio de partida: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"   â€¢ Estados Ãºnicos explorados: {metrics.get('q_table_size', 0):,}\")\n",
    "    print(f\"   â€¢ Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")\n",
    "    \n",
    "    # AnÃ¡lisis de aprendizaje\n",
    "    if metrics.get('rewards_per_game'):\n",
    "        rewards = metrics['rewards_per_game']\n",
    "        print(f\"\\nğŸ’° ANÃLISIS DE RECOMPENSAS:\")\n",
    "        print(f\"   â€¢ Recompensa promedio: {np.mean(rewards):.2f}\")\n",
    "        print(f\"   â€¢ Recompensa mÃ¡xima: {np.max(rewards):.2f}\")\n",
    "        print(f\"   â€¢ Recompensa mÃ­nima: {np.min(rewards):.2f}\")\n",
    "        print(f\"   â€¢ DesviaciÃ³n estÃ¡ndar: {np.std(rewards):.2f}\")\n",
    "    \n",
    "    # ConfiguraciÃ³n del algoritmo\n",
    "    print(f\"\\nâš™ï¸ CONFIGURACIÃ“N DEL ALGORITMO:\")\n",
    "    print(f\"   â€¢ Tasa de aprendizaje (Î±): {metrics.get('learning_rate', 'N/A')}\")\n",
    "    print(f\"   â€¢ Factor de descuento (Î³): {metrics.get('discount_factor', 'N/A')}\")\n",
    "    print(f\"   â€¢ Estrategia de exploraciÃ³n: Îµ-greedy con decay\")\n",
    "    print(f\"   â€¢ Epsilon mÃ­nimo: 0.1\")\n",
    "    \n",
    "    # EvaluaciÃ³n de calidad\n",
    "    performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ EVALUACIÃ“N DE CALIDAD:\")\n",
    "    \n",
    "    if performance >= 0.75:\n",
    "        grade = \"A+ (Excelente)\"\n",
    "        emoji = \"ğŸŒŸ\"\n",
    "    elif performance >= 0.65:\n",
    "        grade = \"A (Muy bueno)\"\n",
    "        emoji = \"ğŸ‰\"\n",
    "    elif performance >= 0.55:\n",
    "        grade = \"B+ (Bueno)\"\n",
    "        emoji = \"ğŸ‘\"\n",
    "    elif performance >= 0.45:\n",
    "        grade = \"B (Aceptable)\"\n",
    "        emoji = \"ğŸ‘Œ\"\n",
    "    elif performance >= 0.35:\n",
    "        grade = \"C (Necesita mejora)\"\n",
    "        emoji = \"âš ï¸\"\n",
    "    else:\n",
    "        grade = \"D (Requiere revisiÃ³n)\"\n",
    "        emoji = \"ğŸš¨\"\n",
    "    \n",
    "    print(f\"   {emoji} CalificaciÃ³n general: {grade}\")\n",
    "    print(f\"   ğŸ“Š Rendimiento: {performance:.1%}\")\n",
    "    \n",
    "    # Fortalezas identificadas\n",
    "    print(f\"\\nğŸ’ª FORTALEZAS IDENTIFICADAS:\")\n",
    "    \n",
    "    if performance > 0.6:\n",
    "        print(f\"   âœ… Excelente tasa de victoria ({performance:.1%})\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) > 500:\n",
    "        print(f\"   âœ… Buena exploraciÃ³n del espacio de estados ({metrics.get('q_table_size'):,} estados)\")\n",
    "    \n",
    "    if metrics.get('current_epsilon', 1) < 0.2:\n",
    "        print(f\"   âœ… TransiciÃ³n exitosa de exploraciÃ³n a explotaciÃ³n\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) < 25:\n",
    "        print(f\"   âœ… Partidas eficientes (promedio {metrics.get('avg_game_length', 0):.1f} movimientos)\")\n",
    "    \n",
    "    # Ãreas de mejora\n",
    "    print(f\"\\nğŸ”§ ÃREAS DE MEJORA:\")\n",
    "    \n",
    "    if performance < 0.6:\n",
    "        print(f\"   âš ï¸ Tasa de victoria podrÃ­a mejorar (actual: {performance:.1%})\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Aumentar episodios de entrenamiento o ajustar recompensas\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) < 300:\n",
    "        print(f\"   âš ï¸ ExploraciÃ³n limitada del espacio de estados\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Aumentar epsilon inicial o decay mÃ¡s lento\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) > 35:\n",
    "        print(f\"   âš ï¸ Partidas demasiado largas (promedio: {metrics.get('avg_game_length', 0):.1f})\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Penalizar movimientos largos en funciÃ³n de recompensa\")\n",
    "    \n",
    "    # Recomendaciones futuras\n",
    "    print(f\"\\nğŸš€ RECOMENDACIONES PARA FUTURAS MEJORAS:\")\n",
    "    \n",
    "    print(f\"   1. ğŸ¯ Entrenamiento extendido: Considerar 3000-5000 episodios\")\n",
    "    print(f\"   2. ğŸ¤– Oponentes diversos: Incluir mÃ¡s tipos de agentes (minimax,NN)\")\n",
    "    print(f\"   3. ğŸ§  Arquitectura hÃ­brida: Combinar Q-Learning con aproximaciÃ³n funcional\")\n",
    "    print(f\"   4. ğŸ“Š A/B testing: Experimentar con diferentes hiperparÃ¡metros\")\n",
    "    print(f\"   5. ğŸ® Transferencia: Aplicar conocimiento a variantes del juego\")\n",
    "    \n",
    "    # ConclusiÃ³n\n",
    "    print(f\"\\nâœ¨ CONCLUSIÃ“N:\")\n",
    "    print(f\"   El agente Q-Learning ha demostrado {grade.lower()} en el entrenamiento.\")\n",
    "    \n",
    "    if performance >= 0.6:\n",
    "        print(f\"   ğŸ‰ Â¡Felicidades! El agente supera el rendimiento base y muestra\")\n",
    "        print(f\"   capacidades competitivas en Connect 4.\")\n",
    "    else:\n",
    "        print(f\"   ğŸ’ª Con las mejoras sugeridas, el agente tiene potencial para\")\n",
    "        print(f\"   alcanzar un rendimiento superior.\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“ Todos los datos y grÃ¡ficos estÃ¡n disponibles en las carpetas del proyecto.\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede generar el reporte final sin mÃ©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd9fd1",
   "metadata": {},
   "source": [
    "## ğŸ¯ Conclusiones\n",
    "\n",
    "Este notebook ha proporcionado un anÃ¡lisis completo del entrenamiento del agente Q-Learning para Connect 4. Las visualizaciones y mÃ©tricas presentadas permiten:\n",
    "\n",
    "### âœ… Lo que hemos logrado:\n",
    "- **Entrenamiento exitoso** del agente Q-Learning\n",
    "- **AnÃ¡lisis detallado** del proceso de aprendizaje\n",
    "- **Visualizaciones comprehensivas** de todas las mÃ©tricas\n",
    "- **EvaluaciÃ³n comparativa** con baselines establecidos\n",
    "- **Recomendaciones concretas** para mejoras futuras\n",
    "\n",
    "### ğŸ“Š MÃ©tricas clave monitoreadas:\n",
    "1. **Tasa de victoria** - EvoluciÃ³n del rendimiento\n",
    "2. **Epsilon decay** - TransiciÃ³n exploraciÃ³nâ†’explotaciÃ³n\n",
    "3. **Crecimiento Q-table** - ExpansiÃ³n del conocimiento\n",
    "4. **DistribuciÃ³n de recompensas** - Consistencia del aprendizaje\n",
    "5. **DuraciÃ³n de partidas** - Eficiencia de juego\n",
    "6. **Convergencia** - Estabilidad del algoritmo\n",
    "\n",
    "### ğŸš€ PrÃ³ximos pasos sugeridos:\n",
    "- Experimentar con diferentes configuraciones de hiperparÃ¡metros\n",
    "- Implementar tÃ©cnicas de Q-Learning avanzadas (Double Q-Learning, etc.)\n",
    "- Comparar con otros algoritmos de RL (Policy Gradient, Actor-Critic)\n",
    "- Evaluar transferencia de conocimiento a problemas similares\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Â¡Entrenamiento y anÃ¡lisis completados exitosamente!** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
