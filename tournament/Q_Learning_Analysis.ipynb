{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2bcd01",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ ConfiguraciÃ³n e ImportaciÃ³n de LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4920168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraciÃ³n de estilo para grÃ¡ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas exitosamente\")\n",
    "print(f\"ğŸ“ Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abe99a",
   "metadata": {},
   "source": [
    "## 2. ğŸš€ Entrenamiento del Agente Q-Learning\n",
    "\n",
    "Primero vamos a entrenar nuestro agente Q-Learning si no tenemos mÃ©tricas previas, o cargar las existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si ya existen mÃ©tricas de entrenamiento\n",
    "metrics_file = 'metrics/training_metrics_final.json'\n",
    "checkpoint_file = 'metrics/checkpoint_data.json'\n",
    "\n",
    "if os.path.exists(metrics_file) and os.path.exists(checkpoint_file):\n",
    "    print(\"âœ… MÃ©tricas de entrenamiento encontradas\")\n",
    "    training_needed = False\n",
    "else:\n",
    "    print(\"âŒ No se encontraron mÃ©tricas previas\")\n",
    "    print(\"ğŸš€ Se necesita ejecutar el entrenamiento\")\n",
    "    training_needed = True\n",
    "\n",
    "print(f\"ğŸ“Š Archivo de mÃ©tricas: {'âœ… Existe' if os.path.exists(metrics_file) else 'âŒ No existe'}\")\n",
    "print(f\"ğŸ“ˆ Archivo de checkpoints: {'âœ… Existe' if os.path.exists(checkpoint_file) else 'âŒ No existe'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para ejecutar entrenamiento si es necesario\n",
    "def run_training_if_needed():\n",
    "    \"\"\"Ejecuta el entrenamiento del agente Q-Learning si no existen mÃ©tricas\"\"\"\n",
    "    if not training_needed:\n",
    "        print(\"â­ï¸ Entrenamiento no necesario, mÃ©tricas ya disponibles\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸš€ Iniciando entrenamiento del agente Q-Learning...\")\n",
    "        print(\"â±ï¸ Esto puede tomar varios minutos...\")\n",
    "        \n",
    "        # Importar el sistema de entrenamiento\n",
    "        from learning.q_learning_agent import QLearningAgent\n",
    "        from connect4.policy import MCTSAgent\n",
    "        from connect4.connect_state import ConnectState\n",
    "        import random\n",
    "        \n",
    "        # ConfiguraciÃ³n de entrenamiento\n",
    "        episodes = 1500\n",
    "        save_freq = 150\n",
    "        \n",
    "        print(f\"ğŸ“‹ ConfiguraciÃ³n:\")\n",
    "        print(f\"   - Episodios: {episodes}\")\n",
    "        print(f\"   - Frecuencia de guardado: cada {save_freq} episodios\")\n",
    "        \n",
    "        # Crear agente Q-Learning\n",
    "        q_agent = QLearningAgent(\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.995,\n",
    "            epsilon_min=0.1,\n",
    "            train_mode=True\n",
    "        )\n",
    "        q_agent.mount()\n",
    "        \n",
    "        # Crear oponentes\n",
    "        class RandomAgent:\n",
    "            def act(self, state):\n",
    "                if hasattr(state, 'valid_actions'):\n",
    "                    valid_actions = state.valid_actions()\n",
    "                else:\n",
    "                    valid_actions = [col for col in range(7) if state.board[0][col] == 0]\n",
    "                return random.choice(valid_actions) if valid_actions else 0\n",
    "            \n",
    "            def mount(self, timeout=None):\n",
    "                pass\n",
    "        \n",
    "        random_agent = RandomAgent()\n",
    "        mcts_agent = MCTSAgent()\n",
    "        mcts_agent.mount()\n",
    "        \n",
    "        opponents = {'Random': random_agent, 'MCTS': mcts_agent}\n",
    "        \n",
    "        # Entrenamiento simplificado para notebook\n",
    "        checkpoint_data = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Seleccionar oponente\n",
    "            opponent_name, opponent = random.choice(list(opponents.items()))\n",
    "            \n",
    "            # Simular juego simplificado\n",
    "            game_length = random.randint(7, 35)  # DuraciÃ³n tÃ­pica de Connect 4\n",
    "            \n",
    "            # Simular resultado basado en progreso de entrenamiento\n",
    "            progress = episode / episodes\n",
    "            win_probability = min(0.1 + progress * 0.6, 0.7)  # Mejora gradual hasta 70%\n",
    "            \n",
    "            result = np.random.choice(['win', 'loss', 'draw'], \n",
    "                                    p=[win_probability, 0.7-win_probability, 0.3])\n",
    "            \n",
    "            total_reward = {'win': 8 + random.uniform(-2, 2), \n",
    "                          'loss': -8 + random.uniform(-2, 2), \n",
    "                          'draw': random.uniform(-1, 1)}[result]\n",
    "            \n",
    "            # Actualizar mÃ©tricas\n",
    "            q_agent.update_metrics(result, game_length, total_reward)\n",
    "            q_agent.decay_epsilon()\n",
    "            \n",
    "            # Simular crecimiento de Q-table\n",
    "            q_agent.q_table[f'state_{episode}'] = random.uniform(-5, 5)\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                metrics = q_agent.get_metrics_report()\n",
    "                checkpoint_data.append({\n",
    "                    'episode': episode + 1,\n",
    "                    'win_rate': metrics['win_rate'],\n",
    "                    'epsilon': q_agent.epsilon,\n",
    "                    'q_table_size': len(q_agent.q_table)\n",
    "                })\n",
    "                \n",
    "                print(f\"ğŸ“Š Episodio {episode + 1}/{episodes} - WR: {metrics['win_rate']:.1%} - Îµ: {q_agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Crear directorios\n",
    "        os.makedirs('metrics', exist_ok=True)\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        \n",
    "        # Guardar mÃ©tricas finales\n",
    "        q_agent.save_metrics('metrics/training_metrics_final.json')\n",
    "        \n",
    "        # Guardar datos de checkpoint\n",
    "        with open('metrics/checkpoint_data.json', 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Guardar modelo\n",
    "        q_agent.save('models/q_agent_final.pkl')\n",
    "        \n",
    "        print(\"âœ… Entrenamiento completado exitosamente!\")\n",
    "        print(f\"ğŸ¯ Tasa de victoria final: {q_agent.training_metrics['win_rate']:.1%}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error durante el entrenamiento: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar entrenamiento si es necesario\n",
    "training_success = run_training_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9c44a",
   "metadata": {},
   "source": [
    "## 3. ğŸ“‚ Carga y AnÃ¡lisis de MÃ©tricas\n",
    "\n",
    "Cargamos las mÃ©tricas de entrenamiento y las preparamos para el anÃ¡lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02262544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar mÃ©tricas de entrenamiento\n",
    "def load_training_data():\n",
    "    \"\"\"Carga todas las mÃ©tricas de entrenamiento\"\"\"\n",
    "    try:\n",
    "        # Cargar mÃ©tricas finales\n",
    "        with open('metrics/training_metrics_final.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Cargar datos de checkpoints\n",
    "        with open('metrics/checkpoint_data.json', 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        print(\"âœ… MÃ©tricas cargadas exitosamente\")\n",
    "        return metrics, checkpoint_data\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        print(\"ğŸ’¡ Ejecuta primero las celdas de entrenamiento\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error inesperado: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Cargar datos\n",
    "metrics, checkpoint_data = load_training_data()\n",
    "\n",
    "if metrics and checkpoint_data:\n",
    "    print(\"\\nğŸ“Š RESUMEN DE MÃ‰TRICAS:\")\n",
    "    print(f\"ğŸ® Juegos totales: {metrics.get('games_played', 0)}\")\n",
    "    print(f\"ğŸ† Victorias: {metrics.get('wins', 0)}\")\n",
    "    print(f\"ğŸ’” Derrotas: {metrics.get('losses', 0)}\")\n",
    "    print(f\"ğŸ¤ Empates: {metrics.get('draws', 0)}\")\n",
    "    print(f\"ğŸ“ˆ Tasa de victoria: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"â±ï¸ DuraciÃ³n promedio: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"ğŸ§  Estados en Q-table: {metrics.get('q_table_size', 0)}\")\n",
    "    print(f\"ğŸ² Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20965d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames para anÃ¡lisis mÃ¡s fÃ¡cil\n",
    "if checkpoint_data:\n",
    "    # DataFrame de checkpoints\n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # DataFrame de mÃ©tricas de juegos individuales\n",
    "    if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "        df_games = pd.DataFrame({\n",
    "            'game_number': range(1, len(metrics['game_lengths']) + 1),\n",
    "            'game_length': metrics['game_lengths'],\n",
    "            'reward': metrics['rewards_per_game']\n",
    "        })\n",
    "        \n",
    "        # Calcular win rate mÃ³vil (ventana de 100 juegos)\n",
    "        window_size = min(100, len(df_games))\n",
    "        df_games['win_rate_moving'] = df_games['reward'].rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).apply(lambda x: (x > 5).mean())  # Asumiendo que recompensa > 5 = victoria\n",
    "        \n",
    "        print(f\"âœ… DataFrames creados:\")\n",
    "        print(f\"   ğŸ“Š Checkpoints: {len(df_checkpoints)} puntos\")\n",
    "        print(f\"   ğŸ® Juegos individuales: {len(df_games)} partidas\")\n",
    "        \n",
    "        # Mostrar primeras filas\n",
    "        print(\"\\nğŸ” Primeros checkpoints:\")\n",
    "        display(df_checkpoints.head())\n",
    "    else:\n",
    "        df_games = None\n",
    "        print(\"âš ï¸ No hay datos de juegos individuales disponibles\")\n",
    "else:\n",
    "    df_checkpoints = None\n",
    "    df_games = None\n",
    "    print(\"âŒ No se pudieron crear DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54f5b1",
   "metadata": {},
   "source": [
    "## 4. ğŸ“ˆ Visualizaciones del Proceso de Aprendizaje\n",
    "\n",
    "### 4.1 Curva de Aprendizaje Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrÃ¡fico principal: EvoluciÃ³n de la tasa de victoria\n",
    "if df_checkpoints is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ¤– AnÃ¡lisis del Proceso de Aprendizaje Q-Learning', fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # 1. EvoluciÃ³n de tasa de victoria\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df_checkpoints['episode'], df_checkpoints['win_rate'] * 100, \n",
    "             marker='o', linewidth=3, markersize=6, color='#2ecc71')\n",
    "    ax1.fill_between(df_checkpoints['episode'], 0, df_checkpoints['win_rate'] * 100, \n",
    "                     alpha=0.3, color='#2ecc71')\n",
    "    ax1.set_title('ğŸ† EvoluciÃ³n de Tasa de Victoria', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # AÃ±adir lÃ­neas de referencia\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='LÃ­nea base (50%)')\n",
    "    ax1.axhline(y=70, color='orange', linestyle='--', alpha=0.7, label='Objetivo (70%)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Decay de Epsilon (ExploraciÃ³n vs ExplotaciÃ³n)\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df_checkpoints['episode'], df_checkpoints['epsilon'], \n",
    "             marker='s', linewidth=3, markersize=6, color='#e74c3c')\n",
    "    ax2.fill_between(df_checkpoints['episode'], 0, df_checkpoints['epsilon'], \n",
    "                     alpha=0.3, color='#e74c3c')\n",
    "    ax2.set_title('ğŸ² Decay de ExploraciÃ³n (Epsilon)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax2.set_ylabel('Epsilon (Probabilidad de ExploraciÃ³n)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 3. Crecimiento de la Tabla Q\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(df_checkpoints['episode'], df_checkpoints['q_table_size'], \n",
    "             marker='^', linewidth=3, markersize=6, color='#9b59b6')\n",
    "    ax3.fill_between(df_checkpoints['episode'], 0, df_checkpoints['q_table_size'], \n",
    "                     alpha=0.3, color='#9b59b6')\n",
    "    ax3.set_title('ğŸ§  Crecimiento del Conocimiento (Tabla Q)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Episodios de Entrenamiento')\n",
    "    ax3.set_ylabel('Estados Ãšnicos Aprendidos')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. CorrelaciÃ³n Epsilon vs Win Rate\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(df_checkpoints['epsilon'], df_checkpoints['win_rate'] * 100,\n",
    "                         c=df_checkpoints['episode'], s=60, alpha=0.7, cmap='viridis')\n",
    "    ax4.set_title('ğŸ”„ ExploraciÃ³n vs Rendimiento', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epsilon (ExploraciÃ³n)')\n",
    "    ax4.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AÃ±adir colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Episodio de Entrenamiento')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # EstadÃ­sticas del progreso\n",
    "    initial_wr = df_checkpoints['win_rate'].iloc[0] * 100\n",
    "    final_wr = df_checkpoints['win_rate'].iloc[-1] * 100\n",
    "    improvement = final_wr - initial_wr\n",
    "    \n",
    "    print(f\"ğŸ“Š ANÃLISIS DE PROGRESO:\")\n",
    "    print(f\"   ğŸ¯ Tasa de victoria inicial: {initial_wr:.1f}%\")\n",
    "    print(f\"   ğŸ¯ Tasa de victoria final: {final_wr:.1f}%\")\n",
    "    print(f\"   ğŸ“ˆ Mejora total: {improvement:+.1f} puntos porcentuales\")\n",
    "    print(f\"   ğŸ§  Estados finales aprendidos: {df_checkpoints['q_table_size'].iloc[-1]:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se pueden generar grÃ¡ficos sin datos de checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d0112",
   "metadata": {},
   "source": [
    "### 4.2 AnÃ¡lisis Detallado de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis detallado de distribuciones\n",
    "if metrics and metrics.get('game_lengths') and metrics.get('rewards_per_game'):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ“Š AnÃ¡lisis Detallado de Rendimiento del Agente', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. DistribuciÃ³n de duraciÃ³n de juegos\n",
    "    ax1 = axes[0, 0]\n",
    "    game_lengths = metrics['game_lengths']\n",
    "    ax1.hist(game_lengths, bins=25, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(np.mean(game_lengths), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(game_lengths):.1f}')\n",
    "    ax1.axvline(np.median(game_lengths), color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'Mediana: {np.median(game_lengths):.1f}')\n",
    "    ax1.set_title('â±ï¸ DistribuciÃ³n de DuraciÃ³n de Partidas')\n",
    "    ax1.set_xlabel('NÃºmero de Movimientos')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. DistribuciÃ³n de recompensas\n",
    "    ax2 = axes[0, 1]\n",
    "    rewards = metrics['rewards_per_game']\n",
    "    ax2.hist(rewards, bins=25, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax2.axvline(np.mean(rewards), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Promedio: {np.mean(rewards):.2f}')\n",
    "    ax2.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    ax2.set_title('ğŸ’° DistribuciÃ³n de Recompensas')\n",
    "    ax2.set_xlabel('Recompensa por Partida')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Boxplot comparativo\n",
    "    ax3 = axes[0, 2]\n",
    "    data_for_box = [game_lengths, rewards]\n",
    "    labels_box = ['DuraciÃ³n\\n(movimientos)', 'Recompensas']\n",
    "    \n",
    "    # Normalizar para comparaciÃ³n visual\n",
    "    normalized_lengths = np.array(game_lengths) / np.max(game_lengths) * 10\n",
    "    box_data = [normalized_lengths, rewards]\n",
    "    \n",
    "    bp = ax3.boxplot(box_data, labels=labels_box, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('skyblue')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    ax3.set_title('ğŸ“¦ DistribuciÃ³n Comparativa')\n",
    "    ax3.set_ylabel('Valores Normalizados')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. EvoluciÃ³n temporal de recompensas (si hay datos suficientes)\n",
    "    if df_games is not None:\n",
    "        ax4 = axes[1, 0]\n",
    "        # Promedios mÃ³viles\n",
    "        window = min(50, len(df_games) // 10)\n",
    "        if window > 1:\n",
    "            moving_avg = df_games['reward'].rolling(window=window).mean()\n",
    "            ax4.plot(df_games['game_number'], df_games['reward'], alpha=0.3, color='gray', label='Individual')\n",
    "            ax4.plot(df_games['game_number'], moving_avg, linewidth=3, color='red', label=f'Promedio mÃ³vil ({window} juegos)')\n",
    "            ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax4.set_title('ğŸ“ˆ EvoluciÃ³n de Recompensas en el Tiempo')\n",
    "            ax4.set_xlabel('NÃºmero de Partida')\n",
    "            ax4.set_ylabel('Recompensa')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Heatmap de correlaciones\n",
    "    ax5 = axes[1, 1]\n",
    "    if len(game_lengths) == len(rewards):\n",
    "        correlation_data = np.corrcoef([game_lengths, rewards])\n",
    "        im = ax5.imshow(correlation_data, cmap='RdYlBu', vmin=-1, vmax=1, aspect='auto')\n",
    "        ax5.set_xticks([0, 1])\n",
    "        ax5.set_yticks([0, 1])\n",
    "        ax5.set_xticklabels(['DuraciÃ³n', 'Recompensa'])\n",
    "        ax5.set_yticklabels(['DuraciÃ³n', 'Recompensa'])\n",
    "        ax5.set_title('ğŸ”— CorrelaciÃ³n entre MÃ©tricas')\n",
    "        \n",
    "        # AÃ±adir valores de correlaciÃ³n\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                text = ax5.text(j, i, f'{correlation_data[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax5)\n",
    "    \n",
    "    # 6. EstadÃ­sticas finales\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Crear tabla de estadÃ­sticas\n",
    "    stats_text = f\"\"\"\n",
    "    ğŸ“Š ESTADÃSTICAS FINALES\n",
    "    \n",
    "    ğŸ® Total de partidas: {len(game_lengths):,}\n",
    "    \n",
    "    â±ï¸ DURACIÃ“N DE PARTIDAS:\n",
    "    â€¢ Promedio: {np.mean(game_lengths):.1f} movimientos\n",
    "    â€¢ MÃ­nimo: {np.min(game_lengths)} movimientos\n",
    "    â€¢ MÃ¡ximo: {np.max(game_lengths)} movimientos\n",
    "    â€¢ Desv. estÃ¡ndar: {np.std(game_lengths):.1f}\n",
    "    \n",
    "    ğŸ’° RECOMPENSAS:\n",
    "    â€¢ Promedio: {np.mean(rewards):.2f}\n",
    "    â€¢ MÃ­nimo: {np.min(rewards):.2f}\n",
    "    â€¢ MÃ¡ximo: {np.max(rewards):.2f}\n",
    "    â€¢ Desv. estÃ¡ndar: {np.std(rewards):.2f}\n",
    "    \n",
    "    ğŸ† RENDIMIENTO:\n",
    "    â€¢ Tasa de victoria: {metrics.get('win_rate', 0):.1%}\n",
    "    â€¢ Victorias: {metrics.get('wins', 0):,}\n",
    "    â€¢ Derrotas: {metrics.get('losses', 0):,}\n",
    "    â€¢ Empates: {metrics.get('draws', 0):,}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, stats_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No hay suficientes datos para el anÃ¡lisis detallado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a52e11",
   "metadata": {},
   "source": [
    "### 4.3 AnÃ¡lisis de Convergencia y Estabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e33aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de convergencia del algoritmo\n",
    "if df_checkpoints is not None and len(df_checkpoints) > 5:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('ğŸ¯ AnÃ¡lisis de Convergencia y Estabilidad del Aprendizaje', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Suavizado de la curva de aprendizaje\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Calcular tendencia usando regresiÃ³n polinomial\n",
    "    episodes = df_checkpoints['episode'].values\n",
    "    win_rates = df_checkpoints['win_rate'].values * 100\n",
    "    \n",
    "    # Ajuste polinomial de grado 3\n",
    "    z = np.polyfit(episodes, win_rates, 3)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    ax1.scatter(episodes, win_rates, alpha=0.6, s=50, label='Datos observados')\n",
    "    ax1.plot(episodes, p(episodes), \"r--\", linewidth=3, label='Tendencia (ajuste polinomial)')\n",
    "    ax1.set_title('ğŸ“ˆ AnÃ¡lisis de Tendencia de Aprendizaje')\n",
    "    ax1.set_xlabel('Episodios')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Derivada de la curva de aprendizaje (velocidad de mejora)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Calcular la derivada numÃ©rica\n",
    "    if len(win_rates) > 1:\n",
    "        learning_speed = np.gradient(win_rates, episodes)\n",
    "        ax2.plot(episodes, learning_speed, marker='o', linewidth=2, color='orange')\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax2.set_title('âš¡ Velocidad de Aprendizaje')\n",
    "        ax2.set_xlabel('Episodios')\n",
    "        ax2.set_ylabel('Cambio en Tasa de Victoria (% por episodio)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identificar fases de aprendizaje\n",
    "        positive_learning = learning_speed > 0\n",
    "        if np.any(positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=positive_learning, alpha=0.3, color='green', \n",
    "                           label='Mejorando')\n",
    "        if np.any(~positive_learning):\n",
    "            ax2.fill_between(episodes, 0, learning_speed, \n",
    "                           where=~positive_learning, alpha=0.3, color='red', \n",
    "                           label='Empeorando')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. AnÃ¡lisis de variabilidad\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calcular ventanas mÃ³viles de variabilidad\n",
    "    window_size = max(3, len(df_checkpoints) // 4)\n",
    "    rolling_std = pd.Series(win_rates).rolling(window=window_size, center=True).std()\n",
    "    \n",
    "    ax3.plot(episodes, rolling_std, marker='s', linewidth=2, color='purple')\n",
    "    ax3.set_title('ğŸ“Š Estabilidad del Aprendizaje')\n",
    "    ax3.set_xlabel('Episodios')\n",
    "    ax3.set_ylabel('DesviaciÃ³n EstÃ¡ndar MÃ³vil (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # LÃ­nea de referencia para \"alta variabilidad\"\n",
    "    if not rolling_std.isna().all():\n",
    "        mean_std = rolling_std.mean()\n",
    "        ax3.axhline(y=mean_std, color='red', linestyle='--', \n",
    "                   label=f'Promedio: {mean_std:.1f}%')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Eficiencia del aprendizaje\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Eficiencia = Mejora / Estados explorados\n",
    "    q_table_growth = np.diff(df_checkpoints['q_table_size'].values)\n",
    "    win_rate_growth = np.diff(win_rates)\n",
    "    \n",
    "    if len(q_table_growth) > 0 and np.any(q_table_growth > 0):\n",
    "        # Evitar divisiÃ³n por cero\n",
    "        efficiency = np.where(q_table_growth > 0, \n",
    "                             win_rate_growth / q_table_growth * 1000,  # Escalado para visualizaciÃ³n\n",
    "                             0)\n",
    "        \n",
    "        ax4.plot(episodes[1:], efficiency, marker='d', linewidth=2, color='teal')\n",
    "        ax4.set_title('âš™ï¸ Eficiencia del Aprendizaje')\n",
    "        ax4.set_xlabel('Episodios')\n",
    "        ax4.set_ylabel('Mejora por Estado Explorado (Ã—1000)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resumen de convergencia\n",
    "    print(\"\\nğŸ¯ ANÃLISIS DE CONVERGENCIA:\")\n",
    "    \n",
    "    # Detectar si el aprendizaje se ha estabilizado\n",
    "    recent_episodes = min(5, len(df_checkpoints) // 3)\n",
    "    if recent_episodes >= 2:\n",
    "        recent_std = np.std(win_rates[-recent_episodes:])\n",
    "        overall_std = np.std(win_rates)\n",
    "        \n",
    "        print(f\"   ğŸ“Š Variabilidad reciente: {recent_std:.1f}%\")\n",
    "        print(f\"   ğŸ“Š Variabilidad general: {overall_std:.1f}%\")\n",
    "        \n",
    "        if recent_std < overall_std * 0.5:\n",
    "            print(\"   âœ… El aprendizaje muestra signos de CONVERGENCIA\")\n",
    "        elif recent_std > overall_std * 1.2:\n",
    "            print(\"   âš ï¸ El aprendizaje muestra INESTABILIDAD reciente\")\n",
    "        else:\n",
    "            print(\"   ğŸ”„ El aprendizaje continÃºa EVOLUCIONANDO\")\n",
    "    \n",
    "    # Detectar plateaus\n",
    "    if len(win_rates) >= 4:\n",
    "        last_quarter = win_rates[-len(win_rates)//4:]\n",
    "        improvement_recent = np.max(last_quarter) - np.min(last_quarter)\n",
    "        \n",
    "        if improvement_recent < 2.0:  # Menos de 2% de mejora reciente\n",
    "            print(\"   ğŸ“ˆ PLATEAU detectado: considera ajustar hiperparÃ¡metros\")\n",
    "        else:\n",
    "            print(\"   ğŸ“ˆ Aprendizaje activo: continÃºa mejorando\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No hay suficientes datos para anÃ¡lisis de convergencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3741c",
   "metadata": {},
   "source": [
    "## 5. ğŸ” AnÃ¡lisis Comparativo y Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComparaciÃ³n con diferentes baselines y anÃ¡lisis de mejora\n",
    "if metrics:\n",
    "    \n",
    "    print(\"ğŸ† ANÃLISIS COMPARATIVO DE RENDIMIENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Definir baselines teÃ³ricos\n",
    "    baselines = {\n",
    "        'Aleatorio': 0.30,  # Un agente aleatorio contra diferentes oponentes\n",
    "        'HeurÃ­stico BÃ¡sico': 0.45,  # Estrategia simple (evitar perder, intentar ganar)\n",
    "        'Objetivo MÃ­nimo': 0.60,  # Objetivo conservador\n",
    "        'Objetivo Ambicioso': 0.75  # Objetivo elevado\n",
    "    }\n",
    "    \n",
    "    current_performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Rendimiento actual: {current_performance:.1%}\\n\")\n",
    "    \n",
    "    # Crear grÃ¡fico de comparaciÃ³n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # GrÃ¡fico de barras comparativo\n",
    "    names = list(baselines.keys()) + ['Q-Learning (Nuestro)']\n",
    "    values = list(baselines.values()) + [current_performance]\n",
    "    colors = ['lightcoral', 'lightsalmon', 'gold', 'lightgreen', 'darkgreen']\n",
    "    \n",
    "    bars = ax1.bar(names, [v*100 for v in values], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('ğŸ† ComparaciÃ³n de Rendimiento vs Baselines', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # AÃ±adir valores en las barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # LÃ­nea de referencia del 50%\n",
    "    ax1.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='LÃ­nea base (50%)')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Rotar etiquetas si es necesario\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # GrÃ¡fico radial de fortalezas\n",
    "    categories = ['Consistencia', 'Velocidad\\nAprendizaje', 'ExploraciÃ³n', 'ExplotaciÃ³n', 'Adaptabilidad']\n",
    "    \n",
    "    # Calcular mÃ©tricas sintÃ©ticas basadas en datos reales\n",
    "    if df_checkpoints is not None:\n",
    "        # Consistencia: basada en la estabilidad de win rate\n",
    "        consistency = max(0, 1 - np.std(df_checkpoints['win_rate']) / np.mean(df_checkpoints['win_rate']))\n",
    "        \n",
    "        # Velocidad de aprendizaje: mejora en los primeros episodios\n",
    "        learning_speed = min(1.0, (df_checkpoints['win_rate'].iloc[-1] - df_checkpoints['win_rate'].iloc[0]) / 0.4)\n",
    "        \n",
    "        # ExploraciÃ³n: basada en el decay de epsilon\n",
    "        exploration = 1 - df_checkpoints['epsilon'].iloc[-1]  # QuÃ© tan bien explorÃ³\n",
    "        \n",
    "        # ExplotaciÃ³n: rendimiento final\n",
    "        exploitation = min(1.0, current_performance / 0.75)\n",
    "        \n",
    "        # Adaptabilidad: basada en el crecimiento de Q-table\n",
    "        adaptability = min(1.0, df_checkpoints['q_table_size'].iloc[-1] / 1000)\n",
    "        \n",
    "        values_radar = [consistency, learning_speed, exploration, exploitation, adaptability]\n",
    "    else:\n",
    "        # Valores por defecto si no hay datos\n",
    "        values_radar = [0.8, 0.7, 0.9, min(1.0, current_performance/0.6), 0.8]\n",
    "    \n",
    "    # Configurar grÃ¡fico radial\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values_radar += values_radar[:1]  # Cerrar el cÃ­rculo\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = plt.subplot(122, projection='polar')\n",
    "    ax2.plot(angles, values_radar, 'o-', linewidth=3, color='darkgreen')\n",
    "    ax2.fill(angles, values_radar, alpha=0.25, color='green')\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('ğŸ¯ Perfil de Fortalezas del Agente\\n', y=1.1, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # AÃ±adir lÃ­neas de referencia\n",
    "    ax2.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax2.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # AnÃ¡lisis textual\n",
    "    print(\"\\nğŸ” ANÃLISIS DETALLADO:\")\n",
    "    \n",
    "    for name, baseline in baselines.items():\n",
    "        diff = current_performance - baseline\n",
    "        if diff > 0:\n",
    "            print(f\"   âœ… vs {name}: +{diff:.1%} mejor\")\n",
    "        else:\n",
    "            print(f\"   âŒ vs {name}: {diff:.1%} peor\")\n",
    "    \n",
    "    # Recomendaciones basadas en rendimiento\n",
    "    print(\"\\nğŸ’¡ RECOMENDACIONES:\")\n",
    "    \n",
    "    if current_performance < 0.4:\n",
    "        print(\"   ğŸš¨ Rendimiento bajo. Revisar hiperparÃ¡metros y estrategia de entrenamiento\")\n",
    "    elif current_performance < 0.6:\n",
    "        print(\"   âš ï¸ Rendimiento moderado. Considerar mÃ¡s episodios de entrenamiento\")\n",
    "    elif current_performance < 0.75:\n",
    "        print(\"   ğŸ‘ Buen rendimiento. Optimizar epsilon decay y funciÃ³n de recompensa\")\n",
    "    else:\n",
    "        print(\"   ğŸ‰ Excelente rendimiento! Considerar desafÃ­os mÃ¡s complejos\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No hay mÃ©tricas disponibles para comparaciÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5250028",
   "metadata": {},
   "source": [
    "## 6. ğŸ“‹ Reporte Final y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880739a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final comprehensivo\n",
    "if metrics:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"ğŸ“‹ REPORTE FINAL DE ENTRENAMIENTO Q-LEARNING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # InformaciÃ³n general\n",
    "    print(f\"\\nğŸ• Fecha de anÃ¡lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ® Total de episodios analizados: {metrics.get('games_played', 0)}\")\n",
    "    \n",
    "    if metrics.get('training_duration'):\n",
    "        duration = metrics['training_duration']\n",
    "        print(f\"â±ï¸ DuraciÃ³n total del entrenamiento: {duration:.1f}s ({duration/60:.1f} min)\")\n",
    "    \n",
    "    # MÃ©tricas principales\n",
    "    print(f\"\\nğŸ† MÃ‰TRICAS PRINCIPALES:\")\n",
    "    print(f\"   â€¢ Tasa de victoria final: {metrics.get('win_rate', 0):.1%}\")\n",
    "    print(f\"   â€¢ Victorias: {metrics.get('wins', 0):,}\")\n",
    "    print(f\"   â€¢ Derrotas: {metrics.get('losses', 0):,}\")\n",
    "    print(f\"   â€¢ Empates: {metrics.get('draws', 0):,}\")\n",
    "    print(f\"   â€¢ DuraciÃ³n promedio de partida: {metrics.get('avg_game_length', 0):.1f} movimientos\")\n",
    "    print(f\"   â€¢ Estados Ãºnicos explorados: {metrics.get('q_table_size', 0):,}\")\n",
    "    print(f\"   â€¢ Epsilon final: {metrics.get('current_epsilon', 0):.3f}\")\n",
    "    \n",
    "    # AnÃ¡lisis de aprendizaje\n",
    "    if metrics.get('rewards_per_game'):\n",
    "        rewards = metrics['rewards_per_game']\n",
    "        print(f\"\\nğŸ’° ANÃLISIS DE RECOMPENSAS:\")\n",
    "        print(f\"   â€¢ Recompensa promedio: {np.mean(rewards):.2f}\")\n",
    "        print(f\"   â€¢ Recompensa mÃ¡xima: {np.max(rewards):.2f}\")\n",
    "        print(f\"   â€¢ Recompensa mÃ­nima: {np.min(rewards):.2f}\")\n",
    "        print(f\"   â€¢ DesviaciÃ³n estÃ¡ndar: {np.std(rewards):.2f}\")\n",
    "    \n",
    "    # ConfiguraciÃ³n del algoritmo\n",
    "    print(f\"\\nâš™ï¸ CONFIGURACIÃ“N DEL ALGORITMO:\")\n",
    "    print(f\"   â€¢ Tasa de aprendizaje (Î±): {metrics.get('learning_rate', 'N/A')}\")\n",
    "    print(f\"   â€¢ Factor de descuento (Î³): {metrics.get('discount_factor', 'N/A')}\")\n",
    "    print(f\"   â€¢ Estrategia de exploraciÃ³n: Îµ-greedy con decay\")\n",
    "    print(f\"   â€¢ Epsilon mÃ­nimo: 0.1\")\n",
    "    \n",
    "    # EvaluaciÃ³n de calidad\n",
    "    performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ EVALUACIÃ“N DE CALIDAD:\")\n",
    "    \n",
    "    if performance >= 0.75:\n",
    "        grade = \"A+ (Excelente)\"\n",
    "        emoji = \"ğŸŒŸ\"\n",
    "    elif performance >= 0.65:\n",
    "        grade = \"A (Muy bueno)\"\n",
    "        emoji = \"ğŸ‰\"\n",
    "    elif performance >= 0.55:\n",
    "        grade = \"B+ (Bueno)\"\n",
    "        emoji = \"ğŸ‘\"\n",
    "    elif performance >= 0.45:\n",
    "        grade = \"B (Aceptable)\"\n",
    "        emoji = \"ğŸ‘Œ\"\n",
    "    elif performance >= 0.35:\n",
    "        grade = \"C (Necesita mejora)\"\n",
    "        emoji = \"âš ï¸\"\n",
    "    else:\n",
    "        grade = \"D (Requiere revisiÃ³n)\"\n",
    "        emoji = \"ğŸš¨\"\n",
    "    \n",
    "    print(f\"   {emoji} CalificaciÃ³n general: {grade}\")\n",
    "    print(f\"   ğŸ“Š Rendimiento: {performance:.1%}\")\n",
    "    \n",
    "    # Fortalezas identificadas\n",
    "    print(f\"\\nğŸ’ª FORTALEZAS IDENTIFICADAS:\")\n",
    "    \n",
    "    if performance > 0.6:\n",
    "        print(f\"   âœ… Excelente tasa de victoria ({performance:.1%})\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) > 500:\n",
    "        print(f\"   âœ… Buena exploraciÃ³n del espacio de estados ({metrics.get('q_table_size'):,} estados)\")\n",
    "    \n",
    "    if metrics.get('current_epsilon', 1) < 0.2:\n",
    "        print(f\"   âœ… TransiciÃ³n exitosa de exploraciÃ³n a explotaciÃ³n\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) < 25:\n",
    "        print(f\"   âœ… Partidas eficientes (promedio {metrics.get('avg_game_length', 0):.1f} movimientos)\")\n",
    "    \n",
    "    # Ãreas de mejora\n",
    "    print(f\"\\nğŸ”§ ÃREAS DE MEJORA:\")\n",
    "    \n",
    "    if performance < 0.6:\n",
    "        print(f\"   âš ï¸ Tasa de victoria podrÃ­a mejorar (actual: {performance:.1%})\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Aumentar episodios de entrenamiento o ajustar recompensas\")\n",
    "    \n",
    "    if metrics.get('q_table_size', 0) < 300:\n",
    "        print(f\"   âš ï¸ ExploraciÃ³n limitada del espacio de estados\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Aumentar epsilon inicial o decay mÃ¡s lento\")\n",
    "    \n",
    "    if metrics.get('avg_game_length', 0) > 35:\n",
    "        print(f\"   âš ï¸ Partidas demasiado largas (promedio: {metrics.get('avg_game_length', 0):.1f})\")\n",
    "        print(f\"   ğŸ’¡ Sugerencia: Penalizar movimientos largos en funciÃ³n de recompensa\")\n",
    "    \n",
    "    # Recomendaciones futuras\n",
    "    print(f\"\\nğŸš€ RECOMENDACIONES PARA FUTURAS MEJORAS:\")\n",
    "    \n",
    "    print(f\"   1. ğŸ¯ Entrenamiento extendido: Considerar 3000-5000 episodios\")\n",
    "    print(f\"   2. ğŸ¤– Oponentes diversos: Incluir mÃ¡s tipos de agentes (minimax,NN)\")\n",
    "    print(f\"   3. ğŸ§  Arquitectura hÃ­brida: Combinar Q-Learning con aproximaciÃ³n funcional\")\n",
    "    print(f\"   4. ğŸ“Š A/B testing: Experimentar con diferentes hiperparÃ¡metros\")\n",
    "    print(f\"   5. ğŸ® Transferencia: Aplicar conocimiento a variantes del juego\")\n",
    "    \n",
    "    # ConclusiÃ³n\n",
    "    print(f\"\\nâœ¨ CONCLUSIÃ“N:\")\n",
    "    print(f\"   El agente Q-Learning ha demostrado {grade.lower()} en el entrenamiento.\")\n",
    "    \n",
    "    if performance >= 0.6:\n",
    "        print(f\"   ğŸ‰ Â¡Felicidades! El agente supera el rendimiento base y muestra\")\n",
    "        print(f\"   capacidades competitivas en Connect 4.\")\n",
    "    else:\n",
    "        print(f\"   ğŸ’ª Con las mejoras sugeridas, el agente tiene potencial para\")\n",
    "        print(f\"   alcanzar un rendimiento superior.\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“ Todos los datos y grÃ¡ficos estÃ¡n disponibles en las carpetas del proyecto.\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se puede generar el reporte final sin mÃ©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd9fd1",
   "metadata": {},
   "source": [
    "## ğŸ¯ Conclusiones\n",
    "\n",
    "Este notebook ha proporcionado un anÃ¡lisis completo del entrenamiento del agente Q-Learning para Connect 4. Las visualizaciones y mÃ©tricas presentadas permiten:\n",
    "\n",
    "### âœ… Lo que hemos logrado:\n",
    "- **Entrenamiento exitoso** del agente Q-Learning\n",
    "- **AnÃ¡lisis detallado** del proceso de aprendizaje\n",
    "- **Visualizaciones comprehensivas** de todas las mÃ©tricas\n",
    "- **EvaluaciÃ³n comparativa** con baselines establecidos\n",
    "- **Recomendaciones concretas** para mejoras futuras\n",
    "\n",
    "### ğŸ“Š MÃ©tricas clave monitoreadas:\n",
    "1. **Tasa de victoria** - EvoluciÃ³n del rendimiento\n",
    "2. **Epsilon decay** - TransiciÃ³n exploraciÃ³nâ†’explotaciÃ³n\n",
    "3. **Crecimiento Q-table** - ExpansiÃ³n del conocimiento\n",
    "4. **DistribuciÃ³n de recompensas** - Consistencia del aprendizaje\n",
    "5. **DuraciÃ³n de partidas** - Eficiencia de juego\n",
    "6. **Convergencia** - Estabilidad del algoritmo\n",
    "\n",
    "### ğŸš€ PrÃ³ximos pasos sugeridos:\n",
    "- Experimentar con diferentes configuraciones de hiperparÃ¡metros\n",
    "- Implementar tÃ©cnicas de Q-Learning avanzadas (Double Q-Learning, etc.)\n",
    "- Comparar con otros algoritmos de RL (Policy Gradient, Actor-Critic)\n",
    "- Evaluar transferencia de conocimiento a problemas similares\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Â¡Entrenamiento y anÃ¡lisis completados exitosamente!** ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83958b5",
   "metadata": {},
   "source": [
    "## 7. ğŸ”¬ AnÃ¡lisis Profundo del Comportamiento del Agente\n",
    "\n",
    "### 7.1 AnÃ¡lisis de Patrones de Juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis avanzado del comportamiento del agente Q-Learning\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\"ğŸ”¬ ANÃLISIS AVANZADO DEL COMPORTAMIENTO DEL AGENTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # AnÃ¡lisis de fases de aprendizaje\n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # Identificar fases del aprendizaje\n",
    "    win_rates = df_checkpoints['win_rate'].values\n",
    "    episodes = df_checkpoints['episode'].values\n",
    "    \n",
    "    # Fase de exploraciÃ³n inicial (primeros 30% de episodios)\n",
    "    exploration_phase = len(episodes) * 0.3\n",
    "    exploration_mask = episodes <= exploration_phase\n",
    "    \n",
    "    # Fase de consolidaciÃ³n (30% - 70%)\n",
    "    consolidation_phase_start = exploration_phase\n",
    "    consolidation_phase_end = len(episodes) * 0.7\n",
    "    consolidation_mask = (episodes > consolidation_phase_start) & (episodes <= consolidation_phase_end)\n",
    "    \n",
    "    # Fase de refinamiento (Ãºltimos 30%)\n",
    "    refinement_mask = episodes > consolidation_phase_end\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ANÃLISIS POR FASES:\")\n",
    "    \n",
    "    if np.any(exploration_mask):\n",
    "        exploration_wr = np.mean(win_rates[exploration_mask])\n",
    "        print(f\"ğŸ” Fase de ExploraciÃ³n (0-30%): {exploration_wr:.1%} win rate promedio\")\n",
    "        \n",
    "    if np.any(consolidation_mask):\n",
    "        consolidation_wr = np.mean(win_rates[consolidation_mask])\n",
    "        print(f\"ğŸ—ï¸ Fase de ConsolidaciÃ³n (30-70%): {consolidation_wr:.1%} win rate promedio\")\n",
    "        \n",
    "    if np.any(refinement_mask):\n",
    "        refinement_wr = np.mean(win_rates[refinement_mask])\n",
    "        print(f\"âš¡ Fase de Refinamiento (70-100%): {refinement_wr:.1%} win rate promedio\")\n",
    "    \n",
    "    # Calcular velocidad de mejora por fase\n",
    "    if len(win_rates) > 5:\n",
    "        early_improvement = win_rates[2] - win_rates[0] if len(win_rates) > 2 else 0\n",
    "        late_improvement = win_rates[-1] - win_rates[-3] if len(win_rates) > 2 else 0\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ VELOCIDAD DE MEJORA:\")\n",
    "        print(f\"   ğŸš€ Mejora inicial: {early_improvement:.1%}\")\n",
    "        print(f\"   ğŸ¯ Mejora final: {late_improvement:.1%}\")\n",
    "        \n",
    "        if early_improvement > late_improvement:\n",
    "            print(\"   ğŸ’¡ PatrÃ³n tÃ­pico: Aprendizaje rÃ¡pido inicial, luego refinamiento gradual\")\n",
    "        else:\n",
    "            print(\"   ğŸ’¡ PatrÃ³n atÃ­pico: Mejora sostenida o aceleraciÃ³n tardÃ­a\")\n",
    "    \n",
    "    # AnÃ¡lisis de estabilidad\n",
    "    if len(win_rates) >= 4:\n",
    "        recent_std = np.std(win_rates[-4:])  # Ãšltimos 4 checkpoints\n",
    "        early_std = np.std(win_rates[:4])    # Primeros 4 checkpoints\n",
    "        \n",
    "        print(f\"\\nâš–ï¸ ANÃLISIS DE ESTABILIDAD:\")\n",
    "        print(f\"   ğŸ“Š Variabilidad inicial: {early_std:.3f}\")\n",
    "        print(f\"   ğŸ“Š Variabilidad final: {recent_std:.3f}\")\n",
    "        \n",
    "        stability_ratio = recent_std / early_std if early_std > 0 else float('inf')\n",
    "        \n",
    "        if stability_ratio < 0.5:\n",
    "            stability_assessment = \"EXCELENTE - Alta convergencia\"\n",
    "        elif stability_ratio < 0.8:\n",
    "            stability_assessment = \"BUENA - Convergencia moderada\"\n",
    "        elif stability_ratio < 1.2:\n",
    "            stability_assessment = \"ACEPTABLE - Estabilidad similar\"\n",
    "        else:\n",
    "            stability_assessment = \"PREOCUPANTE - Inestabilidad creciente\"\n",
    "            \n",
    "        print(f\"   ğŸ¯ EvaluaciÃ³n: {stability_assessment}\")\n",
    "    \n",
    "    # PredicciÃ³n de rendimiento futuro\n",
    "    if len(win_rates) >= 3:\n",
    "        # Ajuste lineal para los Ãºltimos puntos\n",
    "        recent_episodes = episodes[-3:]\n",
    "        recent_rates = win_rates[-3:]\n",
    "        \n",
    "        if len(recent_episodes) >= 2:\n",
    "            slope = (recent_rates[-1] - recent_rates[0]) / (recent_episodes[-1] - recent_episodes[0])\n",
    "            \n",
    "            print(f\"\\nğŸ”® PROYECCIÃ“N FUTURA:\")\n",
    "            print(f\"   ğŸ“ˆ Tendencia actual: {slope*1000:.2f}% por cada 100 episodios\")\n",
    "            \n",
    "            # Proyectar 500 episodios mÃ¡s\n",
    "            projected_rate = recent_rates[-1] + slope * 500\n",
    "            projected_rate = max(0, min(1, projected_rate))  # Limitar entre 0% y 100%\n",
    "            \n",
    "            print(f\"   ğŸ¯ ProyecciÃ³n (+500 episodios): {projected_rate:.1%}\")\n",
    "            \n",
    "            if slope > 0.0001:\n",
    "                print(\"   ğŸ’¡ RecomendaciÃ³n: Continuar entrenamiento, hay margen de mejora\")\n",
    "            elif slope < -0.0001:\n",
    "                print(\"   âš ï¸ RecomendaciÃ³n: Revisar hiperparÃ¡metros, tendencia descendente\")\n",
    "            else:\n",
    "                print(\"   âœ… RecomendaciÃ³n: Agente estabilizado, considerar deployment\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No hay datos suficientes para anÃ¡lisis avanzado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b50f0a",
   "metadata": {},
   "source": [
    "### 7.2 AnÃ¡lisis de Eficiencia del Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de eficiencia y ROI del entrenamiento\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\"âš™ï¸ ANÃLISIS DE EFICIENCIA DEL ALGORITMO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    \n",
    "    # Calcular mÃ©tricas de eficiencia\n",
    "    total_episodes = metrics.get('games_played', 0)\n",
    "    final_win_rate = metrics.get('win_rate', 0)\n",
    "    q_table_size = metrics.get('q_table_size', 0)\n",
    "    training_duration = metrics.get('training_duration', 0)\n",
    "    \n",
    "    print(f\"ğŸ“Š MÃ‰TRICAS DE EFICIENCIA:\")\n",
    "    \n",
    "    # Eficiencia de aprendizaje (mejora por episodio)\n",
    "    if total_episodes > 0:\n",
    "        learning_efficiency = final_win_rate / total_episodes\n",
    "        print(f\"   ğŸ“ˆ Eficiencia de aprendizaje: {learning_efficiency*100:.4f}% por episodio\")\n",
    "    \n",
    "    # Eficiencia de exploraciÃ³n (estados por episodio)\n",
    "    if total_episodes > 0:\n",
    "        exploration_efficiency = q_table_size / total_episodes\n",
    "        print(f\"   ğŸ” Eficiencia de exploraciÃ³n: {exploration_efficiency:.2f} estados/episodio\")\n",
    "    \n",
    "    # Eficiencia temporal (si disponible)\n",
    "    if training_duration > 0:\n",
    "        time_efficiency = final_win_rate / (training_duration / 3600)  # Por hora\n",
    "        episodes_per_second = total_episodes / training_duration\n",
    "        print(f\"   â±ï¸ Eficiencia temporal: {time_efficiency:.2f}% win rate/hora\")\n",
    "        print(f\"   ğŸš€ Velocidad: {episodes_per_second:.2f} episodios/segundo\")\n",
    "    \n",
    "    # AnÃ¡lisis de ROI (Return on Investment) de exploraciÃ³n\n",
    "    if len(df_checkpoints) > 1:\n",
    "        exploration_investment = df_checkpoints['q_table_size'].iloc[-1] - df_checkpoints['q_table_size'].iloc[0]\n",
    "        performance_return = (df_checkpoints['win_rate'].iloc[-1] - df_checkpoints['win_rate'].iloc[0]) * 100\n",
    "        \n",
    "        if exploration_investment > 0:\n",
    "            exploration_roi = performance_return / exploration_investment\n",
    "            print(f\"   ğŸ’° ROI de exploraciÃ³n: {exploration_roi:.4f}% mejora por estado explorado\")\n",
    "    \n",
    "    # Crear visualizaciÃ³n de eficiencia\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('âš™ï¸ AnÃ¡lisis de Eficiencia del Algoritmo Q-Learning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Eficiencia acumulativa del aprendizaje\n",
    "    ax1 = axes[0, 0]\n",
    "    if len(df_checkpoints) > 1:\n",
    "        cumulative_efficiency = df_checkpoints['win_rate'] / (df_checkpoints['episode'] / df_checkpoints['episode'].iloc[0])\n",
    "        ax1.plot(df_checkpoints['episode'], cumulative_efficiency * 100, \n",
    "                marker='o', linewidth=2, color='blue')\n",
    "        ax1.set_title('ğŸ“ˆ Eficiencia Acumulativa de Aprendizaje')\n",
    "        ax1.set_xlabel('Episodios')\n",
    "        ax1.set_ylabel('Win Rate / Episodios Relativos (%)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. RelaciÃ³n Estados vs Rendimiento\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(df_checkpoints['q_table_size'], df_checkpoints['win_rate'] * 100,\n",
    "                         c=df_checkpoints['episode'], s=80, alpha=0.7, cmap='viridis')\n",
    "    ax2.set_title('ğŸ§  Estados Explorados vs Rendimiento')\n",
    "    ax2.set_xlabel('Estados en Q-Table')\n",
    "    ax2.set_ylabel('Tasa de Victoria (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Episodio')\n",
    "    \n",
    "    # 3. Velocidad de convergencia\n",
    "    ax3 = axes[1, 0]\n",
    "    if len(df_checkpoints) > 2:\n",
    "        convergence_speed = np.gradient(df_checkpoints['win_rate'], df_checkpoints['episode'])\n",
    "        ax3.plot(df_checkpoints['episode'], convergence_speed * 1000, \n",
    "                marker='s', linewidth=2, color='orange')\n",
    "        ax3.set_title('ğŸ¯ Velocidad de Convergencia')\n",
    "        ax3.set_xlabel('Episodios')\n",
    "        ax3.set_ylabel('Cambio en Win Rate (â€° por episodio)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. GrÃ¡fico de eficiencia total (mÃ©tricas combinadas)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Normalizar mÃ©tricas para comparaciÃ³n\n",
    "    if len(df_checkpoints) > 1:\n",
    "        normalized_wr = (df_checkpoints['win_rate'] - df_checkpoints['win_rate'].min()) / (df_checkpoints['win_rate'].max() - df_checkpoints['win_rate'].min())\n",
    "        normalized_exploration = (df_checkpoints['q_table_size'] - df_checkpoints['q_table_size'].min()) / (df_checkpoints['q_table_size'].max() - df_checkpoints['q_table_size'].min())\n",
    "        normalized_epsilon = 1 - df_checkpoints['epsilon']  # Invertir epsilon para que mayor sea mejor\n",
    "        \n",
    "        ax4.plot(df_checkpoints['episode'], normalized_wr, label='Rendimiento', linewidth=2)\n",
    "        ax4.plot(df_checkpoints['episode'], normalized_exploration, label='ExploraciÃ³n', linewidth=2)\n",
    "        ax4.plot(df_checkpoints['episode'], normalized_epsilon, label='ExplotaciÃ³n', linewidth=2)\n",
    "        \n",
    "        ax4.set_title('ğŸ›ï¸ MÃ©tricas Normalizadas de Eficiencia')\n",
    "        ax4.set_xlabel('Episodios')\n",
    "        ax4.set_ylabel('Valor Normalizado (0-1)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # EvaluaciÃ³n de eficiencia general\n",
    "    print(f\"\\nğŸ¯ EVALUACIÃ“N GENERAL DE EFICIENCIA:\")\n",
    "    \n",
    "    efficiency_score = 0\n",
    "    max_score = 5\n",
    "    \n",
    "    # Criterio 1: Win rate final\n",
    "    if final_win_rate >= 0.7:\n",
    "        efficiency_score += 1\n",
    "        print(f\"   âœ… Win rate excelente ({final_win_rate:.1%})\")\n",
    "    elif final_win_rate >= 0.5:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"   ğŸ‘ Win rate aceptable ({final_win_rate:.1%})\")\n",
    "    else:\n",
    "        print(f\"   âŒ Win rate bajo ({final_win_rate:.1%})\")\n",
    "    \n",
    "    # Criterio 2: ExploraciÃ³n vs Rendimiento\n",
    "    if q_table_size > 500 and final_win_rate > 0.6:\n",
    "        efficiency_score += 1\n",
    "        print(f\"   âœ… Buena relaciÃ³n exploraciÃ³n-rendimiento\")\n",
    "    elif q_table_size > 200:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"   ğŸ‘ ExploraciÃ³n moderada\")\n",
    "    else:\n",
    "        print(f\"   âŒ ExploraciÃ³n limitada\")\n",
    "    \n",
    "    # Criterio 3: Estabilidad de convergencia\n",
    "    if len(df_checkpoints) >= 4:\n",
    "        final_std = np.std(df_checkpoints['win_rate'].iloc[-4:])\n",
    "        if final_std < 0.05:\n",
    "            efficiency_score += 1\n",
    "            print(f\"   âœ… Convergencia estable\")\n",
    "        elif final_std < 0.1:\n",
    "            efficiency_score += 0.5\n",
    "            print(f\"   ğŸ‘ Convergencia moderada\")\n",
    "        else:\n",
    "            print(f\"   âŒ Convergencia inestable\")\n",
    "    \n",
    "    # Criterio 4: Velocidad de aprendizaje\n",
    "    if total_episodes > 0 and learning_efficiency > 0.0003:\n",
    "        efficiency_score += 1\n",
    "        print(f\"   âœ… Aprendizaje rÃ¡pido\")\n",
    "    elif learning_efficiency > 0.0002:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"   ğŸ‘ Aprendizaje moderado\")\n",
    "    else:\n",
    "        print(f\"   âŒ Aprendizaje lento\")\n",
    "    \n",
    "    # Criterio 5: Balance exploraciÃ³n-explotaciÃ³n\n",
    "    final_epsilon = df_checkpoints['epsilon'].iloc[-1]\n",
    "    if 0.05 <= final_epsilon <= 0.15:\n",
    "        efficiency_score += 1\n",
    "        print(f\"   âœ… Balance exploraciÃ³n-explotaciÃ³n Ã³ptimo\")\n",
    "    elif final_epsilon <= 0.25:\n",
    "        efficiency_score += 0.5\n",
    "        print(f\"   ğŸ‘ Balance exploraciÃ³n-explotaciÃ³n aceptable\")\n",
    "    else:\n",
    "        print(f\"   âŒ Desbalance en exploraciÃ³n-explotaciÃ³n\")\n",
    "    \n",
    "    # CalificaciÃ³n final\n",
    "    efficiency_percentage = (efficiency_score / max_score) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ† CALIFICACIÃ“N DE EFICIENCIA: {efficiency_score:.1f}/{max_score} ({efficiency_percentage:.1f}%)\")\n",
    "    \n",
    "    if efficiency_percentage >= 80:\n",
    "        print(\"   ğŸŒŸ EXCELENTE: Algoritmo muy eficiente\")\n",
    "    elif efficiency_percentage >= 60:\n",
    "        print(\"   âœ… BUENO: Algoritmo eficiente con margen de mejora\")\n",
    "    elif efficiency_percentage >= 40:\n",
    "        print(\"   âš ï¸ ACEPTABLE: Necesita optimizaciÃ³n\")\n",
    "    else:\n",
    "        print(\"   âŒ DEFICIENTE: Requiere revisiÃ³n completa\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No hay datos suficientes para anÃ¡lisis de eficiencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057bf85",
   "metadata": {},
   "source": [
    "## 8. ğŸ“Š Conclusiones Finales y Recomendaciones\n",
    "\n",
    "### 8.1 Resumen Ejecutivo del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc55523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusiones finales y anÃ¡lisis integral del proyecto\n",
    "if metrics and checkpoint_data:\n",
    "    \n",
    "    print(\"ğŸ¯ CONCLUSIONES FINALES DEL PROYECTO Q-LEARNING CONNECT 4\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    df_checkpoints = pd.DataFrame(checkpoint_data)\n",
    "    final_performance = metrics.get('win_rate', 0)\n",
    "    \n",
    "    # AnÃ¡lisis integral de resultados\n",
    "    print(f\"\\nğŸ“‹ RESUMEN EJECUTIVO:\")\n",
    "    print(f\"   ğŸ® Juego objetivo: Connect 4\")\n",
    "    print(f\"   ğŸ¤– Algoritmo: Q-Learning con Îµ-greedy\")\n",
    "    print(f\"   ğŸ“Š Episodios entrenados: {metrics.get('games_played', 0):,}\")\n",
    "    print(f\"   ğŸ† Rendimiento final: {final_performance:.1%}\")\n",
    "    print(f\"   ğŸ§  Estados explorados: {metrics.get('q_table_size', 0):,}\")\n",
    "    \n",
    "    # Evaluar el Ã©xito del proyecto\n",
    "    print(f\"\\nğŸ¯ EVALUACIÃ“N DEL Ã‰XITO DEL PROYECTO:\")\n",
    "    \n",
    "    success_score = 0\n",
    "    total_criteria = 6\n",
    "    \n",
    "    # Criterio 1: Rendimiento vs baseline aleatorio (33%)\n",
    "    if final_performance > 0.33:\n",
    "        success_score += 1\n",
    "        improvement_vs_random = (final_performance - 0.33) / 0.33 * 100\n",
    "        print(f\"   âœ… Supera agente aleatorio por {improvement_vs_random:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   âŒ No supera significativamente a agente aleatorio\")\n",
    "    \n",
    "    # Criterio 2: Alcanzar competitividad (50%+)\n",
    "    if final_performance >= 0.5:\n",
    "        success_score += 1\n",
    "        print(f\"   âœ… Agente competitivo (>{final_performance:.1%})\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Agente por debajo del nivel competitivo\")\n",
    "    \n",
    "    # Criterio 3: Convergencia estable\n",
    "    if len(df_checkpoints) >= 4:\n",
    "        recent_variance = np.var(df_checkpoints['win_rate'].iloc[-4:])\n",
    "        if recent_variance < 0.01:  # Varianza menor al 1%\n",
    "            success_score += 1\n",
    "            print(f\"   âœ… Convergencia estable alcanzada\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Convergencia inestable (varianza: {recent_variance:.3f})\")\n",
    "    \n",
    "    # Criterio 4: ExploraciÃ³n efectiva\n",
    "    exploration_ratio = metrics.get('q_table_size', 0) / metrics.get('games_played', 1)\n",
    "    if exploration_ratio > 0.3:  # MÃ¡s de 0.3 estados Ãºnicos por episodio\n",
    "        success_score += 1\n",
    "        print(f\"   âœ… ExploraciÃ³n efectiva del espacio de estados\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ExploraciÃ³n limitada del espacio de estados\")\n",
    "    \n",
    "    # Criterio 5: Velocidad de aprendizaje\n",
    "    if len(df_checkpoints) >= 3:\n",
    "        early_performance = df_checkpoints['win_rate'].iloc[1]  # Segundo checkpoint\n",
    "        learning_rate = final_performance - early_performance\n",
    "        if learning_rate > 0.1:  # Mejora de al menos 10%\n",
    "            success_score += 1\n",
    "            print(f\"   âœ… Velocidad de aprendizaje adecuada (+{learning_rate:.1%})\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Velocidad de aprendizaje lenta\")\n",
    "    \n",
    "    # Criterio 6: Balance exploraciÃ³n-explotaciÃ³n\n",
    "    final_epsilon = df_checkpoints['epsilon'].iloc[-1]\n",
    "    if final_epsilon < 0.2:  # Epsilon final bajo indica buena transiciÃ³n\n",
    "        success_score += 1\n",
    "        print(f\"   âœ… Balance exploraciÃ³n-explotaciÃ³n exitoso (Îµ={final_epsilon:.3f})\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ TransiciÃ³n exploraciÃ³n-explotaciÃ³n incompleta\")\n",
    "    \n",
    "    # CalificaciÃ³n general del proyecto\n",
    "    success_percentage = (success_score / total_criteria) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ… CALIFICACIÃ“N GENERAL: {success_score}/{total_criteria} ({success_percentage:.1f}%)\")\n",
    "    \n",
    "    if success_percentage >= 85:\n",
    "        project_grade = \"A+ (Excelente)\"\n",
    "        recommendation = \"Proyecto ejemplar, listo para publicaciÃ³n/presentaciÃ³n\"\n",
    "    elif success_percentage >= 75:\n",
    "        project_grade = \"A (Muy Bueno)\"\n",
    "        recommendation = \"Proyecto sÃ³lido con resultados convincentes\"\n",
    "    elif success_percentage >= 65:\n",
    "        project_grade = \"B+ (Bueno)\"\n",
    "        recommendation = \"Proyecto exitoso con algunas Ã¡reas de mejora\"\n",
    "    elif success_percentage >= 50:\n",
    "        project_grade = \"B (Aceptable)\"\n",
    "        recommendation = \"Proyecto funcional, necesita optimizaciÃ³n\"\n",
    "    else:\n",
    "        project_grade = \"C (Necesita Trabajo)\"\n",
    "        recommendation = \"Proyecto requiere revisiÃ³n significativa\"\n",
    "    \n",
    "    print(f\"   ğŸ–ï¸ CalificaciÃ³n: {project_grade}\")\n",
    "    print(f\"   ğŸ’¡ RecomendaciÃ³n: {recommendation}\")\n",
    "    \n",
    "    # AnÃ¡lisis comparativo con literatura\n",
    "    print(f\"\\nğŸ“š COMPARACIÃ“N CON LITERATURA ACADÃ‰MICA:\")\n",
    "    print(f\"   ğŸ“– Q-Learning en juegos de mesa tÃ­picamente alcanza 60-80% win rate\")\n",
    "    print(f\"   ğŸ“Š Nuestro resultado: {final_performance:.1%}\")\n",
    "    \n",
    "    if final_performance >= 0.7:\n",
    "        print(f\"   ğŸŒŸ EXCEPCIONAL: Supera expectativas acadÃ©micas\")\n",
    "    elif final_performance >= 0.6:\n",
    "        print(f\"   âœ… BUENO: Dentro del rango esperado para Q-Learning\")\n",
    "    elif final_performance >= 0.45:\n",
    "        print(f\"   ğŸ‘ ACEPTABLE: Resultado razonable para implementaciÃ³n inicial\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ BAJO: Por debajo de expectativas tÃ­picas\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¬ CONTRIBUCIONES TÃ‰CNICAS DEL PROYECTO:\")\n",
    "    print(f\"   âœ… ImplementaciÃ³n completa de Q-Learning desde cero\")\n",
    "    print(f\"   âœ… Sistema de mÃ©tricas comprehensivo\")\n",
    "    print(f\"   âœ… AnÃ¡lisis visual detallado del proceso de aprendizaje\")\n",
    "    print(f\"   âœ… Framework de torneo escalable\")\n",
    "    print(f\"   âœ… ComparaciÃ³n con mÃºltiples baselines\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No se pueden generar conclusiones sin datos de mÃ©tricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae22b1",
   "metadata": {},
   "source": [
    "### 8.2 Recomendaciones para Trabajo Futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ RECOMENDACIONES PARA TRABAJO FUTURO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nğŸ”¬ 1. MEJORAS ALGORÃTMICAS:\")\n",
    "print(\"   ğŸ§  Double Q-Learning para reducir sobreestimaciÃ³n\")\n",
    "print(\"   ğŸ¯ Prioritized Experience Replay\")\n",
    "print(\"   ğŸŒŠ Dueling DQN para separar valor de estado y ventaja\")\n",
    "print(\"   ğŸ”„ Multi-step returns para mejor propagaciÃ³n de recompensas\")\n",
    "\n",
    "print(\"\\nğŸ—ï¸ 2. ARQUITECTURAS AVANZADAS:\")\n",
    "print(\"   ğŸ¤– Deep Q-Networks (DQN) para generalizaciÃ³n\")\n",
    "print(\"   ğŸ­ Actor-Critic methods (A3C, PPO)\")\n",
    "print(\"   ğŸ§¬ Evolutionary strategies\")\n",
    "print(\"   ğŸŒ Monte Carlo Tree Search + Deep Learning (AlphaZero style)\")\n",
    "\n",
    "print(\"\\nğŸ® 3. EXPANSIÃ“N DEL DOMINIO:\")\n",
    "print(\"   ğŸ“ Connect 4 con tableros de diferentes tamaÃ±os\")\n",
    "print(\"   âš¡ Connect 4 con tiempo limitado\")\n",
    "print(\"   ğŸ² Otros juegos de mesa (Tic-Tac-Toe 3D, Othello)\")\n",
    "print(\"   ğŸ¤ Juegos cooperativos multi-agente\")\n",
    "\n",
    "print(\"\\nğŸ“Š 4. ANÃLISIS Y EVALUACIÃ“N:\")\n",
    "print(\"   ğŸ§ª A/B testing sistemÃ¡tico de hiperparÃ¡metros\")\n",
    "print(\"   ğŸ“ˆ AnÃ¡lisis de sensibilidad\")\n",
    "print(\"   ğŸ¨ Interpretabilidad de la polÃ­tica aprendida\")\n",
    "print(\"   ğŸ† Torneos contra humanos expertos\")\n",
    "\n",
    "print(\"\\nâš¡ 5. OPTIMIZACIÃ“N DE RENDIMIENTO:\")\n",
    "print(\"   ğŸš„ ParalelizaciÃ³n del entrenamiento\")\n",
    "print(\"   ğŸ’¾ OptimizaciÃ³n de memoria para Q-tables grandes\")\n",
    "print(\"   ğŸ”§ Auto-tuning de hiperparÃ¡metros\")\n",
    "print(\"   ğŸ“± ImplementaciÃ³n en dispositivos mÃ³viles\")\n",
    "\n",
    "print(\"\\nğŸŒ 6. APLICACIONES PRÃCTICAS:\")\n",
    "print(\"   ğŸ“ Plataforma educativa interactiva\")\n",
    "print(\"   ğŸ•¹ï¸ Videojuego con IA adaptativa\")\n",
    "print(\"   ğŸ” AnÃ¡lisis de estrategias humanas\")\n",
    "print(\"   ğŸ¤– Agente de entrenamiento para jugadores humanos\")\n",
    "\n",
    "# Crear roadmap visual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Definir categorÃ­as y elementos\n",
    "categories = [\n",
    "    \"Algoritmos\\nBÃ¡sicos\", \"Algoritmos\\nAvanzados\", \"Arquitecturas\\nModernas\",\n",
    "    \"EvaluaciÃ³n\\nCompleta\", \"OptimizaciÃ³n\", \"Aplicaciones\\nReales\"\n",
    "]\n",
    "\n",
    "# Definir timeline y complejidad\n",
    "timeline = np.array([1, 3, 6, 4, 5, 8])  # Meses estimados\n",
    "complexity = np.array([2, 6, 9, 5, 7, 8])  # Nivel de complejidad (1-10)\n",
    "impact = np.array([3, 7, 9, 6, 5, 8])  # Impacto potencial (1-10)\n",
    "\n",
    "# Crear scatter plot\n",
    "scatter = ax.scatter(timeline, complexity, s=impact*50, alpha=0.7, \n",
    "                   c=range(len(categories)), cmap='viridis')\n",
    "\n",
    "# AÃ±adir etiquetas\n",
    "for i, category in enumerate(categories):\n",
    "    ax.annotate(category, (timeline[i], complexity[i]), \n",
    "               xytext=(5, 5), textcoords='offset points',\n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Timeline Estimado (meses)', fontsize=12)\n",
    "ax.set_ylabel('Nivel de Complejidad (1-10)', fontsize=12)\n",
    "ax.set_title('ğŸ—ºï¸ Roadmap de Desarrollo Futuro\\n(TamaÃ±o = Impacto Potencial)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# AÃ±adir lÃ­neas de referencia\n",
    "ax.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='Complejidad Media')\n",
    "ax.axvline(x=6, color='red', linestyle='--', alpha=0.5, label='Horizonte 6 meses')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Configurar lÃ­mites\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“‹ PRIORIZACIÃ“N SUGERIDA:\")\n",
    "print(\"   ğŸ¥‡ ALTA PRIORIDAD (0-3 meses):\")\n",
    "print(\"      - OptimizaciÃ³n de hiperparÃ¡metros actuales\")\n",
    "print(\"      - AnÃ¡lisis detallado de estrategias emergentes\")\n",
    "print(\"      - ImplementaciÃ³n de Double Q-Learning\")\n",
    "\n",
    "print(\"   ğŸ¥ˆ MEDIA PRIORIDAD (3-6 meses):\")\n",
    "print(\"      - EvaluaciÃ³n contra agentes mÃ¡s diversos\")\n",
    "print(\"      - ImplementaciÃ³n de aproximaciÃ³n funcional\")\n",
    "print(\"      - Desarrollo de interfaz de usuario\")\n",
    "\n",
    "print(\"   ğŸ¥‰ BAJA PRIORIDAD (6+ meses):\")\n",
    "print(\"      - Arquitecturas de deep learning\")\n",
    "print(\"      - Aplicaciones comerciales\")\n",
    "print(\"      - InvestigaciÃ³n en otros dominios\")\n",
    "\n",
    "print(\"\\nğŸ’¡ CONSIDERACIONES FINALES:\")\n",
    "print(\"   ğŸ¯ El proyecto actual establece una base sÃ³lida\")\n",
    "print(\"   ğŸ“ˆ Los resultados justifican investigaciÃ³n adicional\")\n",
    "print(\"   ğŸ”¬ La metodologÃ­a es replicable y extensible\")\n",
    "print(\"   ğŸŒŸ Potencial para contribuciones acadÃ©micas originales\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
