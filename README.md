# ğŸ® Connect 4 AI Tournament

Un proyecto completo de Inteligencia Artificial que implementa diferentes agentes para jugar Connect 4 y los enfrenta en un torneo eliminatorio.

## ğŸš€ CaracterÃ­sticas

### ğŸ¤– Agentes Implementados

1. **MCTS Agent** - Monte Carlo Tree Search
   - ImplementaciÃ³n completa con UCB1
   - HeurÃ­sticas de victoria inmediata y bloqueo
   - Rollouts limitados para optimizaciÃ³n
   - SelecciÃ³n hacia el centro en empates

2. **Q-Learning Agent** - Aprendizaje por Refuerzo
   - Tabla Q para aprendizaje de estados
   - ExploraciÃ³n epsilon-greedy con decay
   - Entrenamiento contra oponente aleatorio
   - MÃ©tricas de entrenamiento detalladas

3. **Random Agents** - Baseline
   - Diferentes implementaciones aleatorias
   - Ãštiles para testing y baseline

### ğŸ† Sistema de Torneo

- **Torneo eliminatorio** con emparejamientos automÃ¡ticos
- **Manejo de BYEs** para nÃºmeros impares de participantes
- **Guardado automÃ¡tico** de todos los matches en JSON
- **ConfiguraciÃ³n flexible** (best_of, distribuciÃ³n de primer jugador, etc.)

### ğŸ“Š MÃ©tricas y AnÃ¡lisis

- **Logging automÃ¡tico** durante entrenamiento
- **Visualizaciones** de progreso de aprendizaje
- **AnÃ¡lisis estadÃ­stico** de rendimiento
- **Jupyter notebook** para anÃ¡lisis interactivo

## ğŸ“ Estructura del Proyecto

```
Proyecto_IA/
â”œâ”€â”€ main.py                     # Script principal
â”œâ”€â”€ Informe.ipynb              # Informe del proyecto
â””â”€â”€ tournament/
    â”œâ”€â”€ main.py                # Ejecutor del torneo
    â”œâ”€â”€ tournament.py          # LÃ³gica del torneo
    â”œâ”€â”€ train_q_learning.py    # Script de entrenamiento
    â”œâ”€â”€ connect4/              # Core del juego
    â”‚   â”œâ”€â”€ connect_state.py   # Estado del juego
    â”‚   â”œâ”€â”€ policy.py          # PolÃ­ticas/Agentes
    â”‚   â”œâ”€â”€ dtos.py           # Estructuras de datos
    â”‚   â””â”€â”€ utils.py          # Utilidades
    â”œâ”€â”€ learning/              # MÃ³dulos de aprendizaje
    â”‚   â””â”€â”€ q_learning_agent.py
    â”œâ”€â”€ metrics/               # MÃ©tricas y anÃ¡lisis
    â”‚   â”œâ”€â”€ metrics_logger.py
    â”‚   â””â”€â”€ metrics_analisys.ipynb
    â”œâ”€â”€ groups/                # Agentes participantes
    â”‚   â”œâ”€â”€ Group A/
    â”‚   â”œâ”€â”€ Group B/
    â”‚   â””â”€â”€ Group C/
    â””â”€â”€ versus/                # Resultados de matches
        â””â”€â”€ *.json
```

## ğŸ› ï¸ InstalaciÃ³n

```bash
# Clonar el repositorio
git clone <repository-url>
cd Proyecto_IA

# Instalar dependencias
pip install numpy matplotlib pydantic jupyter
```

## ğŸš€ Uso

### Modo RÃ¡pido

```bash
# Entrenar Q-Learning y ejecutar torneo
python main.py --mode train
python main.py --mode tournament
```

### Modo Detallado

1. **Entrenar el agente Q-Learning:**
```bash
python main.py --mode train
# O directamente:
cd tournament
python train_q_learning.py
```

2. **Ejecutar torneo:**
```bash
python main.py --mode tournament
# O directamente:
cd tournament
python main.py
```

3. **Analizar mÃ©tricas:**
```bash
python main.py --mode metrics
# O abrir directamente:
jupyter notebook tournament/metrics/metrics_analisys.ipynb
```

4. **Match personalizado:**
```bash
python main.py --mode play
```

## ğŸ“Š InterpretaciÃ³n de Resultados

### MÃ©tricas de Entrenamiento

- **Tasa de Victoria**: Porcentaje de partidas ganadas vs oponente aleatorio
- **Recompensas**: EvoluciÃ³n del aprendizaje (1=victoria, -1=derrota, 0=empate)
- **DuraciÃ³n de Partidas**: NÃºmero de movimientos por partida
- **Progreso Acumulado**: Victorias, derrotas y empates totales

### Resultados del Torneo

Los archivos en `tournament/versus/` contienen:
- **EstadÃ­sticas del match**: victorias, derrotas, empates
- **Historia completa**: cada movimiento de cada partida
- **Formato JSON**: fÃ¡cil anÃ¡lisis posterior

## ğŸ”§ ConfiguraciÃ³n

### ParÃ¡metros del Q-Learning

En `train_q_learning.py`:
- `episodes`: NÃºmero de partidas de entrenamiento (default: 2000)
- `alpha`: Tasa de aprendizaje (default: 0.1)
- `gamma`: Factor de descuento (default: 0.95)
- `epsilon_decay`: Decaimiento de exploraciÃ³n (default: 0.995)

### ParÃ¡metros del MCTS

En `connect4/policy.py`:
- `iterations`: Simulaciones por movimiento (default: 400)
- `c`: ParÃ¡metro de exploraciÃ³n UCB1 (default: 1.4)
- `rollout_limit`: LÃ­mite de pasos por rollout (default: 100)

### ParÃ¡metros del Torneo

En `tournament.py`:
- `best_of`: Partidas por match (default: 7)
- `first_player_distribution`: ProporciÃ³n de partidas como primer jugador (default: 0.5)
- `shuffle`: Mezclar emparejamientos iniciales (default: True)

## ğŸ§ª Testing

```bash
# Test rÃ¡pido con pocos episodios
cd tournament
python -c "from train_q_learning import train_q_learning_agent; train_q_learning_agent(episodes=100)"

# Test de torneo con agentes simples
python tournament.py
```

## ğŸ“ˆ OptimizaciÃ³n de Rendimiento

### Para Q-Learning:
- Aumentar `episodes` para mejor aprendizaje
- Ajustar `alpha` segÃºn velocidad de convergencia
- Modificar `epsilon_decay` para balance exploraciÃ³n/explotaciÃ³n

### Para MCTS:
- Aumentar `iterations` para mejor juego (mÃ¡s lento)
- Ajustar `c` para balance exploraciÃ³n/explotaciÃ³n
- Modificar `rollout_limit` segÃºn recursos computacionales

## ğŸ› Troubleshooting

### Problemas Comunes:

1. **"No module found"**: AsegÃºrate de estar en el directorio correcto
2. **"No se encontrÃ³ q_table"**: Ejecuta primero el entrenamiento
3. **"Memoria insuficiente"**: Reduce `episodes` o `iterations`
4. **"Partidas muy lentas"**: Reduce `iterations` del MCTS

### Logs y Debug:

- Los entrenamientos muestran progreso cada 50-100 episodios
- Los matches se guardan automÃ¡ticamente en `versus/`
- Las mÃ©tricas se registran en `metrics/training_metrics.json`

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ¯ PrÃ³ximas Mejoras

- [ ] Implementar agente basado en redes neuronales
- [ ] AÃ±adir mÃ¡s oponentes para entrenamiento
- [ ] Optimizar representaciÃ³n de estado para Q-Learning
- [ ] Implementar aprendizaje multi-agente
- [ ] AÃ±adir interfaz grÃ¡fica para juego humano vs IA
- [ ] ParalelizaciÃ³n del entrenamiento

---

**Â¡Buena suerte en el torneo! ğŸ†**